{
  "hash": "73ee2a5ffb6535d21fb97b9dbefe7a30",
  "result": {
    "engine": "knitr",
    "markdown": "# The Classical Linear Model (Gauss-Markov) Assumptions\n\n## CLRM Assumptions\n\nFor the Ordinary Least Squares (OLS) estimators $\\hat{\\alpha}$ and $\\hat{\\beta}$ to have desirable properties (like being the **Best Linear Unbiased Estimators**, or BLUE), a set of assumptions about the population model must hold. These are the core assumptions for **cross-sectional** data analysis.\n\nGiven that the population model is\n\n$$Y_i = \\alpha + \\beta X_i + u_i$$\n\n**Assumption 1: Conditional Mean Zero**\n\nThe error term $u$ has an expected value of zero, **given any value of the explanatory variable** $X$.\n\n$$E(u_i | X_i) = 0$$\n\n-   **Interpretation:** This means that the explanatory variable $X$ provides no information about the mean of the unobserved factors. On average, the positive and negative omitted effects cancel out. This is the single most important assumption.\n-   **Implication:** It implies that the model is correctly specified in its functional form and that there are **no omitted variables that are correlated with** $X$. If this assumption fails, our OLS estimates are **biased**.\n\n**Assumption 2: Homoskedasticity**\n\nThe error term $u$ has the same variance given any value of the explanatory variable.\n\n$$Var(u_i | X_i) = \\sigma^2$$\n\n-   **Interpretation:** The variance of the unobserved factors is constant across all values of $X$. The spread of the data points around the regression line is the same whether $X$ is small or large.\n-   **Implication:** If this holds, OLS standard errors are valid. If it fails, we have **heteroskedasticity**, which means OLS estimates are still unbiased but their standard errors are incorrect. This leads to faulty hypothesis tests and confidence intervals.\n\n**Assumption 3: No Autocorrelation**\n\nThe error terms for any two different observations are uncorrelated.\n\n$$Cov(u_i, u_j | X_i) = 0 \\quad \\text{for all } i \\neq j$$\n\n-   **Interpretation:** The unobserved factors affecting $Y$ for one observation are not correlated with the unobserved factors affecting $Y$ for any other observation. In cross-sectional data, this is usually guaranteed by random sampling. However autocorrelation is usually a concern for **time-series data**.\n-   **Implication:** Like heteroskedasticity, if this assumption fails, OLS estimates remain unbiased but the standard errors are incorrect/inefficient, leading to unreliable inference.\n\n**Assumption 4: Exogeneity**\n\nThe explanatory variable $X$ is uncorrelated with the error term $u$.\n\n$$Cov(X_i, u_i) = 0$$\n\n-   **Interpretation:** This is essentially a weaker version of Assumption 1. It means $X$ is not influenced by the unobserved factors in $u$.\n-   **Implication:** This assumption is crucial especially for **causal interpretation**. If $X$ is correlated with $u$, it could mean an omitted variable that affects $Y$ is also correlated with $X$ (a.k.a confounding, or the \"third variable\" problem). This is the famous **omitted variable bias**, which causes $\\hat{\\beta}$ to be biased and inconsistent.\n\n**Assumption 5: No Perfect Multicollinearity**\n\n-   **Interpretation (for simple regression):** In the simple regression model with one explanatory variable, this assumption is automatically satisfied as long as $X$ is not constant.\n-   **Interpretation (for multiple regression):** This assumption becomes critical when we have more than one explanatory variable. It states that no independent variable is a perfect linear combination of another independent variable(s).\n\n## The Gauss-Markov Theorem\n\nIf Assumptions 1 through 4 hold (and 5 for multiple regression), the OLS estimators $\\hat{\\alpha}$ and $\\hat{\\beta}$ are the **Best Linear Unbiased Estimators (BLUE)**.\n\n-   **Linear:** They are linear functions of the data.\n-   **Unbiased:** On average, across repeated samples, they equal the true population parameters, i.e. $E[\\hat{\\beta}] = \\beta$.\n-   **Best:** They have the smallest variance among all other linear unbiased estimators. This means they are the most precise (efficient).\n\nThis theorem is why OLS is the workhorse of econometrics—under these conditions, no other linear estimator is better.\n\n### Checking Assumptions\n\nWhile a full diagnostic check will be done in later chapters, we'll start by checking for non-constant variances, or heteroskedasticty.\n\n#### Testing for Heteroskedasticity\n\nHere is a quick example of how to generate residual plots whch can be useful to visually check for homoskedasticity and mean zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model\nmodel <- lm(mpg ~ wt, data = mtcars)\n\n# Create a dataframe of fitted values and residuals\ndiagnostic_data <- data.frame(\n  fitted = fitted(model),\n  residuals = resid(model)\n)\n\n# Plot residuals vs. fitted values\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(diagnostic_data, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values (Y_hat)\",\n       y = \"Residuals (u_hat)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](assumptions_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n**Interpreting the plot:** We look for a random scatter of points around the red zero line. The absence of a clear pattern (e.g., a curve or a funnel) is a good sign that Assumptions 1 and 2 are plausible. In this `mtcars` example, there might be a slight pattern, suggesting a potential minor violation worth investigating further.\n\nA more formal test is the **White test**, a general and powerful test for heteroskedasticity. It does not assume a specific form for the heteroskedasticity (e.g., that variance increases with X). The test works by regressing the squared residuals from the original model on the original explanatory variables, their squares, and their cross-products.\n\nThe null and alternative hypotheses are:\n\n$H_0$: Homoskedasticity exists (the error variance is constant).\n\n$H_1$: Heteroskedasticity exists (the error variance is not constant).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"lmtest\") # Uncomment and run if needed\nlibrary(lmtest)\n\n# Perform the White test for the simple model mpg ~ wt\nbptest(model, ~ wt + I(wt^2), data = mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  model\nBP = 1.3663, df = 2, p-value = 0.505\n```\n\n\n:::\n:::\n\n\nThe `bptest()` output shows a BP test statistic and a p-value. A p-value \\< 0.05 means you reject the null hypothesis of homoskedasticity. In the `mtcars` example above, we fail to reject the null hypothesis, suggesting that we have may not have heteroskedasticty.\n\nAlternatively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Estimate the original simple regression model\nsimple_model <- lm(mpg ~ wt, data = mtcars)\n\n# 2. Obtain the squared residuals from the model\nsquared_residuals <- resid(simple_model)^2\n\n# 3. Perform the \"auxiliary regression\" for the White test:\n# Regress the squared residuals on the original regressor (wt) and its square (wt²).\nwhite_aux_model <- lm(squared_residuals ~ wt + I(wt^2), data = mtcars)\n\n# 4. Conduct an F-test on the auxiliary model.\n# The null hypothesis is that the coefficients on 'wt' and 'I(wt^2)' are zero.\n# install.packages(\"car\") # Uncomment and run if you don't have the 'car' package\nlibrary(car)\n\nlinearHypothesis(white_aux_model, c(\"wt=0\", \"I(wt^2)=0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nwt = 0\nI(wt^2) = 0\n\nModel 1: restricted model\nModel 2: squared_residuals ~ wt + I(wt^2)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     31 4542.5                           \n2     29 4348.6  2    193.95 0.6467 0.5312\n```\n\n\n:::\n:::\n\n\nWe look at the F-statistic and its p-value (Pr(\\>F)). A p-value \\< 0.05 provides evidence to reject the null hypothesis ($H_0$) of homoskedasticity, thereby suggesting the error variance is not constant and depends on weight (wt). Here, again Pr(\\>F) is 0.5312, hence we fail to reject the null.\n\n",
    "supporting": [
      "assumptions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}