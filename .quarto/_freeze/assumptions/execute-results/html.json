{
  "hash": "5e960d69b249e9222259ba47d95592a1",
  "result": {
    "engine": "knitr",
    "markdown": "# The Classical Linear Model (Gauss-Markov) Assumptions\n\n## CLRM Assumptions\n\nFor the Ordinary Least Squares (OLS) estimators $\\hat{\\alpha}$ and $\\hat{\\beta}$ to have desirable properties (like being the **Best Linear Unbiased Estimators**, or BLUE), a set of assumptions about the population model must hold. These are the core assumptions for **cross-sectional** data analysis.\n\nGiven that the population model is\n\n$$Y_i = \\alpha + \\beta X_i + u_i$$\n\n**Assumption 1: Conditional Mean Zero**\n\nThe error term $u$ has an expected value of zero, **given any value of the explanatory variable** $X$.\n\n$$E(u_i | X_i) = 0$$\n\n-   **Interpretation:** This means that the explanatory variable $X$ provides no information about the mean of the unobserved factors. On average, the positive and negative omitted effects cancel out. This is the single most important assumption.\n-   **Implication:** It implies that the model is correctly specified in its functional form and that there are **no omitted variables that are correlated with** $X$. If this assumption fails, our OLS estimates are **biased**.\n\n**Assumption 2: Homoskedasticity**\n\nThe error term $u$ has the same variance given any value of the explanatory variable.\n\n$$Var(u_i | X_i) = \\sigma^2$$\n\n-   **Interpretation:** The variance of the unobserved factors is constant across all values of $X$. The spread of the data points around the regression line is the same whether $X$ is small or large.\n-   **Implication:** If this holds, OLS standard errors are valid. If it fails, we have **heteroskedasticity**, which means OLS estimates are still unbiased but their standard errors are incorrect. This leads to faulty hypothesis tests and confidence intervals.\n\n**Assumption 3: No Autocorrelation**\n\nThe error terms for any two different observations are uncorrelated.\n\n$$Cov(u_i, u_j | X_i) = 0 \\quad \\text{for all } i \\neq j$$\n\n-   **Interpretation:** The unobserved factors affecting $Y$ for one observation are not correlated with the unobserved factors affecting $Y$ for any other observation. In cross-sectional data, this is usually guaranteed by random sampling. However autocorrelation is usually a concern for **time-series data**.\n-   **Implication:** Like heteroskedasticity, if this assumption fails, OLS estimates remain unbiased but the standard errors are incorrect/inefficient, leading to unreliable inference.\n\n**Assumption 4: Exogeneity**\n\nThe explanatory variable $X$ is uncorrelated with the error term $u$.\n\n$$Cov(X_i, u_i) = 0$$\n\n-   **Interpretation:** This is essentially a weaker version of Assumption 1. It means $X$ is not influenced by the unobserved factors in $u$.\n-   **Implication:** This assumption is crucial especially for **causal interpretation**. If $X$ is correlated with $u$, it could mean an omitted variable that affects $Y$ is also correlated with $X$ (a.k.a confounding, or the \"third variable\" problem). This is the famous **omitted variable bias**, which causes $\\hat{\\beta}$ to be biased and inconsistent.\n\n**Assumption 5: No Perfect Multicollinearity**\n\n-   **Interpretation (for simple regression):** In the simple regression model with one explanatory variable, this assumption is automatically satisfied as long as $X$ is not constant.\n-   **Interpretation (for multiple regression):** This assumption becomes critical when we have more than one explanatory variable. It states that no independent variable is a perfect linear combination of another independent variable(s).\n\n## The Gauss-Markov Theorem\n\nIf Assumptions 1 through 4 hold (and 5 for multiple regression), the OLS estimators $\\hat{\\alpha}$ and $\\hat{\\beta}$ are the **Best Linear Unbiased Estimators (BLUE)**.\n\n-   **Linear:** They are linear functions of the data.\n-   **Unbiased:** On average, across repeated samples, they equal the true population parameters, i.e. $E[\\hat{\\beta}] = \\beta$.\n-   **Best:** They have the smallest variance among all other linear unbiased estimators. This means they are the most precise (efficient).\n\nThis theorem is why OLS is the workhorse of econometrics—under these conditions, no other linear estimator is better.\n\n### Checking Assumptions\n\nWhile a full diagnostic check will be done in later chapters, we'll start by checking for nonconstant variances, or heteroskedasticty.\n\n#### Testing for Heteroskedasticity\n\nHere is a quick example of how to generate residual plots whch can be useful to visually check for homoskedasticity and mean zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model\nmodel <- lm(mpg ~ wt, data = mtcars)\n\n# Create a dataframe of fitted values and residuals\ndiagnostic_data <- data.frame(\n  fitted = fitted(model),\n  residuals = resid(model)\n)\n\n# Plot residuals vs. fitted values\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(diagnostic_data, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values (Y_hat)\",\n       y = \"Residuals (u_hat)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](assumptions_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n**Interpreting the plot:** We look for a random scatter of points around the red zero line. The absence of a clear pattern (e.g., a curve or a funnel) is a good sign that Assumptions 1 and 2 are plausible. In this `mtcars` example, there might be a slight pattern, suggesting a potential minor violation worth investigating further.\n\nA more formal test is the **White test**, a general and powerful test for heteroskedasticity. It does not assume a specific form for the heteroskedasticity (e.g., that variance increases with X). The test works by regressing the squared residuals from the original model on the original explanatory variables, their squares, and their cross-products.\n\nThe null and alternative hypotheses are:\n\n$H_0$: Homoskedasticity exists (the error variance is constant).\n\n$H_1$: Heteroskedasticity exists (the error variance is not constant).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"lmtest\") # Uncomment and run if needed\nlibrary(lmtest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'lmtest' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: zoo\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'zoo' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'zoo'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n\n\n:::\n\n```{.r .cell-code}\n# Perform the White test for the simple model mpg ~ wt\nbptest(model, ~ wt * hp + I(wt^2) + I(hp^2), data = mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  model\nBP = 6.4892, df = 5, p-value = 0.2615\n```\n\n\n:::\n:::\n\nThe `bptest()` output shows a BP test statistic and a p-value. A p-value < 0.05 means you reject the null hypothesis of homoskedasticity. In the `mtcars` example above, we fail to reject the null hypothesis, suggesting that we have may not have heteroskedasticty.\n\nAltrrnatively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Estimate the original simple regression model\nsimple_model <- lm(mpg ~ wt, data = mtcars)\n\n# 2. Obtain the squared residuals from the model\nsquared_residuals <- resid(simple_model)^2\n\n# 3. Perform the \"auxiliary regression\" for the White test:\n# Regress the squared residuals on the original regressor (wt) and its square (wt²).\nwhite_aux_model <- lm(squared_residuals ~ wt + I(wt^2), data = mtcars)\n\n# 4. Conduct an F-test on the auxiliary model.\n# The null hypothesis is that the coefficients on 'wt' and 'I(wt^2)' are zero.\n# install.packages(\"car\") # Uncomment and run if you don't have the 'car' package\nlibrary(car)\n\nlinearHypothesis(white_aux_model, c(\"wt=0\", \"I(wt^2)=0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nwt = 0\nI(wt^2) = 0\n\nModel 1: restricted model\nModel 2: squared_residuals ~ wt + I(wt^2)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     31 4542.5                           \n2     29 4348.6  2    193.95 0.6467 0.5312\n```\n\n\n:::\n:::\n\nWe look at the F-statistic and its p-value (Pr(>F)). A p-value < 0.05 provides evidence to reject the null hypothesis ($H_0$) of homoskedasticity, thereby suggesting the error variance is not constant and depends on weight (wt). Here, again Pr(>F) is 0.5312, hence we fail to reject the null.\n\n#### Testing for Endogeneity\n\nEndogeneity occurs when an explanatory variable is correlated with the error term ($Cov(X, u) \\neq 0$), violating a key Gauss-Markov assumption (4). This often arises from:\n\n1.  Confounding leading to omitted variable bias\n2.  Reverse causality\n3.  Measurement Error\n4. Simultaneity\n\nThe consequence is that the OLS estimator becomes **biased and inconsistent**.\n\n**The Logic of the Hausman Test**\n\nThe test follows a straightforward logic:\n\n1.  **Null Hypothesis ($H_0$):** The variable in question is *exogenous* ($Cov(X, u) = 0$). OLS is consistent and efficient.\n\n2.  **Alternative Hypothesis ($H_1$):** The variable is *endogenous* ($Cov(X, u) \\neq 0$). OLS is inconsistent.\n\nThe test compares two estimators:\n\n-   **OLS Estimator:** Efficient (has the smallest possible variance) under $H_0$, but **inconsistent** under $H_1$.\n-   **IV (Instrumental Variables) Estimator:** Consistent under both $H_0$ and $H_1$, but **inefficient** (has larger variance) under $H_0$.\n\nIf the variable is exogenous ($H_0$ is true), the OLS and IV estimates should be similar. If they are significantly different, we have evidence that endogeneity is present ($H_1$ is true).\n\n**Implementing the Hausman Test in R: A Step-by-Step Guide**\n\nThe test is implemented as a **Durbin-Wu-Hausman** test via a convenient auxiliary regression.\n\n**Prerequisite:** You must have at least one **valid instrument** for the potentially endogenous variable. A valid instrument must be:\n\n1.  **Relevant:** Correlated with the endogenous variable.\n2.  **Exogenous:** *Not* correlated with the error term ($Cov(Z, u) = 0$).\n\n**Scenario:** Suppose we fear that `wt` (weight) is endogenous in our model `mpg ~ wt`. Suppose we use `hp` (horsepower) as instruments.\n\n**Step 1: Estimate the First Stage Regression**\n\nRegress the potentially endogenous variable (`hp`) on all exogenous variables and the instruments.\n\n::: {.cell}\n\n```{.r .cell-code}\n# First Stage: Regress the endogenous variable on instruments and other exogenous vars\nfirst_stage <- lm(hp ~ wt + hp, data = mtcars)\n\n# Retrieve the residuals from the first stage\nfirst_stage_residuals <- resid(first_stage)\n\n# Add these residuals to the original dataset for the next step\nmtcars$fs_resid <- first_stage_residuals\n```\n:::\n\n\n**Step 2: Estimate the Auxiliary Regression**\nRun the original model, but *include the first-stage residuals* as an additional regressor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Auxiliary Regression: Original model + first stage residuals\nauxiliary_model <- lm(mpg ~ wt + fs_resid, data = mtcars)\nsummary(auxiliary_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + fs_resid, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.28513    1.59870  23.322  < 2e-16 ***\nwt          -5.34447    0.47605 -11.227 4.49e-12 ***\nfs_resid    -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n\n\n:::\n:::\n\n\n**Step 3: Interpret the Result**\n-   The key is the **t-test on the coefficient of the residual variable (`fs_resid`)**.\n-   **Null Hypothesis ($H_0$):** The coefficient on the residuals is zero. This means the variable (`wt`) is exogenous.\n-   **Alternative Hypothesis ($H_1$):** The coefficient on the residuals is *not* zero. This is evidence of endogeneity.\n\nA low p-value (typically < 0.05) on the `fs_resid` coefficient leads to a rejection of the null hypothesis, suggesting that `wt` is indeed endogenous.\n\n**Using the `ivreg` and `lmtest` packages**\n\nA more efficient method is to use the `ivreg()` function from the `AER` package and then formally test for endogeneity.\n\nSpecifying a model with instruments: The syntax y ~ x1 + x2 | z1 + z2 means that we are regressing y on x1 and x2, using for x2 instruments z1 and z2 (and x1 is included as its own instrument).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"AER\") # Install the Applied Econometrics with R package\nlibrary(AER)\nlibrary(lmtest)\n\n# 1. Estimate the model via IV and OLS\n# IV model: Specify the formula and instruments\niv_model <- ivreg(mpg ~ wt | hp, data = mtcars)\n\n# 2. Perform the Hausman test\n# The null is that OLS is consistent (no endogeneity)\n\n# Run summary with diagnostics\nsummary(iv_model, diagnostics = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nivreg(formula = mpg ~ wt | hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.634 -2.428 -1.063  2.291 10.052 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   43.440      3.282  13.237 4.62e-14 ***\nwt            -7.258      1.001  -7.252 4.50e-08 ***\n\nDiagnostic tests:\n                 df1 df2 statistic  p-value    \nWeak instruments   1  30     23.00 4.15e-05 ***\nWu-Hausman         1  29     12.38  0.00145 ** \nSargan             0  NA        NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.591 on 30 degrees of freedom\nMultiple R-Squared: 0.6564,\tAdjusted R-squared: 0.6449 \nWald test: 52.59 on 1 and 30 DF,  p-value: 4.497e-08 \n```\n\n\n:::\n:::\n\n\n**Interpreting the Test:**\n\n**Weak Instruments Test**\n\nWhat it tests: Whether the instrument(s) (hp) is sufficiently correlated with the endogenous regressor (wt). A strong first-stage relationship is crucial for IV to work.\n\nInterpretation: The null hypothesis is that the instruments are weak.\n\nThe result (p-value = 4.15e-05), an extremely low p-value, suggests that we should strongly reject the null. The instrument(s) are not weak; they are strong and relevant. This is a good sign.\n\n**Wu-Hausman Test (for Endogeneity)**\n\nWhat it tests: This is the test for endogeneity. The null hypothesis ($H_0$) is that the variable (wt) is exogenous. The alternative ($H_1$) is that it is endogenous.\n\nInterpretation: A low p-value suggests you should reject $H_0$ and use the IV estimator. A high p-value means you cannot reject $H_0$ and should prefer the efficient OLS estimator.\n\nThe result (p-value = 0.0014), a low p-value means we should reject the null hypothesis. There is statistical evidence that wt is endogenous. Therefore, the standard OLS estimates for mpg ~ wt gives biased etimates.\n\n**Sargan Test (for Overidentifying Restrictions)**\n\nWhat it tests: This test checks the validity of your overidentifying instruments. It is only relevant if you have more instruments than endogenous variables. The null hypothesis ($H_0$) is that the extra instruments are valid (uncorrelated with the error term).\n\nInterpretation: A low p-value is undesirable, as it means you should reject $H_0$ and suspect that at least one of your extra instruments is invalid.\n\n**What to Do If You Find Endogeneity?**\n\nIf the test suggests endogeneity, you should **not trust the OLS results**. You must use a method that addresses the endogeneity, such as:\n\n-   **Finding Better Controls:** If the endogeneity is from omitted variable bias.\n-   **Instrumental Variables (IV) Regression:** The primary solution, implemented with `ivreg()`.\n-   **Using Panel Data Methods:** Such as fixed effects models, if you have panel data.\n\n**Important:** The validity of the Hausman test hinges on the **quality of your instruments**. If your instruments are weak or invalid, the test itself is unreliable. Always check the first-stage F-statistic to ensure instrument strength (a rule of thumb is F-stat > 10).\n",
    "supporting": [
      "assumptions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}