{
  "hash": "60ef2905762da65513c367184e2f8f37",
  "result": {
    "engine": "knitr",
    "markdown": "# The Multiple Regression Model\n\nThe simple regression model is powerful but often insufficient. Economic relationships are rarely driven by a single factor. The **multiple regression model** allows us to quantify the relationship between a dependent variable and *several* independent variables simultaneously, which is crucial for tackling omitted variable bias.\n\n## The Trivariate Model & Interpretation\n\nThe population multiple regression model with two explanatory variables (the trivariate model) is written as:\n\n$$Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i$$\n\nTaking expectations on both sides gives the conditional mean function:\n\n$$E(Y_i | X_{1i}, X_{2i}) = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i}$$\n\nThe key advantage of multiple regression is the **ceteris paribus** (all else equal) interpretation of its coefficients.\n\n-   $\\beta_1$ is the marginal effect of $X_1$ on $Y$. It is the effect of a small change in the $X_1$ on the dependent variable, **while holding $X_2$ constant**.\n-   $\\beta_2$ is the marginal effect of $X_2$ on $Y$, **while holding $X_1$ constant**.\n\nThis holding-constant effect is what helps isolate the direct effect of one variable, controlling for the influence of others.\n\n## The OLS Estimators and Normal Equations\n\nThe sample regression function (SRF) is:\n\n$$\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i}$$\n$$Y_i = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i} + \\hat{u}_i$$\n\nAs in the bivariate case, the OLS procedure consists of choosing the unknown parameters $(\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2)$ such that the residual sum of squares (SSR) is minimized:\n\n$$\\min_{\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2} \\sum_{i=1}^n \\hat{u}_i^2 = \\min_{\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2} \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta}_1 X_{1i} - \\hat{\\beta}_2 X_{2i})^2$$\n\nDifferentiating and setting the derivatives to zero yields the system of **normal equations**:\n\n$$\\begin{aligned}\n\\sum Y_i &= n\\hat{\\alpha} + \\hat{\\beta}_1 \\sum X_{1i} + \\hat{\\beta}_2 \\sum X_{2i} \\\\\n\\sum X_{1i}Y_i &= \\hat{\\alpha} \\sum X_{1i} + \\hat{\\beta}_1 \\sum X_{1i}^2 + \\hat{\\beta}_2 \\sum X_{1i}X_{2i} \\\\\n\\sum X_{2i}Y_i &= \\hat{\\alpha} \\sum X_{2i} + \\hat{\\beta}_1 \\sum X_{1i}X_{2i} + \\hat{\\beta}_2 \\sum X_{2i}^2\n\\end{aligned}$$\n\nSolving this system of three equations provides the formulas for the OLS estimators $\\hat{\\alpha}$, $\\hat{\\beta}_1$, and $\\hat{\\beta}_2$. While the formulas are more complex than in the simple regression case, the intuition is similar. \n\nHence, the OLS estimators for the slope coefficients in a multiple regression model $Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i$ are given by:\n\n$$\n\\begin{align*}\n\\hat{\\beta}_1&=\\frac{(\\sum_{i=1}^n x_{1i}y_i)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{2i}y_i)(\\sum_{i=1}^n x_{1i}x_{2i})}{(\\sum_{i=1}^n x_{1i}^2)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{1i}x_{2i})^2}\\\\ \n\\hat{\\beta}_2&=\\frac{(\\sum_{i=1}^n x_{2i}y_i)(\\sum_{i=1}^n x_{1i}^2)-(\\sum_{i=1}^n x_{1i}y_i)(\\sum_{i=1}^n x_{1i}x_{2i})}{(\\sum_{i=1}^n x_{1i}^2)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{1i}x_{2i})^2}\n\\end{align*}\n$$\nAnd the estimator for the intercept is:\n\n$$\n\\hat\\alpha = \\bar Y - \\hat\\beta_1 \\bar X_1 - \\hat\\beta_2 \\bar X_2\n$$\n\n### Extending the `mtcars` Model\n\nWe can estimate the coefficients easily using R. Let's first expand our car mileage model. Perhaps a car's horsepower (`hp`) also affects its fuel efficiency (`mpg`), in addition to its weight (`wt`). We can estimate this trivariate model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate a multiple regression model\n# mpg = alpha + beta1 * wt + beta2 * hp + u\n\nmulti_model <- lm(mpg ~ wt + hp, data = mtcars)\n\n# Print the summary results\nsummary(multi_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.22727    1.59879  23.285  < 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n\n\n:::\n:::\n\n\n**Interpretation of the Coefficients:**\n\n-   **wt coefficient (-3.878):** Holding horsepower constant, a one-ton increase in weight is associated with a decrease in fuel efficiency of approximately 3.88 miles per gallon, on average.\n-   **hp coefficient (-0.032):** Holding weight constant, a one-horsepower increase is associated with a decrease in fuel efficiency of approximately 0.032 miles per gallon, on average.\n-   **Intercept (37.227):** The predicted miles per gallon for a car that weighs 0 tons and has 0 horsepower. (Note: This is often not a meaningful value and serves just as a baseline for the regression line.)\n\nNotice how the coefficient on `wt` changed from -5.34 in the simple regression to -3.88 in this multiple regression. This suggests that horsepower was an omitted variable that was correlated with both weight and mileage, and failing to control for it biased our initial estimate of the effect of weight.\n\n## Standard Errors of the OLS Estimates\n\nThe formula for the variance of a slope estimator, say $\\hat{\\beta}_1$, in the trivariate model is:\n\n$$Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum (X_{1i} - \\bar{X}_1)^2 (1 - r^2_{12})}$$\n\n-   $\\sigma^2$ is the variance of the error term $u_i$.\n-   $\\sum (X_{1i} - \\bar{X}_1)^2$ is the total variation in $X_1$.\n-   $r_{12}$ is the sample correlation between $X_1$ and $X_2$.\n\nSince we never observe the true error variance ($\\sigma^2$), we bootstrap and use its estimate:\n\n$$\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2$$\n\nwhere $k$ is the number of estimated coefficients (including the constant, so $k=3$ for our model). The standard error of the coefficient is then the square root of its estimated variance: $SE(\\hat{\\beta}_1) = \\sqrt{\\widehat{Var}(\\hat{\\beta}_1)}$.\n\nHence, the standard errors for $\\beta 's$ in the model $Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i$ are given by:\n\n$$\n\\begin{aligned}\nSE(\\hat{\\beta}_1) &= \\sqrt{\\widehat{Var}(\\hat{\\beta}_1)} \\quad  \\text{or} \\quad \\hat{\\sigma} \\frac{1}{\\sqrt{\\sum x_{1i}^2 (1 - r_{12}^2)}} \\\\\nSE(\\hat{\\beta}_2) &= \\sqrt{\\widehat{Var}(\\hat{\\beta}_2)} \\text{or} \\quad \\hat{\\sigma} \\frac{1}{\\sqrt{\\sum x_{2i}^2 (1 - r_{12}^2)}}\n\\end{aligned}\n$$\n\nThe standard error for the intercept is:\n\n$$\nSE(\\hat{\\alpha}) = \\sqrt{\\widehat{Var}(\\hat{\\alpha})} = \\hat{\\sigma} \\sqrt{ \\frac{1}{n} + \\frac{\\overline{X}_1^2 \\sum x_{2i}^2 + \\overline{X}_2^2 \\sum x_{1i}^2 - 2\\overline{X}_1 \\overline{X}_2 \\sum x_{1i} x_{2i}}{\\sum x_{1i}^2 \\sum x_{2i}^2 - (\\sum x_{1i} x_{2i})^2} }\n$$\n\nwhere:\n\n- $\\hat{\\sigma} = \\sqrt{\\frac{\\sum \\hat{u}_i^2}{n - k}}$ is the standard error of the regression ($SER$),\n- $r_{12}$ is the sample correlation between $X_1$ and $X_2$,\n- $x_{1i} = X_{1i} - \\bar{X}_1$ and $x_{2i} = X_{2i} - \\bar{X}_2$ are deviations\n\nThe `summary()` function in R displays these standard errors, as seen in the output above.\n\n## R-squared and The Adjusted R-squared\n\nAs in the bivariate case, the coefficient of determination, $R^2$, is the fraction of the sample variation in $Y$ explained by the model:\n\n$$R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}$$\n\nA key feature of multiple regression is that adding *any* new variable (even an irrelevant one) will never decrease the $R^2$. Because of this, we often prefer the **adjusted R-squared**, which penalizes for adding irrelevant variables.\n\n$$\\bar{R}^2 = 1 - \\frac{SSR/(n-k)}{SST/(n-1)} = 1 - \\left( \\frac{n-1}{n-k} \\right) \\frac{SSR}{SST}$$\n\n-   $n$ is the sample size.\n-   $k$ is the number of coefficients, including the constant.\n-   $\\bar{R}^2$ can decrease if a new variable adds little explanatory power, providing a better gauge of whether a variable should be included.\n-   Always compare $\\bar{R}^2$, not $R^2$, when models have a different number of predictors.\n\nIn our `mtcars` output, we see both `R-squared:  0.8268` and `Adjusted R-squared:  0.8148`.\n\n### Interpreting $R^2$ and $\\bar{R}^2$ in Practice\n\nIt is critical to remember that:\n1.  An increase in the $R^2$ or $\\bar{R}^2$ does not necessarily mean that an added variable is statistically significant.\n2.  A high $R^2$ or $\\bar{R}^2$ does not mean that the regressors are a true cause of the dependent variable (causation vs. correlation).\n3.  A high $R^2$ or $\\bar{R}^2$ does not mean that there is no omitted variable bias.\n4.  A high $R^2$ or $\\bar{R}^2$ does not necessarily mean that you have the most appropriate set of regressors, nor does a low $R^2$ or $\\bar{R}^2$ mean that you have an inappropriate model.\n\n## Hypothesis Testing\n\n### Testing Individual Coefficients\n\nThe procedure for testing hypotheses about a single coefficient is identical to the simple regression case. For example, to test if $X_1$ has a significant effect on $Y$ after controlling for $X_2$:\n\n-   $H_0: \\beta_1 = 0$\n-   $H_1: \\beta_1 \\neq 0$\n\nThe test statistic is the t-ratio: $t = \\displaystyle\\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}$.\n\nAs a rule of thumb, we reject the null hypothesis if the absolute value of the t-ratio is greater than \"2\".\n\n### Testing the Overall Significance: The F-Test\n\nThe test for the overall significance of the regression is a joint test that all slope coefficients are equal to zero:\n\n$$\nH_0: \\beta_1 = \\beta_2 = 0 \\quad \\text{vs.} \\quad H_1: \\text{at least one } \\beta_j \\neq 0\n$$\n\nThis test is conducted using the **F-statistic**. The F-statistic can be computed in several equivalent forms:\n\n**1. Using sums of squares and cross-products:**\n$$\nF = \\frac{ \\left( \\hat{\\beta}_1 \\sum y_i x_{1i} + \\hat{\\beta}_2 \\sum y_i x_{2i} \\right) / m }{ \\sum \\hat{u}_i^2 / (n - k) }\n$$\nwhere\n\n- $m$ is the number of restrictions\n- $n-k$ is the degree of freedom of the regression\n\nThe above is the ratio of explained and unexplained sums of squares divided by their respective degrees of freedom which corresponds to ANOVA:\n\n$$\nF = \\frac{SSE / (k - 1)}{SSR / (n - k)}\n$$\nwhere $k$ is the total number of estimated parameters (for a model with two regressors, $k=3$: $\\alpha$, $\\beta_1$, and $\\beta_2$).\n\nNote this is the same if we use the coefficient of determination ($R^2$):\n\n$$\nF = \\frac{R^2 / (k - 1)}{(1 - R^2) / (n - k)}\n$$\n\nHence, the new test in multiple regression is the test for **overall significance of the regression**, which is a joint test that all slope coefficients are simultaneously equal to zero.\n\n-   $H_0: \\beta_1 = \\beta_2 = 0$\n-   $H_1: \\text{At least one } \\beta_j \\neq 0$\n\nThis test is given by the **F-statistic**, which is reported in the standard regression output. The F-statistic is constructed using the sums of squares from the ANOVA (Analysis of Variance) framework:\n\n$$SST = SSE + SSR$$\n\nThe F-statistic is the ratio of the explained to unexplained variance, adjusted for degrees of freedom:\n\n$$F = \\frac{SSE / (k-1)}{SSR / (n-k)} = \\frac{MSR}{MSE}$$\n\nwhere $k-1$ is the number of slope coefficients. A large F-statistic provides evidence against the null hypothesis that the model provides no better fit than a model with only an intercept.\n\n### Joint Tests and Restricted Models\n\nThe F-test can be generalized to test any set of **linear restrictions**. For example, we can test if a subset of coefficients is equal to zero.\n\nAssume the **unrestricted model** is:\n\n$$Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + u_i$$\n\nWe could test the joint hypothesis:\n\n-   $H_0: \\beta_2 = 0, \\beta_3 = 0$\n-   $H_1: H_0 \\text{ is not true}$\n\nThis involves estimating a **restricted model** where the restrictions under the null are imposed:\n$$Y_i = \\alpha + \\beta_1 X_{1i} + u_i$$\n\nThe general F-statistic formula for testing $m$ restrictions is:\n\n$$F = \\frac{(SSR_r - SSR_{ur}) / m}{SSR_{ur} / (n - k)}$$\n\nwhere:\n\n-   $SSR_r$ is the sum of squared residuals from the restricted model.\n-   $SSR_{ur}$ is the sum of squared residuals from the unrestricted model.\n-   $m$ is the number of restrictions.\n-   $n - k$ is the degrees of freedom in the unrestricted model.\n\n\n\nHere is another example where We could test the joint hypothesis:\n\n-   $H_0: \\beta_1 = 0, \\beta_2 + \\beta_3 = 1$\n-   $H_1: H_0 \\text{ is not true}$\n\nHere, the unrestricted model is the same as before:\n\n$$Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + u_i$$\n\nTo find the restricted model, we impose the null hypothesis:\n\nSubstitute $\\beta_1 = 0$ and $\\beta_3 = 1 - \\beta_2$ into the model:\n\n$$Y_i = \\alpha + \\beta_2 X_{2i} + (1 - \\beta_2) X_{3i} + u_i$$\n\nRearrange the equation:\n\n$$Y_i = \\alpha + X_{3i} + \\beta_2 (X_{2i} - X_{3i}) + u_i$$\n\nDefine a new dependent variable $Y_i^* = Y_i - X_{3i}$:\n\n$$Y_i^* = \\alpha + \\beta_2 (X_{2i} - X_{3i}) + u_i$$\n\nWe then perform the F-test as usual and a low p-value  would lead to a rejection of the null hypothesis $H_0$. Note here that $m$ the number of restrictions is 2 and not 3!\n\n\n## The Problem of Omitted Variable Bias\n\nOften, we are interested in understanding the relation between two variables ($Y$ and $X$). But running a simple regression of $Y$ on $X$ might not be enough. The assumption $Cov(X, u)=0$ might be violated if the error term $u$ contains a variable that is correlated with $X$, thereby introducing a bias. Multiple regression is a primary tool to help resolve this **endogeneity** problem.\n\n### Direction of the Bias\n\nOmitting an important variable introduces a bias to the OLS estimator. The direction of this bias can be formalized.\n\nSuppose the **true model** is:\n$$Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i$$\nBut we incorrectly estimate the **misspecified model**:\n$$Y_i = \\alpha + \\beta_1 X_{1i} + v_i \\quad \\text{where} \\quad v_i = \\beta_2 X_{2i} + u_i$$\n\nThe bias in the simple regression estimator $\\tilde{\\beta}_1$ is:\n$$Bias(\\tilde{\\beta}_1) = E[\\tilde{\\beta}_1] - \\beta_1 = \\beta_2 \\cdot \\tilde{\\delta}_1$$\n\nwhere $\\tilde{\\delta}_1$ is the slope coefficient from an **auxiliary regression** of the omitted variable ($X_2$) on the included variable ($X_1$):\n$$X_{2i} = \\delta_0 + \\delta_1 X_{1i} + e_i$$\n\n-   The **sign of the bias** depends on the signs of $\\beta_2$ (the effect of the omitted variable on $Y$) and $\\tilde{\\delta}_1$ (the correlation between $X_2$ and $X_1$).\n-   The **size of the bias** depends on the magnitude of $\\beta_2$ and $\\tilde{\\delta}_1$.\n\n## The Cost of Including an Irrelevant Variable\n\nConversely, including a variable that does not belong in the true model (i.e., whose true coefficient is zero) has different consequences.\n\nSuppose the **true model** is:\n$$Y_i = \\alpha + \\beta_1 X_{1i} + u_i$$\nBut we incorrectly estimate:\n$$Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + v_i$$\n\n-   **Does it introduce bias?** No. The OLS estimators for all coefficients, including $\\hat{\\beta}_1$, remain **unbiased**.\n-   **What is the cost?** **Increased variance.** The estimates become less precise. The standard error of $\\hat{\\beta}_1$ will generally be larger than it would be in the correctly specified simple regression model, leading to less powerful hypothesis tests and wider confidence intervals.\n\nThe trade-off is clear: omitting a relevant variable causes bias, while including an irrelevant variable reduces efficiency. When in doubt, it is often less harmful to include a potentially irrelevant variable than to omit a potentially relevant one.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}