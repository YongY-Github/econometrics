[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics Notes",
    "section": "",
    "text": "Welcome\nThis is a Quarto document containing notes for the MAAE Applied Econometrics class, Fall 2025. Kindly note that this is work-in-progress and your comments are welcome to help improve this work!\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "1.1 Introduction\nEconomic theory suggests relationships between variables, but it rarely provides the quantitative magnitude of these causal effects. For example, we are interested in questions such as:\nIdeally, we would answer these questions with controlled experiments. However, this is often impractical, unethical, or impossible. Instead, econometricians must rely on observational data.\nThe core challenge with observational studies is that correlation does not imply causation. Some major threats to establishing a proper empirical understanding of economic relationships are:\nAs a way of introduction, we introduce the primary tools used to estimate relationships from observational data in econometrics: the Ordinary Least Squares (OLS) method.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-population-regression-function-prf",
    "href": "intro.html#the-population-regression-function-prf",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "3.1 The Population Regression Function (PRF)",
    "text": "3.1 The Population Regression Function (PRF)\nImagine we could collect data on everyone in the population of interest. The true relationship in the population is given by the Population Regression Function:\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\n\n\\(Y_i\\) is the dependent variable for observation \\(i\\).\n\\(X_i\\) is the independent variable for observation \\(i\\).\n\\(\\alpha\\) is the population intercept.\n\\(\\beta\\) is the population slope coefficient (the parameter of primary interest).\n\\(u_i\\) is the error term, which contains all factors other than \\(X\\) that influence \\(Y\\).\n\nWe can never observe the true PRF because we cannot collect data on the entire population. The error term \\(u_i\\) exists due to: (1) The inherent randomness of human behavior, (2) Unavailable or incomplete data, (3) Omitted variables from the model, (4) Imperfect functional form specification, (5) Aggregation errors, (6) Measurement errors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-sample-regression-function-srf",
    "href": "intro.html#the-sample-regression-function-srf",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "3.2 The Sample Regression Function (SRF)",
    "text": "3.2 The Sample Regression Function (SRF)\nSince we can’t work with the population, we take a sample and use it to estimate the PRF. The estimated model is called the Sample Regression Function:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\\]\n\\[Y_i = \\hat{\\alpha} + \\hat{\\beta} X_i + \\hat{u}_i\\]\n\n\\(\\hat{Y}_i\\) is the predicted or fitted value of \\(Y_i\\).\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the estimators of the population parameters \\(\\alpha\\) and \\(\\beta\\). These coefficients are calculated from our sample data.\n\\(\\hat{u}_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\), which is our estimate of the unobserved error term \\(u_i\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#fitting-an-ols-model-in-r",
    "href": "intro.html#fitting-an-ols-model-in-r",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "4.2 Fitting an OLS Model in R",
    "text": "4.2 Fitting an OLS Model in R\nLet’s use the mtcars dataset to estimate a simple regression model, predicting miles per gallon (mpg) using car weight (wt).\n\n# Load the built-in dataset\ndata(mtcars)\n\n# Estimate the OLS model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print a summary of the results\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe output shows our estimated coefficients: - \\(\\hat{\\alpha}\\) (Intercept) = 37.29 - \\(\\hat{\\beta}\\) (wt) = -5.34\nThis gives us the Sample Regression Line: \\[\\widehat{mpg}_i = 37.29 - 5.34 \\times wt_i\\] Interpretation: For a one-ton increase in car weight, we predict miles per gallon will decrease by about 5.34 units, holding all else constant.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#deriving-the-ols-estimators",
    "href": "intro.html#deriving-the-ols-estimators",
    "title": "1  Ordinary Least Squares (OLS) and the Simple Regression Model",
    "section": "4.2 Deriving the OLS estimators",
    "text": "4.2 Deriving the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are derived by solving the minimization problem of the Sum of Squared Residuals (SSR). This process involves calculus, specifically taking derivatives and setting them to zero to find the minimum. The resulting equations are called the normal equations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares (OLS) and the Simple Regression Model</span>"
    ]
  },
  {
    "objectID": "intro.html#the-minimization-problem",
    "href": "intro.html#the-minimization-problem",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "5.1 The Minimization Problem",
    "text": "5.1 The Minimization Problem\nWe aim to find the values of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimize: \\[S(\\hat{\\alpha}, \\hat{\\beta}) = \\sum_{i=1}^n \\hat{u}_i^2 = \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-normal-equations",
    "href": "intro.html#the-normal-equations",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "5.2 The Normal Equations",
    "text": "5.2 The Normal Equations\nTo find the minimum, we take the partial derivatives of \\(S(\\hat{\\alpha}, \\hat{\\beta})\\) with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and set them equal to zero.\n\nDerivative with respect to \\(\\hat{\\alpha}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\alpha}} = -2 \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the first normal equation: \\[\\sum_{i=1}^n Y_i = n\\hat{\\alpha} + \\hat{\\beta} \\sum_{i=1}^n X_i \\quad \\text{(1)}\\]\nDerivative with respect to \\(\\hat{\\beta}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\beta}} = -2 \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the second normal equation: \\[\\sum_{i=1}^n X_iY_i = \\hat{\\alpha} \\sum_{i=1}^n X_i + \\hat{\\beta} \\sum_{i=1}^n X_i^2 \\quad \\text{(2)}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#solving-the-normal-equations",
    "href": "intro.html#solving-the-normal-equations",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "5.3 Solving the Normal Equations",
    "text": "5.3 Solving the Normal Equations\nWe now have a system of two equations with two unknowns (\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)).\n\nSolving for \\(\\hat{\\alpha}\\): Start by rearranging the first normal equation (1): \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] where \\(\\bar{Y} = \\frac{1}{n}\\sum Y_i\\) and \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\). This is our formula for the intercept.\nSolving for \\(\\hat{\\beta}\\): Substitute the expression for \\(\\hat{\\alpha}\\) into the second normal equation (2): \\[\\sum X_iY_i = (\\bar{Y} - \\hat{\\beta}\\bar{X})\\sum X_i + \\hat{\\beta} \\sum X_i^2\\] Solving this for \\(\\hat{\\beta}\\) involves some algebra. Subtract \\(\\bar{Y}\\sum X_i\\) from both sides and factor out \\(\\hat{\\beta}\\): \\[\\sum X_iY_i - \\bar{Y}\\sum X_i = \\hat{\\beta} \\left( \\sum X_i^2 - \\bar{X}\\sum X_i \\right)\\] Note that \\(\\sum X_iY_i - \\bar{Y}\\sum X_i = \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\) and \\(\\sum X_i^2 - \\bar{X}\\sum X_i = \\sum (X_i - \\bar{X})^2\\). This gives us the final formula: \\[\\hat{\\beta} = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-formula-for-the-standard-error-of-hatbeta",
    "href": "intro.html#the-formula-for-the-standard-error-of-hatbeta",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)",
    "text": "6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)\nUnder the classical linear model assumptions, the formula for the variance of the slope estimator is: \\[Var(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\] where \\(\\sigma^2\\) is the variance of the error term \\(u_i\\).\nSince we never know the true \\(\\sigma^2\\), we estimate it using the sample. The estimator for the error variance is: \\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2 = \\frac{SSR}{n - k}\\]\nwhere \\(k\\) is the number of estimated parameters (for a simple regression, \\(k=2\\): the intercept and the slope). We use \\(n-k\\) instead of \\(n\\) to make this an unbiased estimator of \\(\\sigma^2\\) (\\(E[\\hat{\\sigma}^2] = \\sigma^2\\)). This adjustment is called using degrees of freedom.\nThe Standard Error of the Regression (SER) is an estimator of the standard deviation of the error term \\(u_i\\). It measures the average distance that the observed values fall from the regression line—the “typical” size of a residual.\n\\[SER = \\sqrt{\\frac{1}{n-k} \\sum_{i=1}^n \\hat{u}_i^2} = \\sqrt{\\frac{SSR}{n-k}}\\]\nwhere \\(n\\) is the sample size and \\(k\\) is the number of independent variables plus the constant (for a simple regression, \\(k=2\\)).\nHence, the estimated variance of \\(\\hat{\\beta}\\) is: \\[\\widehat{Var}(\\hat{\\beta}) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nThe standard error of \\(\\hat{\\beta}\\) is just the square root of this estimated variance: \\[SE(\\hat{\\beta}) = \\sqrt{\\widehat{Var}(\\hat{\\beta})} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}} = \\sigma \\sqrt{\\frac{1}{\\sum_{i=1}^n x_i}}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#what-drives-the-standard-error",
    "href": "intro.html#what-drives-the-standard-error",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "6.2 What Drives the Standard Error?",
    "text": "6.2 What Drives the Standard Error?\nThe formula for \\(SE(\\hat{\\beta})\\) provides deep intuition about what makes an estimate precise: 1. Spread of the error term (\\(\\hat{\\sigma}\\)): A larger error variance (a noisier relationship, where points are scattered farther from the line) leads to a larger standard error and less precise estimates. 2. Sample size (\\(n\\)): A larger sample size \\(n\\) will (all else equal) make \\(\\hat{\\sigma}\\) smaller and the denominator larger, leading to a smaller standard error and more precise estimates. 3. Spread of the regressor \\(X\\) (\\(SST_X\\)): More variation in the independent variable \\(X\\) provides more “information” and leads to a smaller standard error. If all values of \\(X\\) are clustered closely together, it is harder to pin down the slope of the relationship.\nThe standard error for the intercept \\(\\hat{\\alpha}\\) has a more complex formula but is driven by the same factors: \\(n\\), \\(\\hat{\\sigma}\\), and the spread of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#calculating-standard-errors-in-r",
    "href": "intro.html#calculating-standard-errors-in-r",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "6.3 Calculating Standard Errors in R",
    "text": "6.3 Calculating Standard Errors in R\nYou don’t need to calculate these by hand. The summary() function in R computes them automatically using the formulas above.\n\n# Re-running the model from earlier for clarity\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# The summary output shows the coefficients and their standard errors\nsummary_model &lt;- summary(model)\nprint(summary_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIn the output, the Std. Error column next to the (Intercept) and wt estimates contains the calculated \\(SE(\\hat{\\alpha})\\) and \\(SE(\\hat{\\beta})\\). These values are used to compute the t-statistics and p-values for hypothesis testing, allowing us to assess the statistical significance of our estimates.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "assumptions.html",
    "href": "assumptions.html",
    "title": "2  The Classical Linear Model (Gauss-Markov) Assumptions",
    "section": "",
    "text": "2.1 CLRM Assumptions\nFor the Ordinary Least Squares (OLS) estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) to have desirable properties (like being the Best Linear Unbiased Estimators, or BLUE), a set of assumptions about the population model must hold. These are the core assumptions for cross-sectional data analysis.\nGiven that the population model is\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\nAssumption 1: Conditional Mean Zero\nThe error term \\(u\\) has an expected value of zero, given any value of the explanatory variable \\(X\\).\n\\[E(u_i | X_i) = 0\\]\nAssumption 2: Homoskedasticity\nThe error term \\(u\\) has the same variance given any value of the explanatory variable.\n\\[Var(u_i | X_i) = \\sigma^2\\]\nAssumption 3: No Autocorrelation\nThe error terms for any two different observations are uncorrelated.\n\\[Cov(u_i, u_j | X_i) = 0 \\quad \\text{for all } i \\neq j\\]\nAssumption 4: Exogeneity\nThe explanatory variable \\(X\\) is uncorrelated with the error term \\(u\\).\n\\[Cov(X_i, u_i) = 0\\]\nAssumption 5: No Perfect Multicollinearity",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Classical Linear Model (Gauss-Markov) Assumptions</span>"
    ]
  },
  {
    "objectID": "intro.html#the-r-squared-r2",
    "href": "intro.html#the-r-squared-r2",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "7.1 The R-Squared (\\(R^2\\))",
    "text": "7.1 The R-Squared (\\(R^2\\))\nThe most common measure of fit is the R-squared statistic. It represents the fraction of the sample variation in \\(Y\\) that is explained by \\(X\\).\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\\(R^2\\) always lies between 0 and 1.\nAn \\(R^2\\) of 0 means \\(X\\) explains none of the variation in \\(Y\\).\nAn \\(R^2\\) of 1 means \\(X\\) explains all of the variation in \\(Y\\).\nIn a simple regression, \\(R^2\\) is also the square of the correlation coefficient between \\(X\\) and \\(Y\\), that is,\\(R^2 = r_{xy}^2\\).\n\nIn our mtcars example, the \\(R^2\\) is 0.7528. This means that about 75% of the variation in miles per gallon is explained by the weight of the car.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#intuition-behind-the-ols-estimators",
    "href": "intro.html#intuition-behind-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "4.1 Intuition behind the OLS estimators",
    "text": "4.1 Intuition behind the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) aren’t arbitrary; they are the direct mathematical solution to the problem of minimizing the sum of squared residuals. But we can also understand them intuitively.\n\n4.1.1 Intuition for the Slope (\\(\\hat{\\beta}\\))\nLet’s look at the formula for the slope estimator more closely:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\n\nDividing the numerator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\), which is the sample covariance between \\(X\\) and \\(Y\\). Recall that the covariance measures how two variables move together:\n\nIf \\(X\\) is above its mean when \\(Y\\) is above its mean (and vice versa), the products \\((X_i - \\bar{X})(Y_i - \\bar{Y})\\) will be positive, leading to a positive covariance and a positive \\(\\hat{\\beta}\\).\nIf \\(X\\) is above its mean when \\(Y\\) is below its mean, the products will be negative, leading to a negative covariance and a negative \\(\\hat{\\beta}\\).\n\nNote also that dividing the denominator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2\\), which is the sample variance of \\(X\\). The variance measures the spread or variation of \\(X\\) around its own mean.\n\nSo, we can think of \\(\\hat{\\beta}\\) as: \\[\\hat{\\beta} = \\frac{\\text{Sample Covariance between X and Y}}{\\text{Sample Variance of X}}\\]\nIn other words, the OLS slope estimator answers the question: “For a given amount of movement in \\(X\\), how much associated movement do we see in \\(Y\\)?” It scales the co-movement of \\(X\\) and \\(Y\\) by the movement in \\(X\\) itself. A steeper slope (larger \\(|\\hat{\\beta}|\\)) means a unit change in \\(X\\) is associated with a larger change in \\(Y\\).\n\n\n4.1.2 Intuition for the Intercept (\\(\\hat{\\alpha}\\))\nThe formula for the intercept is: \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\nThis ensures that the regression line always passes through the point of the means \\((\\bar{X}, \\bar{Y})\\). Think of it as an “anchor” point for the line.\n\n\\(\\hat{\\beta}\\bar{X}\\) tells us where the regression line would predict \\(\\bar{Y}\\) to be based only on the average value of \\(X\\).\n\\(\\bar{Y} - \\hat{\\beta}\\bar{X}\\) is the adjustment needed so that the prediction is correct precisely at the means. It represents the predicted value of \\(Y\\) when \\(X = 0\\), which may or may not be a meaningful value depending on the context (e.g., predicting a company’s profit when revenue is zero might not be sensible).\n\n\n\n4.1.3 The Core Idea of “Least Squares”\nThe goal is to minimize the sum of squared residuals (\\(\\sum \\hat{u}_i^2\\)). Why squares? 1. Squaring penalizes large errors more severely than small errors. A residual of 2 is four times “worse” than a residual of 1 \\((2^2 = 4\\) vs. \\(1^2 = 1)\\). This makes the estimator very sensitive to outliers. 2. Squaring ensures all errors are positive. We don’t want positive and negative errors to cancel each other out. 3. The math works out nicely. Minimizing a quadratic function (like the sum of squares) leads to the clean, linear equations (“normal equations”) that give us the formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nThe OLS method therefore finds the unique line that minimizes the total squared vertical distance between the observed data points \\((X_i, Y_i)\\) and the line itself. It’s a best-fit line by its own specific definition of “best” (minimum sum of squared errors).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "assumptions.html#the-gauss-markov-theorem",
    "href": "assumptions.html#the-gauss-markov-theorem",
    "title": "2  The Classical Linear Model (Gauss-Markov) Assumptions",
    "section": "2.2 The Gauss-Markov Theorem",
    "text": "2.2 The Gauss-Markov Theorem\nIf Assumptions 1 through 4 hold (and 5 for multiple regression), the OLS estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the Best Linear Unbiased Estimators (BLUE).\n\nLinear: They are linear functions of the data.\nUnbiased: On average, across repeated samples, they equal the true population parameters, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nBest: They have the smallest variance among all other linear unbiased estimators. This means they are the most precise (efficient).\n\nThis theorem is why OLS is the workhorse of econometrics—under these conditions, no other linear estimator is better.\n\n2.2.1 Checking Assumptions\nWhile a full diagnostic check will be done in later chapters, we’ll start by checking for nonconstant variances, or heteroskedasticty.\n\n2.2.1.1 Testing for Heteroskedasticity\nHere is a quick example of how to generate residual plots whch can be useful to visually check for homoskedasticity and mean zero.\n\n# Fit the model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Create a dataframe of fitted values and residuals\ndiagnostic_data &lt;- data.frame(\n  fitted = fitted(model),\n  residuals = resid(model)\n)\n\n# Plot residuals vs. fitted values\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nggplot(diagnostic_data, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values (Y_hat)\",\n       y = \"Residuals (u_hat)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpreting the plot: We look for a random scatter of points around the red zero line. The absence of a clear pattern (e.g., a curve or a funnel) is a good sign that Assumptions 1 and 2 are plausible. In this mtcars example, there might be a slight pattern, suggesting a potential minor violation worth investigating further.\nA more formal test is the White test, a general and powerful test for heteroskedasticity. It does not assume a specific form for the heteroskedasticity (e.g., that variance increases with X). The test works by regressing the squared residuals from the original model on the original explanatory variables, their squares, and their cross-products.\nThe null and alternative hypotheses are:\n\\(H_0\\): Homoskedasticity exists (the error variance is constant).\n\\(H_1\\): Heteroskedasticity exists (the error variance is not constant).\n\n# install.packages(\"lmtest\") # Uncomment and run if needed\nlibrary(lmtest)\n\nWarning: package 'lmtest' was built under R version 4.4.3\n\n\nLoading required package: zoo\n\n\nWarning: package 'zoo' was built under R version 4.4.3\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# Perform the White test for the simple model mpg ~ wt\nbptest(model, ~ wt * hp + I(wt^2) + I(hp^2), data = mtcars)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 6.4892, df = 5, p-value = 0.2615\n\n\nThe bptest() output shows a BP test statistic and a p-value. A p-value &lt; 0.05 means you reject the null hypothesis of homoskedasticity. In the mtcars example above, we fail to reject the null hypothesis, suggesting that we have may not have heteroskedasticty.\nAltrrnatively:\n\n# 1. Estimate the original simple regression model\nsimple_model &lt;- lm(mpg ~ wt, data = mtcars)\n\n# 2. Obtain the squared residuals from the model\nsquared_residuals &lt;- resid(simple_model)^2\n\n# 3. Perform the \"auxiliary regression\" for the White test:\n# Regress the squared residuals on the original regressor (wt) and its square (wt²).\nwhite_aux_model &lt;- lm(squared_residuals ~ wt + I(wt^2), data = mtcars)\n\n# 4. Conduct an F-test on the auxiliary model.\n# The null hypothesis is that the coefficients on 'wt' and 'I(wt^2)' are zero.\n# install.packages(\"car\") # Uncomment and run if you don't have the 'car' package\nlibrary(car)\n\nlinearHypothesis(white_aux_model, c(\"wt=0\", \"I(wt^2)=0\"))\n\n\nLinear hypothesis test:\nwt = 0\nI(wt^2) = 0\n\nModel 1: restricted model\nModel 2: squared_residuals ~ wt + I(wt^2)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     31 4542.5                           \n2     29 4348.6  2    193.95 0.6467 0.5312\n\n\nWe look at the F-statistic and its p-value (Pr(&gt;F)). A p-value &lt; 0.05 provides evidence to reject the null hypothesis (\\(H_0\\)) of homoskedasticity, thereby suggesting the error variance is not constant and depends on weight (wt). Here, again Pr(&gt;F) is 0.5312, hence we fail to reject the null.\n\n\n2.2.1.2 Testing for Endogeneity\nEndogeneity occurs when an explanatory variable is correlated with the error term (\\(Cov(X, u) \\neq 0\\)), violating a key Gauss-Markov assumption (4). This often arises from:\n\nConfounding leading to omitted variable bias\nReverse causality\nMeasurement Error\nSimultaneity\n\nThe consequence is that the OLS estimator becomes biased and inconsistent.\nThe Logic of the Hausman Test\nThe test follows a straightforward logic:\n\nNull Hypothesis (\\(H_0\\)): The variable in question is exogenous (\\(Cov(X, u) = 0\\)). OLS is consistent and efficient.\nAlternative Hypothesis (\\(H_1\\)): The variable is endogenous (\\(Cov(X, u) \\neq 0\\)). OLS is inconsistent.\n\nThe test compares two estimators:\n\nOLS Estimator: Efficient (has the smallest possible variance) under \\(H_0\\), but inconsistent under \\(H_1\\).\nIV (Instrumental Variables) Estimator: Consistent under both \\(H_0\\) and \\(H_1\\), but inefficient (has larger variance) under \\(H_0\\).\n\nIf the variable is exogenous (\\(H_0\\) is true), the OLS and IV estimates should be similar. If they are significantly different, we have evidence that endogeneity is present (\\(H_1\\) is true).\nImplementing the Hausman Test in R: A Step-by-Step Guide\nThe test is implemented as a Durbin-Wu-Hausman test via a convenient auxiliary regression.\nPrerequisite: You must have at least one valid instrument for the potentially endogenous variable. A valid instrument must be:\n\nRelevant: Correlated with the endogenous variable.\nExogenous: Not correlated with the error term (\\(Cov(Z, u) = 0\\)).\n\nScenario: Suppose we fear that wt (weight) is endogenous in our model mpg ~ wt. Suppose we use hp (horsepower) as instruments.\nStep 1: Estimate the First Stage Regression\nRegress the potentially endogenous variable (hp) on all exogenous variables and the instruments.\n\n# First Stage: Regress the endogenous variable on instruments and other exogenous vars\nfirst_stage &lt;- lm(hp ~ wt + hp, data = mtcars)\n\n# Retrieve the residuals from the first stage\nfirst_stage_residuals &lt;- resid(first_stage)\n\n# Add these residuals to the original dataset for the next step\nmtcars$fs_resid &lt;- first_stage_residuals\n\nStep 2: Estimate the Auxiliary Regression Run the original model, but include the first-stage residuals as an additional regressor.\n\n# Auxiliary Regression: Original model + first stage residuals\nauxiliary_model &lt;- lm(mpg ~ wt + fs_resid, data = mtcars)\nsummary(auxiliary_model)\n\n\nCall:\nlm(formula = mpg ~ wt + fs_resid, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.28513    1.59870  23.322  &lt; 2e-16 ***\nwt          -5.34447    0.47605 -11.227 4.49e-12 ***\nfs_resid    -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nStep 3: Interpret the Result - The key is the t-test on the coefficient of the residual variable (fs_resid). - Null Hypothesis (\\(H_0\\)): The coefficient on the residuals is zero. This means the variable (wt) is exogenous. - Alternative Hypothesis (\\(H_1\\)): The coefficient on the residuals is not zero. This is evidence of endogeneity.\nA low p-value (typically &lt; 0.05) on the fs_resid coefficient leads to a rejection of the null hypothesis, suggesting that wt is indeed endogenous.\nUsing the ivreg and lmtest packages\nA more efficient method is to use the ivreg() function from the AER package and then formally test for endogeneity.\nSpecifying a model with instruments: The syntax y ~ x1 + x2 | z1 + z2 means that we are regressing y on x1 and x2, using for x2 instruments z1 and z2 (and x1 is included as its own instrument).\n\n# install.packages(\"AER\") # Install the Applied Econometrics with R package\nlibrary(AER)\nlibrary(lmtest)\n\n# 1. Estimate the model via IV and OLS\n# IV model: Specify the formula and instruments\niv_model &lt;- ivreg(mpg ~ wt | hp, data = mtcars)\n\n# 2. Perform the Hausman test\n# The null is that OLS is consistent (no endogeneity)\n\n# Run summary with diagnostics\nsummary(iv_model, diagnostics = TRUE)\n\n\nCall:\nivreg(formula = mpg ~ wt | hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.634 -2.428 -1.063  2.291 10.052 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   43.440      3.282  13.237 4.62e-14 ***\nwt            -7.258      1.001  -7.252 4.50e-08 ***\n\nDiagnostic tests:\n                 df1 df2 statistic  p-value    \nWeak instruments   1  30     23.00 4.15e-05 ***\nWu-Hausman         1  29     12.38  0.00145 ** \nSargan             0  NA        NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.591 on 30 degrees of freedom\nMultiple R-Squared: 0.6564, Adjusted R-squared: 0.6449 \nWald test: 52.59 on 1 and 30 DF,  p-value: 4.497e-08 \n\n\nInterpreting the Test:\nWeak Instruments Test\nWhat it tests: Whether the instrument(s) (hp) is sufficiently correlated with the endogenous regressor (wt). A strong first-stage relationship is crucial for IV to work.\nInterpretation: The null hypothesis is that the instruments are weak.\nThe result (p-value = 4.15e-05), an extremely low p-value, suggests that we should strongly reject the null. The instrument(s) are not weak; they are strong and relevant. This is a good sign.\nWu-Hausman Test (for Endogeneity)\nWhat it tests: This is the test for endogeneity. The null hypothesis (\\(H_0\\)) is that the variable (wt) is exogenous. The alternative (\\(H_1\\)) is that it is endogenous.\nInterpretation: A low p-value suggests you should reject \\(H_0\\) and use the IV estimator. A high p-value means you cannot reject \\(H_0\\) and should prefer the efficient OLS estimator.\nThe result (p-value = 0.0014), a low p-value means we should reject the null hypothesis. There is statistical evidence that wt is endogenous. Therefore, the standard OLS estimates for mpg ~ wt gives biased etimates.\nSargan Test (for Overidentifying Restrictions)\nWhat it tests: This test checks the validity of your overidentifying instruments. It is only relevant if you have more instruments than endogenous variables. The null hypothesis (\\(H_0\\)) is that the extra instruments are valid (uncorrelated with the error term).\nInterpretation: A low p-value is undesirable, as it means you should reject \\(H_0\\) and suspect that at least one of your extra instruments is invalid.\nWhat to Do If You Find Endogeneity?\nIf the test suggests endogeneity, you should not trust the OLS results. You must use a method that addresses the endogeneity, such as:\n\nFinding Better Controls: If the endogeneity is from omitted variable bias.\nInstrumental Variables (IV) Regression: The primary solution, implemented with ivreg().\nUsing Panel Data Methods: Such as fixed effects models, if you have panel data.\n\nImportant: The validity of the Hausman test hinges on the quality of your instruments. If your instruments are weak or invalid, the test itself is unreliable. Always check the first-stage F-statistic to ensure instrument strength (a rule of thumb is F-stat &gt; 10).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Classical Linear Model (Gauss-Markov) Assumptions</span>"
    ]
  },
  {
    "objectID": "multiple.html",
    "href": "multiple.html",
    "title": "3  The Multiple Regression Model",
    "section": "",
    "text": "3.1 The Trivariate Model & Interpretation\nThe population multiple regression model with two explanatory variables (the trivariate model) is written as:\n\\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\]\nTaking expectations on both sides gives the conditional mean function:\n\\[E(Y_i | X_{1i}, X_{2i}) = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i}\\]\nThe key advantage of multiple regression is the ceteris paribus (all else equal) interpretation of its coefficients.\nThis holding-constant effect is what helps isolate the direct effect of one variable, controlling for the influence of others.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-population-model-interpretation",
    "href": "multiple.html#the-population-model-interpretation",
    "title": "3  The Multiple Regression Model",
    "section": "",
    "text": "\\(\\beta_1\\) is the marginal effect of \\(X_1\\) on \\(Y\\). It is the effect of a small change in the \\(X_1\\) on the dependent variable, while holding \\(X_2\\) constant.\n\\(\\beta_2\\) is the marginal effect of \\(X_2\\) on \\(Y\\), while holding \\(X_1\\) constant.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-ols-estimators-and-normal-equations",
    "href": "multiple.html#the-ols-estimators-and-normal-equations",
    "title": "3  The Multiple Regression Model",
    "section": "3.2 The OLS Estimators and Normal Equations",
    "text": "3.2 The OLS Estimators and Normal Equations\nThe sample regression function (SRF) is:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i}\\] \\[Y_i = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i} + \\hat{u}_i\\]\nAs in the bivariate case, the OLS procedure consists of choosing the unknown parameters \\((\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2)\\) such that the residual sum of squares (SSR) is minimized:\n\\[\\min_{\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2} \\sum_{i=1}^n \\hat{u}_i^2 = \\min_{\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2} \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta}_1 X_{1i} - \\hat{\\beta}_2 X_{2i})^2\\]\nDifferentiating and setting the derivatives to zero yields the system of normal equations:\n\\[\\begin{aligned}\n\\sum Y_i &= n\\hat{\\alpha} + \\hat{\\beta}_1 \\sum X_{1i} + \\hat{\\beta}_2 \\sum X_{2i} \\\\\n\\sum X_{1i}Y_i &= \\hat{\\alpha} \\sum X_{1i} + \\hat{\\beta}_1 \\sum X_{1i}^2 + \\hat{\\beta}_2 \\sum X_{1i}X_{2i} \\\\\n\\sum X_{2i}Y_i &= \\hat{\\alpha} \\sum X_{2i} + \\hat{\\beta}_1 \\sum X_{1i}X_{2i} + \\hat{\\beta}_2 \\sum X_{2i}^2\n\\end{aligned}\\]\nSolving this system of three equations provides the formulas for the OLS estimators \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}_1\\), and \\(\\hat{\\beta}_2\\). While the formulas are more complex than in the simple regression case, the intuition is similar.\nHence, the OLS estimators for the slope coefficients in a multiple regression model \\(Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\) are given by:\n\\[\n\\begin{align*}\n\\hat{\\beta}_1&=\\frac{(\\sum_{i=1}^n x_{1i}y_i)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{2i}y_i)(\\sum_{i=1}^n x_{1i}x_{2i})}{(\\sum_{i=1}^n x_{1i}^2)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{1i}x_{2i})^2}\\\\\n\\hat{\\beta}_2&=\\frac{(\\sum_{i=1}^n x_{2i}y_i)(\\sum_{i=1}^n x_{1i}^2)-(\\sum_{i=1}^n x_{1i}y_i)(\\sum_{i=1}^n x_{1i}x_{2i})}{(\\sum_{i=1}^n x_{1i}^2)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{1i}x_{2i})^2}\n\\end{align*}\n\\] And the estimator for the intercept is:\n\\[\n\\hat\\alpha = \\bar Y - \\hat\\beta_1 \\bar X_1 - \\hat\\beta_2 \\bar X_2\n\\]\n\n3.2.1 Extending the mtcars Model\nWe can estimate the coefficients easily using R. Let’s first expand our car mileage model. Perhaps a car’s horsepower (hp) also affects its fuel efficiency (mpg), in addition to its weight (wt). We can estimate this trivariate model.\n\n# Estimate a multiple regression model\n# mpg = alpha + beta1 * wt + beta2 * hp + u\nmulti_model &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Print the summary results\nsummary(multi_model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nInterpretation of the Coefficients:\n\nwt coefficient (-3.878): Holding horsepower constant, a one-ton increase in weight is associated with a decrease in fuel efficiency of approximately 3.88 miles per gallon, on average.\nhp coefficient (-0.032): Holding weight constant, a one-horsepower increase is associated with a decrease in fuel efficiency of approximately 0.032 miles per gallon, on average.\nIntercept (37.227): The predicted miles per gallon for a car that weighs 0 tons and has 0 horsepower. (Note: This is often not a meaningful value and serves just as a baseline for the regression line.)\n\nNotice how the coefficient on wt changed from -5.34 in the simple regression to -3.88 in this multiple regression. This suggests that horsepower was an omitted variable that was correlated with both weight and mileage, and failing to control for it biased our initial estimate of the effect of weight.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#standard-errors-of-the-ols-estimates",
    "href": "multiple.html#standard-errors-of-the-ols-estimates",
    "title": "3  The Multiple Regression Model",
    "section": "3.3 Standard Errors of the OLS Estimates",
    "text": "3.3 Standard Errors of the OLS Estimates\nThe formula for the variance of a slope estimator, say \\(\\hat{\\beta}_1\\), in the trivariate model is:\n\\[Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum (X_{1i} - \\bar{X}_1)^2 (1 - r^2_{12})}\\]\n\n\\(\\sigma^2\\) is the variance of the error term \\(u_i\\).\n\\(\\sum (X_{1i} - \\bar{X}_1)^2\\) is the total variation in \\(X_1\\).\n\\(r_{12}\\) is the sample correlation between \\(X_1\\) and \\(X_2\\).\n\nSince we never observe the true error variance (\\(\\sigma^2\\)), we bootstrap and use its estimate:\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2\\]\nwhere \\(k\\) is the number of estimated coefficients (including the constant, so \\(k=3\\) for our model). The standard error of the coefficient is then the square root of its estimated variance: \\(SE(\\hat{\\beta}_1) = \\sqrt{\\widehat{Var}(\\hat{\\beta}_1)}\\).\nHence, the standard errors for \\(\\beta 's\\) in the model \\(Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\) are given by:\n\\[\n\\begin{aligned}\nSE(\\hat{\\beta}_1) &= \\sqrt{\\widehat{Var}(\\hat{\\beta}_1)} \\quad  \\text{or} \\quad \\hat{\\sigma} \\frac{1}{\\sqrt{\\sum x_{1i}^2 (1 - r_{12}^2)}} \\\\\nSE(\\hat{\\beta}_2) &= \\sqrt{\\widehat{Var}(\\hat{\\beta}_2)} \\text{or} \\quad \\hat{\\sigma} \\frac{1}{\\sqrt{\\sum x_{2i}^2 (1 - r_{12}^2)}}\n\\end{aligned}\n\\]\nThe standard error for the intercept is:\n\\[\nSE(\\hat{\\alpha}) = \\sqrt{\\widehat{Var}(\\hat{\\alpha})} = \\hat{\\sigma} \\sqrt{ \\frac{1}{n} + \\frac{\\overline{X}_1^2 \\sum x_{2i}^2 + \\overline{X}_2^2 \\sum x_{1i}^2 - 2\\overline{X}_1 \\overline{X}_2 \\sum x_{1i} x_{2i}}{\\sum x_{1i}^2 \\sum x_{2i}^2 - (\\sum x_{1i} x_{2i})^2} }\n\\]\nwhere:\n\n\\(\\hat{\\sigma} = \\sqrt{\\frac{\\sum \\hat{u}_i^2}{n - k}}\\) is the standard error of the regression (\\(SER\\)),\n\\(r_{12}\\) is the sample correlation between \\(X_1\\) and \\(X_2\\),\n\\(x_{1i} = X_{1i} - \\bar{X}_1\\) and \\(x_{2i} = X_{2i} - \\bar{X}_2\\) are deviations\n\nThe summary() function in R displays these standard errors, as seen in the output above.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#r-squared-and-the-adjusted-r-squared",
    "href": "multiple.html#r-squared-and-the-adjusted-r-squared",
    "title": "3  The Multiple Regression Model",
    "section": "3.4 R-squared and The Adjusted R-squared",
    "text": "3.4 R-squared and The Adjusted R-squared\nAs in the bivariate case, the coefficient of determination, \\(R^2\\), is the fraction of the sample variation in \\(Y\\) explained by the model:\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\nA key feature of multiple regression is that adding any new variable (even an irrelevant one) will never decrease the \\(R^2\\). Because of this, we often prefer the adjusted R-squared, which penalizes for adding irrelevant variables.\n\\[\\bar{R}^2 = 1 - \\frac{SSR/(n-k)}{SST/(n-1)} = 1 - \\left( \\frac{n-1}{n-k} \\right) \\frac{SSR}{SST}\\]\n\n\\(n\\) is the sample size.\n\\(k\\) is the number of coefficients, including the constant.\n\\(\\bar{R}^2\\) can decrease if a new variable adds little explanatory power, providing a better gauge of whether a variable should be included.\nAlways compare \\(\\bar{R}^2\\), not \\(R^2\\), when models have a different number of predictors.\n\nIn our mtcars output, we see both R-squared:  0.8268 and Adjusted R-squared:  0.8148.\n\n3.4.1 Interpreting \\(R^2\\) and \\(\\bar{R}^2\\) in Practice\nIt is critical to remember that: 1. An increase in the \\(R^2\\) or \\(\\bar{R}^2\\) does not necessarily mean that an added variable is statistically significant. 2. A high \\(R^2\\) or \\(\\bar{R}^2\\) does not mean that the regressors are a true cause of the dependent variable (causation vs. correlation). 3. A high \\(R^2\\) or \\(\\bar{R}^2\\) does not mean that there is no omitted variable bias. 4. A high \\(R^2\\) or \\(\\bar{R}^2\\) does not necessarily mean that you have the most appropriate set of regressors, nor does a low \\(R^2\\) or \\(\\bar{R}^2\\) mean that you have an inappropriate model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#hypothesis-testing",
    "href": "multiple.html#hypothesis-testing",
    "title": "3  The Multiple Regression Model",
    "section": "3.5 Hypothesis Testing",
    "text": "3.5 Hypothesis Testing\n\n3.5.1 Testing Individual Coefficients\nThe procedure for testing hypotheses about a single coefficient is identical to the simple regression case. For example, to test if \\(X_1\\) has a significant effect on \\(Y\\) after controlling for \\(X_2\\):\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_1: \\beta_1 \\neq 0\\)\n\nThe test statistic is the t-ratio: \\(t = \\displaystyle\\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}\\).\nAs a rule of thumb, we reject the null hypothesis if the absolute value of the t-ratio is greater than “2”.\n\n\n3.5.2 Testing the Overall Significance: The F-Test\nThe test for the overall significance of the regression is a joint test that all slope coefficients are equal to zero:\n\\[\nH_0: \\beta_1 = \\beta_2 = 0 \\quad \\text{vs.} \\quad H_1: \\text{at least one } \\beta_j \\neq 0\n\\]\nThis test is conducted using the F-statistic. The F-statistic can be computed in several equivalent forms:\n1. Using sums of squares and cross-products: \\[\nF = \\frac{ \\left( \\hat{\\beta}_1 \\sum y_i x_{1i} + \\hat{\\beta}_2 \\sum y_i x_{2i} \\right) / m }{ \\sum \\hat{u}_i^2 / (n - k) }\n\\] where\n\n\\(m\\) is the number of restrictions\n\\(n-k\\) is the degree of freedom of the regression\n\nThe above is the ratio of explained and unexplained sums of squares divided by their respective degrees of freedom which corresponds to ANOVA:\n\\[\nF = \\frac{SSE / (k - 1)}{SSR / (n - k)}\n\\] where \\(k\\) is the total number of estimated parameters (for a model with two regressors, \\(k=3\\): \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\)).\nNote this is the same if we use the coefficient of determination (\\(R^2\\)):\n\\[\nF = \\frac{R^2 / (k - 1)}{(1 - R^2) / (n - k)}\n\\]\nHence, the new test in multiple regression is the test for overall significance of the regression, which is a joint test that all slope coefficients are simultaneously equal to zero.\n\n\\(H_0: \\beta_1 = \\beta_2 = 0\\)\n\\(H_1: \\text{At least one } \\beta_j \\neq 0\\)\n\nThis test is given by the F-statistic, which is reported in the standard regression output. The F-statistic is constructed using the sums of squares from the ANOVA (Analysis of Variance) framework:\n\\[SST = SSE + SSR\\]\nThe F-statistic is the ratio of the explained to unexplained variance, adjusted for degrees of freedom:\n\\[F = \\frac{SSE / (k-1)}{SSR / (n-k)} = \\frac{MSR}{MSE}\\]\nwhere \\(k-1\\) is the number of slope coefficients. A large F-statistic provides evidence against the null hypothesis that the model provides no better fit than a model with only an intercept.\n\n\n3.5.3 Joint Tests and Restricted Models\nThe F-test can be generalized to test any set of linear restrictions. For example, we can test if a subset of coefficients is equal to zero.\nAssume the unrestricted model is:\n\\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + u_i\\]\nWe could test the joint hypothesis:\n\n\\(H_0: \\beta_2 = 0, \\beta_3 = 0\\)\n\\(H_1: H_0 \\text{ is not true}\\)\n\nThis involves estimating a restricted model where the restrictions under the null are imposed: \\[Y_i = \\alpha + \\beta_1 X_{1i} + u_i\\]\nThe general F-statistic formula for testing \\(m\\) restrictions is:\n\\[F = \\frac{(SSR_r - SSR_{ur}) / m}{SSR_{ur} / (n - k)}\\]\nwhere:\n\n\\(SSR_r\\) is the sum of squared residuals from the restricted model.\n\\(SSR_{ur}\\) is the sum of squared residuals from the unrestricted model.\n\\(m\\) is the number of restrictions.\n\\(n - k\\) is the degrees of freedom in the unrestricted model.\n\nHere is another example where We could test the joint hypothesis:\n\n\\(H_0: \\beta_1 = 0, \\beta_2 + \\beta_3 = 1\\)\n\\(H_1: H_0 \\text{ is not true}\\)\n\nHere, the unrestricted model is the same as before:\n\\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + u_i\\]\nTo find the restricted model, we impose the null hypothesis:\nSubstitute \\(\\beta_1 = 0\\) and \\(\\beta_3 = 1 - \\beta_2\\) into the model:\n\\[Y_i = \\alpha + \\beta_2 X_{2i} + (1 - \\beta_2) X_{3i} + u_i\\]\nRearrange the equation:\n\\[Y_i = \\alpha + X_{3i} + \\beta_2 (X_{2i} - X_{3i}) + u_i\\]\nDefine a new dependent variable \\(Y_i^* = Y_i - X_{3i}\\):\n\\[Y_i^* = \\alpha + \\beta_2 (X_{2i} - X_{3i}) + u_i\\]\nWe then perform the F-test as usual and a low p-value would lead to a rejection of the null hypothesis \\(H_0\\). Note here that \\(m\\) the number of restrictions is 2 and not 3!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-problem-of-omitted-variable-bias",
    "href": "multiple.html#the-problem-of-omitted-variable-bias",
    "title": "3  The Multiple Regression Model",
    "section": "3.6 The Problem of Omitted Variable Bias",
    "text": "3.6 The Problem of Omitted Variable Bias\nOften, we are interested in understanding the relation between two variables (\\(Y\\) and \\(X\\)). But running a simple regression of \\(Y\\) on \\(X\\) might not be enough. The assumption \\(Cov(X, u)=0\\) might be violated if the error term \\(u\\) contains a variable that is correlated with \\(X\\), thereby introducing a bias. Multiple regression is a primary tool to help resolve this endogeneity problem.\n\n3.6.1 Direction of the Bias\nOmitting an important variable introduces a bias to the OLS estimator. The direction of this bias can be formalized.\nSuppose the true model is: \\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\] But we incorrectly estimate the misspecified model: \\[Y_i = \\alpha + \\beta_1 X_{1i} + v_i \\quad \\text{where} \\quad v_i = \\beta_2 X_{2i} + u_i\\]\nThe bias in the simple regression estimator \\(\\tilde{\\beta}_1\\) is: \\[Bias(\\tilde{\\beta}_1) = E[\\tilde{\\beta}_1] - \\beta_1 = \\beta_2 \\cdot \\tilde{\\delta}_1\\]\nwhere \\(\\tilde{\\delta}_1\\) is the slope coefficient from an auxiliary regression of the omitted variable (\\(X_2\\)) on the included variable (\\(X_1\\)): \\[X_{2i} = \\delta_0 + \\delta_1 X_{1i} + e_i\\]\n\nThe sign of the bias depends on the signs of \\(\\beta_2\\) (the effect of the omitted variable on \\(Y\\)) and \\(\\tilde{\\delta}_1\\) (the correlation between \\(X_2\\) and \\(X_1\\)).\nThe size of the bias depends on the magnitude of \\(\\beta_2\\) and \\(\\tilde{\\delta}_1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-cost-of-including-an-irrelevant-variable",
    "href": "multiple.html#the-cost-of-including-an-irrelevant-variable",
    "title": "3  The Multiple Regression Model",
    "section": "3.7 The Cost of Including an Irrelevant Variable",
    "text": "3.7 The Cost of Including an Irrelevant Variable\nConversely, including a variable that does not belong in the true model (i.e., whose true coefficient is zero) has different consequences.\nSuppose the true model is: \\[Y_i = \\alpha + \\beta_1 X_{1i} + u_i\\] But we incorrectly estimate: \\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + v_i\\]\n\nDoes it introduce bias? No. The OLS estimators for all coefficients, including \\(\\hat{\\beta}_1\\), remain unbiased.\nWhat is the cost? Increased variance. The estimates become less precise. The standard error of \\(\\hat{\\beta}_1\\) will generally be larger than it would be in the correctly specified simple regression model, leading to less powerful hypothesis tests and wider confidence intervals.\n\nThe trade-off is clear: omitting a relevant variable causes bias, while including an irrelevant variable reduces efficiency. When in doubt, it is often less harmful to include a potentially irrelevant variable than to omit a potentially relevant one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "intro.html#introduction",
    "href": "intro.html#introduction",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "What is the effect of reducing class size on student academic performance?\nWhat is the price elasticity of cigarettes?\nWhat is the return to an additional year of education?\nHow does a 1 percentage point increase in interest rates affect output growth?\n\n\n\n\nOmitted Variable Bias (Confounding Factors): A variable we have not accounted for is influencing both the dependent and independent variable.\nSimultaneous Causality: Two variables influence each other simultaneously (e.g., police numbers and crime rates).\nSample Selection Bias: The process by which data is collected influences the availability of data, leading to a non-random sample that may not represent the population of interest.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-data",
    "href": "intro.html#types-of-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.2 Types of Data",
    "text": "1.2 Types of Data\nBefore we begin, it’s useful to recognize the common structures of econometric data:\n\nCross-sectional: Data on multiple entities (individuals, firms, countries) at a single point in time.\nTime-series: Data on a single entity collected at multiple time periods (e.g., daily, quarterly, yearly).\nPanel/Longitudinal: Data on multiple entities where each entity is observed at multiple time periods. This combines cross-sectional and time-series dimensions.\nPooled Cross-sectional: Multiple cross-sectional samples taken at different points in time, where the entities in each sample are different.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-simple-regression-model",
    "href": "intro.html#the-simple-regression-model",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.3 The Simple Regression Model",
    "text": "1.3 The Simple Regression Model\nLet’s begin by investigating the linear relationship between two variables, \\(Y\\) (the dependent variable) and \\(X\\) (the independent or explanatory variable).\n\n1.3.1 The Population Regression Function (PRF)\nImagine we could collect data on everyone in the population of interest. The true relationship in the population is given by the Population Regression Function:\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\n\n\\(Y_i\\) is the dependent variable for observation \\(i\\).\n\\(X_i\\) is the independent variable for observation \\(i\\).\n\\(\\alpha\\) is the population intercept.\n\\(\\beta\\) is the population slope coefficient (the parameter of primary interest).\n\\(u_i\\) is the error term, which contains all factors other than \\(X\\) that influence \\(Y\\).\n\nWe can never observe the true PRF because we cannot collect data on the entire population. The error term \\(u_i\\) exists due to: (1) The inherent randomness of human behavior, (2) Unavailable or incomplete data, (3) Omitted variables from the model, (4) Imperfect functional form specification, (5) Aggregation errors, (6) Measurement errors.\n\n\n1.3.2 The Sample Regression Function (SRF)\nSince we can’t work with the population, we take a sample and use it to estimate the PRF. The estimated model is called the Sample Regression Function:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\\]\n\\[Y_i = \\hat{\\alpha} + \\hat{\\beta} X_i + \\hat{u}_i\\]\n\n\\(\\hat{Y}_i\\) is the predicted or fitted value of \\(Y_i\\).\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the estimators of the population parameters \\(\\alpha\\) and \\(\\beta\\). These coefficients are calculated from our sample data.\n\\(\\hat{u}_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\), which is our estimate of the unobserved error term \\(u_i\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-ordinary-least-squares-ols-method",
    "href": "intro.html#the-ordinary-least-squares-ols-method",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.4 The Ordinary Least Squares (OLS) Method",
    "text": "1.4 The Ordinary Least Squares (OLS) Method\nHow do we find the “best” line through our scatter of data points? The OLS method chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimizes the Sum of Squared Residuals (SSR).\n\\[\\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n \\hat{u}_i^2 = \\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\nThe formulas for the OLS estimators, derived by solving this minimization problem (the “normal equations”), are:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]\n\\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\n\n1.4.1 Intuition behind the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) aren’t arbitrary; they are the direct mathematical solution to the problem of minimizing the sum of squared residuals. But we can also understand them intuitively.\n\n1.4.1.1 Intuition for the Slope (\\(\\hat{\\beta}\\))\nLet’s look at the formula for the slope estimator more closely:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\n\nDividing the numerator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\), which is the sample covariance between \\(X\\) and \\(Y\\). Recall that the covariance measures how two variables move together:\n\nIf \\(X\\) is above its mean when \\(Y\\) is above its mean (and vice versa), the products \\((X_i - \\bar{X})(Y_i - \\bar{Y})\\) will be positive, leading to a positive covariance and a positive \\(\\hat{\\beta}\\).\nIf \\(X\\) is above its mean when \\(Y\\) is below its mean, the products will be negative, leading to a negative covariance and a negative \\(\\hat{\\beta}\\).\n\nNote also that dividing the denominator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2\\), which is the sample variance of \\(X\\). The variance measures the spread or variation of \\(X\\) around its own mean.\n\nSo, we can think of \\(\\hat{\\beta}\\) as: \\[\\hat{\\beta} = \\frac{\\text{Sample Covariance between X and Y}}{\\text{Sample Variance of X}}\\]\nIn other words, the OLS slope estimator answers the question: “For a given amount of movement in \\(X\\), how much associated movement do we see in \\(Y\\)?” It scales the co-movement of \\(X\\) and \\(Y\\) by the movement in \\(X\\) itself. A steeper slope (larger \\(|\\hat{\\beta}|\\)) means a unit change in \\(X\\) is associated with a larger change in \\(Y\\).\n\n\n1.4.1.2 Intuition for the Intercept (\\(\\hat{\\alpha}\\))\nThe formula for the intercept is: \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\nThis ensures that the regression line always passes through the point of the means \\((\\bar{X}, \\bar{Y})\\). Think of it as an “anchor” point for the line.\n\n\\(\\hat{\\beta}\\bar{X}\\) tells us where the regression line would predict \\(\\bar{Y}\\) to be based only on the average value of \\(X\\).\n\\(\\bar{Y} - \\hat{\\beta}\\bar{X}\\) is the adjustment needed so that the prediction is correct precisely at the means. It represents the predicted value of \\(Y\\) when \\(X = 0\\), which may or may not be a meaningful value depending on the context (e.g., predicting a company’s profit when revenue is zero might not be sensible).\n\n\n\n1.4.1.3 The Core Idea of “Least Squares”\nThe goal is to minimize the sum of squared residuals (\\(\\sum \\hat{u}_i^2\\)). Why squares? 1. Squaring penalizes large errors more severely than small errors. A residual of 2 is four times “worse” than a residual of 1 \\((2^2 = 4\\) vs. \\(1^2 = 1)\\). This makes the estimator very sensitive to outliers. 2. Squaring ensures all errors are positive. We don’t want positive and negative errors to cancel each other out. 3. The math works out nicely. Minimizing a quadratic function (like the sum of squares) leads to the clean, linear equations (“normal equations”) that give us the formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nThe OLS method therefore finds the unique line that minimizes the total squared vertical distance between the observed data points \\((X_i, Y_i)\\) and the line itself. It’s a best-fit line by its own specific definition of “best” (minimum sum of squared errors).\n\n\n\n1.4.2 Fitting an OLS Model in R\nLet’s use the mtcars dataset to estimate a simple regression model, predicting miles per gallon (mpg) using car weight (wt).\n\n# Load the built-in dataset\ndata(mtcars)\n\n# Estimate the OLS model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print a summary of the results\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe output shows our estimated coefficients is \\(\\hat{\\alpha}\\) (Intercept) = 37.29 and \\(\\hat{\\beta}\\) (wt) = -5.34\nThis gives us the Sample Regression Line\n\\[\\widehat{mpg}_i = 37.29 - 5.34 \\times wt_i\\]\nHence, for a one-ton increase in car weight, we predict miles per gallon will decrease by about 5.34 units.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#derivation-of-the-ols-estimators",
    "href": "intro.html#derivation-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.5 Derivation of the OLS Estimators",
    "text": "1.5 Derivation of the OLS Estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are derived by solving the minimization problem of the Sum of Squared Residuals (SSR). This process involves calculus, specifically taking derivatives and setting them to zero to find the minimum. The resulting equations are called the normal equations.\n\n1.5.1 The Minimization Problem\nWe aim to find the values of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimize: \\[S(\\hat{\\alpha}, \\hat{\\beta}) = \\sum_{i=1}^n \\hat{u}_i^2 = \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\n\n\n1.5.2 The Normal Equations\nTo find the minimum, we take the partial derivatives of \\(S(\\hat{\\alpha}, \\hat{\\beta})\\) with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and set them equal to zero.\n\nDerivative with respect to \\(\\hat{\\alpha}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\alpha}} = -2 \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the first normal equation: \\[\\sum_{i=1}^n Y_i = n\\hat{\\alpha} + \\hat{\\beta} \\sum_{i=1}^n X_i \\quad \\text{(1)}\\]\nDerivative with respect to \\(\\hat{\\beta}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\beta}} = -2 \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the second normal equation: \\[\\sum_{i=1}^n X_iY_i = \\hat{\\alpha} \\sum_{i=1}^n X_i + \\hat{\\beta} \\sum_{i=1}^n X_i^2 \\quad \\text{(2)}\\]\n\n\n\n1.5.3 Solving the Normal Equations\nWe now have a system of two equations with two unknowns (\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)).\n\nSolving for \\(\\hat{\\alpha}\\): Start by rearranging the first normal equation (1): \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] where \\(\\bar{Y} = \\frac{1}{n}\\sum Y_i\\) and \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\). This is our formula for the intercept.\nSolving for \\(\\hat{\\beta}\\): Substitute the expression for \\(\\hat{\\alpha}\\) into the second normal equation (2): \\[\\sum X_iY_i = (\\bar{Y} - \\hat{\\beta}\\bar{X})\\sum X_i + \\hat{\\beta} \\sum X_i^2\\] Solving this for \\(\\hat{\\beta}\\) involves some algebra. Subtract \\(\\bar{Y}\\sum X_i\\) from both sides and factor out \\(\\hat{\\beta}\\): \\[\\sum X_iY_i - \\bar{Y}\\sum X_i = \\hat{\\beta} \\left( \\sum X_i^2 - \\bar{X}\\sum X_i \\right)\\] Note that \\(\\sum X_iY_i - \\bar{Y}\\sum X_i = \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\) and \\(\\sum X_i^2 - \\bar{X}\\sum X_i = \\sum (X_i - \\bar{X})^2\\). This gives us the final formula: \\[\\hat{\\beta} = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#standard-errors-of-the-ols-estimators",
    "href": "intro.html#standard-errors-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.6 Standard Errors of the OLS Estimators",
    "text": "1.6 Standard Errors of the OLS Estimators\nThe OLS estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random variables—their values vary from sample to sample. The standard error measures the precision of these estimates by estimating the standard deviation of their sampling distributions. Smaller standard errors indicate more precise estimates.\n\n1.6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)\nTo conduct statistical inference on our OLS estimate \\(\\hat{\\beta}\\) (e.g., to build confidence intervals or test hypotheses), we need to estimate its sampling variability. This variability is measured by its variance or, more commonly, its standard error.\n\n1.6.1.1 The True Variance of \\(\\hat{\\beta}\\)\nUnder the classical linear model assumptions, the true variance of the OLS slope estimator in a simple regression is given by:\n\\[Var(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nwhere, \\(\\sigma^2 = Var(u_i)\\) is the variance of the unobserved error term, and \\(\\sum_{i=1}^n (X_i - \\bar{X})^2\\) is the total variation of the independent variable \\(X\\) around its mean.\nThis formula shows that the precision of \\(\\hat{\\beta}\\) improves (its variance decreases) when either (1) the error variance (\\(\\sigma^2\\)) is smaller (the data points are tighter around the line, and/or (2) the spread of the explanatory variable \\(X\\) is larger (there is more information in the data).\n\n\n1.6.1.2 Estimating the Unknown Error Variance (\\(\\sigma^2\\))\nSince the error variance \\(\\sigma^2\\) is unknown, we must estimate it using the sample data. An unbiased estimator for \\(\\sigma^2\\) is\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2   \\quad \\text{or}   \\quad \\frac{SSR}{n - k}\\]\nwhere, \\(SSR = \\sum_{i=1}^n \\hat{u}_i^2\\) is the Sum of Squared Residuals (SSR), \\(n\\) is the sample size, and \\(k\\) is the total number of parameters estimated. In a simple regression, we estimate two parameters, i.e. the slope (\\(\\beta\\)) and the intercept (\\(\\alpha\\)), so \\(k=2\\). The term \\(n - k\\) is the degrees of freedom. Using \\(n-k\\) instead of \\(n\\) ensures that \\(E[\\hat{\\sigma}^2] = \\sigma^2\\), making it an unbiased estimator.\n\n\n1.6.1.3 The Standard Error of the Regression (SER)\nThe square root of \\(\\hat{\\sigma}^2\\) is called the Standard Error of the Regression (SER) or the residual standard error. It is an estimate of the standard deviation of the error term \\(u_i\\) and represents the average distance that the observed values fall from the regression line—the typical size of a residual.\n\\[SER = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{SSR}{n - k}}\\]\n\n\n1.6.1.4 The Estimated Variance and Standard Error of \\(\\hat{\\beta}\\)\nBy plugging the estimate \\(\\hat{\\sigma}^2\\) into the true variance formula, we obtain the estimated variance of the OLS estimator\n\\[\\widehat{Var}(\\hat{\\beta}) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nThe standard error of \\(\\hat{\\beta}\\) is simply the square root of this estimated variance. It is the estimated standard deviation of the sampling distribution of \\(\\hat{\\beta}\\).\n\\[SE(\\hat{\\beta}) = \\sqrt{\\widehat{Var}(\\hat{\\beta})} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}} \\quad \\text{or} \\quad \\hat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n x^2}}\\]\nThis final formula is the most intuitive: the standard error of the coefficient depends directly on the “noise” in the model (\\(SER\\)) and inversely on the amount of information in the explanatory variable, i.e. \\(\\sqrt{\\sum_{i=1}^n x^2}\\).\n\n\n\n1.6.2 What Drives the Standard Error?\nThe formula for \\(SE(\\hat{\\beta})\\) provides deep intuition about what makes an estimate precise: 1. Spread of the error term (\\(\\hat{\\sigma}\\)): A larger error variance (a noisier relationship, where points are scattered farther from the line) leads to a larger standard error and less precise estimates. 2. Sample size (\\(n\\)): A larger sample size \\(n\\) will (all else equal) make \\(\\hat{\\sigma}\\) smaller and the denominator larger, leading to a smaller standard error and more precise estimates. 3. Spread of the regressor \\(X\\) (\\(SST_X\\)): More variation in the independent variable \\(X\\) provides more “information” and leads to a smaller standard error. If all values of \\(X\\) are clustered closely together, it is harder to pin down the slope of the relationship.\nThe standard error for the intercept \\(\\hat{\\alpha}\\) has a more complex formula but is driven by the same factors: \\(n\\), \\(\\hat{\\sigma}\\), and the spread of \\(X\\).\n\n\n1.6.3 Calculating Standard Errors\nUsually, you don’t need to calculate these by hand. In R, the summary() function computes them automatically using the formulas above.\n\n# Re-running the model from earlier for clarity\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# The summary output shows the coefficients and their standard errors\nsummary_model &lt;- summary(model)\nprint(summary_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIn the output, the Std. Error column next to the (Intercept) and wt estimates contains the calculated \\(SE(\\hat{\\alpha})\\) and \\(SE(\\hat{\\beta})\\). These values are used to compute the t-statistics and p-values for hypothesis testing, allowing us to assess the statistical significance of our estimates.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "href": "intro.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.8 Measures of Fit: How Well Does the Line Explain the Data?",
    "text": "1.8 Measures of Fit: How Well Does the Line Explain the Data?\nOnce we have our regression line, we want to know how well it fits the data. We decompose the total variation in \\(Y\\):\n\nSST (Total Sum of Squares): Total variation in \\(Y\\) around its mean. \\(SST = \\sum (Y_i - \\bar{Y})^2\\)\nSSE (Explained Sum of Squares): Variation in \\(Y\\) explained by the model. \\(SSE = \\sum (\\hat{Y}_i - \\bar{Y})^2\\)\nSSR (Residual Sum of Squares): Variation in \\(Y\\) not explained by the model. \\(SSR = \\sum \\hat{u}_i^2\\)\n\nThey are related by the identity: \\(SST = SSE + SSR\\).\n\n1.8.1 The R-Squared (\\(R^2\\))\nThe most common measure of fit is the R-squared statistic. It represents the fraction of the sample variation in \\(Y\\) that is explained by \\(X\\).\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\\(R^2\\) always lies between 0 and 1.\nAn \\(R^2\\) of 0 means \\(X\\) explains none of the variation in \\(Y\\).\nAn \\(R^2\\) of 1 means \\(X\\) explains all of the variation in \\(Y\\).\nIn a simple regression, \\(R^2\\) is also the square of the correlation coefficient between \\(X\\) and \\(Y\\), that is,\\(R^2 = r_{xy}^2\\).\n\nIn our mtcars example, the \\(R^2\\) is 0.7528. This means that about 75% of the variation in miles per gallon is explained by the weight of the car.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#hypothesis-testing-and-standard-errors",
    "href": "intro.html#hypothesis-testing-and-standard-errors",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.7 Hypothesis Testing and Standard Errors",
    "text": "1.7 Hypothesis Testing and Standard Errors\nOur estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random—they would change if we collected a new sample. To conduct statistical inference, we need to estimate their variability, which is captured by the Standard Error (S.E.).\nThe most common hypothesis test in regression is whether the slope coefficient \\(\\beta\\) is statistically different from zero.\n\\(H_0: \\beta = 0\\) (There is no relationship between \\(X\\) and \\(Y\\)) vs. \\(H_1: \\beta \\neq 0\\) (There is a relationship between \\(X\\) and \\(Y\\))\nWe use a t-test to evaluate this hypothesis, where the test statistic is\n\\[TS = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\]\nYou can reject the null hypothesis (\\(H_0\\)) if\n\n|t-statistic| &gt; critical value (approx. “2” for a 5% significance level)\np-value &lt; significance level (e.g., \\(\\alpha = 0.05\\)). The p-value is the probability of observing a result as extreme as the one in your sample if the null hypothesis were true.\nIf the 95% confidence interval \\([\\hat{\\beta} - 1.96 \\cdot SE(\\hat{\\beta}), \\hat{\\beta} + 1.96 \\cdot SE(\\hat{\\beta})]\\) does not contain zero.\n\nIn our mtcars output\n\nThe t-statistic for wt is -9.559.\nThe p-value is 1.29e-10 (effectively 0), which is much less than 0.05.\nThe 95% confidence interval can be calculated with confint(model) and will not contain zero.\n\nConclusion: We strongly reject the null hypothesis. Hence there is a statistically significant relationship between car weight and fuel efficiency at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "assumptions.html#clrm-assumptions",
    "href": "assumptions.html#clrm-assumptions",
    "title": "2  The Classical Linear Model (Gauss-Markov) Assumptions",
    "section": "",
    "text": "Interpretation: This means that the explanatory variable \\(X\\) provides no information about the mean of the unobserved factors. On average, the positive and negative omitted effects cancel out. This is the single most important assumption.\nImplication: It implies that the model is correctly specified in its functional form and that there are no omitted variables that are correlated with \\(X\\). If this assumption fails, our OLS estimates are biased.\n\n\n\n\n\nInterpretation: The variance of the unobserved factors is constant across all values of \\(X\\). The spread of the data points around the regression line is the same whether \\(X\\) is small or large.\nImplication: If this holds, OLS standard errors are valid. If it fails, we have heteroskedasticity, which means OLS estimates are still unbiased but their standard errors are incorrect. This leads to faulty hypothesis tests and confidence intervals.\n\n\n\n\n\nInterpretation: The unobserved factors affecting \\(Y\\) for one observation are not correlated with the unobserved factors affecting \\(Y\\) for any other observation. In cross-sectional data, this is usually guaranteed by random sampling. However autocorrelation is usually a concern for time-series data.\nImplication: Like heteroskedasticity, if this assumption fails, OLS estimates remain unbiased but the standard errors are incorrect/inefficient, leading to unreliable inference.\n\n\n\n\n\nInterpretation: This is essentially a weaker version of Assumption 1. It means \\(X\\) is not influenced by the unobserved factors in \\(u\\).\nImplication: This assumption is crucial especially for causal interpretation. If \\(X\\) is correlated with \\(u\\), it could mean an omitted variable that affects \\(Y\\) is also correlated with \\(X\\) (a.k.a confounding, or the “third variable” problem). This is the famous omitted variable bias, which causes \\(\\hat{\\beta}\\) to be biased and inconsistent.\n\n\n\nInterpretation (for simple regression): In the simple regression model with one explanatory variable, this assumption is automatically satisfied as long as \\(X\\) is not constant.\nInterpretation (for multiple regression): This assumption becomes critical when we have more than one explanatory variable. It states that no independent variable is a perfect linear combination of another independent variable(s).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Classical Linear Model (Gauss-Markov) Assumptions</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-trivariate-model-interpretation",
    "href": "multiple.html#the-trivariate-model-interpretation",
    "title": "3  The Multiple Regression Model",
    "section": "",
    "text": "\\(\\beta_1\\) is the marginal effect of \\(X_1\\) on \\(Y\\). It is the effect of a small change in the \\(X_1\\) on the dependent variable, while holding \\(X_2\\) constant.\n\\(\\beta_2\\) is the marginal effect of \\(X_2\\) on \\(Y\\), while holding \\(X_1\\) constant.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "simple.html",
    "href": "simple.html",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "1.1 Introduction\nEconomic theory suggests relationships between variables, but it rarely provides the quantitative magnitude of these causal effects. For example, we are interested in questions such as:\nIdeally, we would answer these questions with controlled experiments. However, this is often impractical, unethical, or impossible. Instead, econometricians must rely on observational data.\nThe core challenge with observational studies is that correlation does not imply causation. Some major threats to establishing a proper empirical understanding of economic relationships are:\nAs a way of introduction, we introduce the primary tools used to estimate relationships from observational data in econometrics: the Ordinary Least Squares (OLS) method.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#introduction",
    "href": "simple.html#introduction",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "What is the effect of reducing class size on student academic performance?\nWhat is the price elasticity of cigarettes?\nWhat is the return to an additional year of education?\nHow does a 1 percentage point increase in interest rates affect output growth?\n\n\n\n\nOmitted Variable Bias (Confounding Factors): A variable we have not accounted for is influencing both the dependent and independent variable.\nSimultaneous Causality: Two variables influence each other simultaneously (e.g., police numbers and crime rates).\nSample Selection Bias: The process by which data is collected influences the availability of data, leading to a non-random sample that may not represent the population of interest.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#types-of-data",
    "href": "simple.html#types-of-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.2 Types of Data",
    "text": "1.2 Types of Data\nBefore we begin, it’s useful to recognize the common structures of econometric data:\n\nCross-sectional: Data on multiple entities (individuals, firms, countries) at a single point in time.\nTime-series: Data on a single entity collected at multiple time periods (e.g., daily, quarterly, yearly).\nPanel/Longitudinal: Data on multiple entities where each entity is observed at multiple time periods. This combines cross-sectional and time-series dimensions.\nPooled Cross-sectional: Multiple cross-sectional samples taken at different points in time, where the entities in each sample are different.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#the-simple-regression-model",
    "href": "simple.html#the-simple-regression-model",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.3 The Simple Regression Model",
    "text": "1.3 The Simple Regression Model\nLet’s begin by investigating the linear relationship between two variables, \\(Y\\) (the dependent variable) and \\(X\\) (the independent or explanatory variable).\n\n1.3.1 The Population Regression Function (PRF)\nImagine we could collect data on everyone in the population of interest. The true relationship in the population is given by the Population Regression Function:\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\n\n\\(Y_i\\) is the dependent variable for observation \\(i\\).\n\\(X_i\\) is the independent variable for observation \\(i\\).\n\\(\\alpha\\) is the population intercept.\n\\(\\beta\\) is the population slope coefficient (the parameter of primary interest).\n\\(u_i\\) is the error term, which contains all factors other than \\(X\\) that influence \\(Y\\).\n\nWe can never observe the true PRF because we cannot collect data on the entire population. The error term \\(u_i\\) exists due to: (1) The inherent randomness of human behavior, (2) Unavailable or incomplete data, (3) Omitted variables from the model, (4) Imperfect functional form specification, (5) Aggregation errors, (6) Measurement errors.\n\n\n1.3.2 The Sample Regression Function (SRF)\nSince we can’t work with the population, we take a sample and use it to estimate the PRF. The estimated model is called the Sample Regression Function:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\\]\n\\[Y_i = \\hat{\\alpha} + \\hat{\\beta} X_i + \\hat{u}_i\\]\n\n\\(\\hat{Y}_i\\) is the predicted or fitted value of \\(Y_i\\).\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the estimators of the population parameters \\(\\alpha\\) and \\(\\beta\\). These coefficients are calculated from our sample data.\n\\(\\hat{u}_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\), which is our estimate of the unobserved error term \\(u_i\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#the-ordinary-least-squares-ols-method",
    "href": "simple.html#the-ordinary-least-squares-ols-method",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.4 The Ordinary Least Squares (OLS) Method",
    "text": "1.4 The Ordinary Least Squares (OLS) Method\nHow do we find the “best” line through our scatter of data points? The OLS method chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimizes the Sum of Squared Residuals (SSR).\n\\[\\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n \\hat{u}_i^2 = \\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\nThe formulas for the OLS estimators, derived by solving this minimization problem (the “normal equations”), are:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]\n\\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\n\n1.4.1 Intuition behind the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) aren’t arbitrary; they are the direct mathematical solution to the problem of minimizing the sum of squared residuals. But we can also understand them intuitively.\n\n1.4.1.1 Intuition for the Slope (\\(\\hat{\\beta}\\))\nLet’s look at the formula for the slope estimator more closely:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\n\nDividing the numerator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\), which is the sample covariance between \\(X\\) and \\(Y\\). Recall that the covariance measures how two variables move together:\n\nIf \\(X\\) is above its mean when \\(Y\\) is above its mean (and vice versa), the products \\((X_i - \\bar{X})(Y_i - \\bar{Y})\\) will be positive, leading to a positive covariance and a positive \\(\\hat{\\beta}\\).\nIf \\(X\\) is above its mean when \\(Y\\) is below its mean, the products will be negative, leading to a negative covariance and a negative \\(\\hat{\\beta}\\).\n\nNote also that dividing the denominator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2\\), which is the sample variance of \\(X\\). The variance measures the spread or variation of \\(X\\) around its own mean.\n\nSo, we can think of \\(\\hat{\\beta}\\) as: \\[\\hat{\\beta} = \\frac{\\text{Sample Covariance between X and Y}}{\\text{Sample Variance of X}}\\]\nIn other words, the OLS slope estimator answers the question: “For a given amount of movement in \\(X\\), how much associated movement do we see in \\(Y\\)?” It scales the co-movement of \\(X\\) and \\(Y\\) by the movement in \\(X\\) itself. A steeper slope (larger \\(|\\hat{\\beta}|\\)) means a unit change in \\(X\\) is associated with a larger change in \\(Y\\).\n\n\n1.4.1.2 Intuition for the Intercept (\\(\\hat{\\alpha}\\))\nThe formula for the intercept is: \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\nThis ensures that the regression line always passes through the point of the means \\((\\bar{X}, \\bar{Y})\\). Think of it as an “anchor” point for the line.\n\n\\(\\hat{\\beta}\\bar{X}\\) tells us where the regression line would predict \\(\\bar{Y}\\) to be based only on the average value of \\(X\\).\n\\(\\bar{Y} - \\hat{\\beta}\\bar{X}\\) is the adjustment needed so that the prediction is correct precisely at the means. It represents the predicted value of \\(Y\\) when \\(X = 0\\), which may or may not be a meaningful value depending on the context (e.g., predicting a company’s profit when revenue is zero might not be sensible).\n\n\n\n1.4.1.3 The Core Idea of “Least Squares”\nThe goal is to minimize the sum of squared residuals (\\(\\sum \\hat{u}_i^2\\)). Why squares? 1. Squaring penalizes large errors more severely than small errors. A residual of 2 is four times “worse” than a residual of 1 \\((2^2 = 4\\) vs. \\(1^2 = 1)\\). This makes the estimator very sensitive to outliers. 2. Squaring ensures all errors are positive. We don’t want positive and negative errors to cancel each other out. 3. The math works out nicely. Minimizing a quadratic function (like the sum of squares) leads to the clean, linear equations (“normal equations”) that give us the formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nThe OLS method therefore finds the unique line that minimizes the total squared vertical distance between the observed data points \\((X_i, Y_i)\\) and the line itself. It’s a best-fit line by its own specific definition of “best” (minimum sum of squared errors).\n\n\n\n1.4.2 Fitting an OLS Model in R\nLet’s use the mtcars dataset to estimate a simple regression model, predicting miles per gallon (mpg) using car weight (wt).\n\n# Load the built-in dataset\ndata(mtcars)\n\n# Estimate the OLS model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print a summary of the results\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe output shows our estimated coefficients is \\(\\hat{\\alpha}\\) (Intercept) = 37.29 and \\(\\hat{\\beta}\\) (wt) = -5.34\nThis gives us the Sample Regression Line\n\\[\\widehat{mpg}_i = 37.29 - 5.34 \\times wt_i\\]\nHence, for a one-ton increase in car weight, we predict miles per gallon will decrease by about 5.34 units.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#derivation-of-the-ols-estimators",
    "href": "simple.html#derivation-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.5 Derivation of the OLS Estimators",
    "text": "1.5 Derivation of the OLS Estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are derived by solving the minimization problem of the Sum of Squared Residuals (SSR). This process involves calculus, specifically taking derivatives and setting them to zero to find the minimum. The resulting equations are called the normal equations.\n\n1.5.1 The Minimization Problem\nWe aim to find the values of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimize: \\[S(\\hat{\\alpha}, \\hat{\\beta}) = \\sum_{i=1}^n \\hat{u}_i^2 = \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\n\n\n1.5.2 The Normal Equations\nTo find the minimum, we take the partial derivatives of \\(S(\\hat{\\alpha}, \\hat{\\beta})\\) with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and set them equal to zero.\n\nDerivative with respect to \\(\\hat{\\alpha}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\alpha}} = -2 \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the first normal equation: \\[\\sum_{i=1}^n Y_i = n\\hat{\\alpha} + \\hat{\\beta} \\sum_{i=1}^n X_i \\quad \\text{(1)}\\]\nDerivative with respect to \\(\\hat{\\beta}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\beta}} = -2 \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the second normal equation: \\[\\sum_{i=1}^n X_iY_i = \\hat{\\alpha} \\sum_{i=1}^n X_i + \\hat{\\beta} \\sum_{i=1}^n X_i^2 \\quad \\text{(2)}\\]\n\n\n\n1.5.3 Solving the Normal Equations\nWe now have a system of two equations with two unknowns (\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)).\n\nSolving for \\(\\hat{\\alpha}\\): Start by rearranging the first normal equation (1): \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] where \\(\\bar{Y} = \\frac{1}{n}\\sum Y_i\\) and \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\). This is our formula for the intercept.\nSolving for \\(\\hat{\\beta}\\): Substitute the expression for \\(\\hat{\\alpha}\\) into the second normal equation (2): \\[\\sum X_iY_i = (\\bar{Y} - \\hat{\\beta}\\bar{X})\\sum X_i + \\hat{\\beta} \\sum X_i^2\\] Solving this for \\(\\hat{\\beta}\\) involves some algebra. Subtract \\(\\bar{Y}\\sum X_i\\) from both sides and factor out \\(\\hat{\\beta}\\): \\[\\sum X_iY_i - \\bar{Y}\\sum X_i = \\hat{\\beta} \\left( \\sum X_i^2 - \\bar{X}\\sum X_i \\right)\\] Note that \\(\\sum X_iY_i - \\bar{Y}\\sum X_i = \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\) and \\(\\sum X_i^2 - \\bar{X}\\sum X_i = \\sum (X_i - \\bar{X})^2\\). This gives us the final formula: \\[\\hat{\\beta} = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#standard-errors-of-the-ols-estimators",
    "href": "simple.html#standard-errors-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.6 Standard Errors of the OLS Estimators",
    "text": "1.6 Standard Errors of the OLS Estimators\nThe OLS estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random variables—their values vary from sample to sample. The standard error measures the precision of these estimates by estimating the standard deviation of their sampling distributions. Smaller standard errors indicate more precise estimates.\n\n1.6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)\nTo conduct statistical inference on our OLS estimate \\(\\hat{\\beta}\\) (e.g., to build confidence intervals or test hypotheses), we need to estimate its sampling variability. This variability is measured by its variance or, more commonly, its standard error.\nThe True Variance of \\(\\hat{\\beta}\\)\nUnder the classical linear model assumptions, the true variance of the OLS slope estimator in a simple regression is given by:\n\\[Var(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nwhere, \\(\\sigma^2 = Var(u_i)\\) is the variance of the unobserved error term, and \\(\\sum_{i=1}^n (X_i - \\bar{X})^2\\) is the total variation of the independent variable \\(X\\) around its mean.\nThis formula shows that the precision of \\(\\hat{\\beta}\\) improves (its variance decreases) when either (1) the error variance (\\(\\sigma^2\\)) is smaller (the data points are tighter around the line, and/or (2) the spread of the explanatory variable \\(X\\) is larger (there is more information in the data).\nEstimating the Unknown Error Variance (\\(\\sigma^2\\))\nSince the error variance \\(\\sigma^2\\) is unknown, we must estimate it using the sample data. An unbiased estimator for \\(\\sigma^2\\) is\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2   \\quad \\text{or}   \\quad \\frac{SSR}{n - k}\\]\nwhere, \\(SSR = \\sum_{i=1}^n \\hat{u}_i^2\\) is the Sum of Squared Residuals (SSR), \\(n\\) is the sample size, and \\(k\\) is the total number of parameters estimated. In a simple regression, we estimate two parameters, i.e. the slope (\\(\\beta\\)) and the intercept (\\(\\alpha\\)), so \\(k=2\\). The term \\(n - k\\) is the degrees of freedom. Using \\(n-k\\) instead of \\(n\\) ensures that \\(E[\\hat{\\sigma}^2] = \\sigma^2\\), making it an unbiased estimator.\nThe Standard Error of the Regression (SER)\nThe square root of \\(\\hat{\\sigma}^2\\) is called the Standard Error of the Regression (SER) or the residual standard error. It is an estimate of the standard deviation of the error term \\(u_i\\) and represents the average distance that the observed values fall from the regression line—the typical size of a residual.\n\\[SER = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{SSR}{n - k}}\\]\nThe Estimated Variance and Standard Error of \\(\\hat{\\beta}\\)\nBy plugging the estimate \\(\\hat{\\sigma}^2\\) into the true variance formula, we obtain the estimated variance of the OLS estimator\n\\[\\widehat{Var}(\\hat{\\beta}) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nThe standard error of \\(\\hat{\\beta}\\) is simply the square root of this estimated variance. It is the estimated standard deviation of the sampling distribution of \\(\\hat{\\beta}\\).\n\\[SE(\\hat{\\beta}) = \\sqrt{\\widehat{Var}(\\hat{\\beta})} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}} \\quad \\text{or} \\quad \\hat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n x^2}}\\]\nThis final formula is the most intuitive: the standard error of the coefficient depends directly on the “noise” in the model (\\(SER\\)) and inversely on the amount of information in the explanatory variable, i.e. \\(\\sqrt{\\sum_{i=1}^n x^2}\\).\n\n\n1.6.2 What Drives the Standard Error?\nThe formula for \\(SE(\\hat{\\beta})\\) provides deep intuition about what makes an estimate precise: 1. Spread of the error term (\\(\\hat{\\sigma}\\)): A larger error variance (a noisier relationship, where points are scattered farther from the line) leads to a larger standard error and less precise estimates. 2. Sample size (\\(n\\)): A larger sample size \\(n\\) will (all else equal) make \\(\\hat{\\sigma}\\) smaller and the denominator larger, leading to a smaller standard error and more precise estimates. 3. Spread of the regressor \\(X\\) (\\(SST_X\\)): More variation in the independent variable \\(X\\) provides more “information” and leads to a smaller standard error. If all values of \\(X\\) are clustered closely together, it is harder to pin down the slope of the relationship.\nThe standard error for the intercept \\(\\hat{\\alpha}\\) has a more complex formula but is driven by the same factors: \\(n\\), \\(\\hat{\\sigma}\\), and the spread of \\(X\\).\n\n\n1.6.3 Calculating Standard Errors\nUsually, you don’t need to calculate these by hand. In R, the summary() function computes them automatically using the formulas above.\n\n# Re-running the model from earlier for clarity\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# The summary output shows the coefficients and their standard errors\nsummary_model &lt;- summary(model)\nprint(summary_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIn the output, the Std. Error column next to the (Intercept) and wt estimates contains the calculated \\(SE(\\hat{\\alpha})\\) and \\(SE(\\hat{\\beta})\\). These values are used to compute the t-statistics and p-values for hypothesis testing, allowing us to assess the statistical significance of our estimates.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#hypothesis-testing-and-standard-errors",
    "href": "simple.html#hypothesis-testing-and-standard-errors",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.7 Hypothesis Testing and Standard Errors",
    "text": "1.7 Hypothesis Testing and Standard Errors\nOur estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random—they would change if we collected a new sample. To conduct statistical inference, we need to estimate their variability, which is captured by the Standard Error (S.E.).\nThe most common hypothesis test in regression is whether the slope coefficient \\(\\beta\\) is statistically different from zero.\n\\(H_0: \\beta = 0\\) (There is no relationship between \\(X\\) and \\(Y\\)) vs. \\(H_1: \\beta \\neq 0\\) (There is a relationship between \\(X\\) and \\(Y\\))\nWe use a t-test to evaluate this hypothesis, where the test statistic is\n\\[TS = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\]\nYou can reject the null hypothesis (\\(H_0\\)) if\n\n|t-statistic| &gt; critical value (approx. “2” for a 5% significance level)\np-value &lt; significance level (e.g., \\(\\alpha = 0.05\\)). The p-value is the probability of observing a result as extreme as the one in your sample if the null hypothesis were true.\nIf the 95% confidence interval \\([\\hat{\\beta} - 1.96 \\cdot SE(\\hat{\\beta}), \\hat{\\beta} + 1.96 \\cdot SE(\\hat{\\beta})]\\) does not contain zero.\n\nIn our mtcars output\n\nThe t-statistic for wt is -9.559.\nThe p-value is 1.29e-10 (effectively 0), which is much less than 0.05.\nThe 95% confidence interval can be calculated with confint(model) and will not contain zero.\n\nConclusion: We strongly reject the null hypothesis. Hence there is a statistically significant relationship between car weight and fuel efficiency at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "href": "simple.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.8 Measures of Fit: How Well Does the Line Explain the Data?",
    "text": "1.8 Measures of Fit: How Well Does the Line Explain the Data?\nOnce we have our regression line, we want to know how well it fits the data. We decompose the total variation in \\(Y\\):\n\nSST (Total Sum of Squares): Total variation in \\(Y\\) around its mean. \\(SST = \\sum (Y_i - \\bar{Y})^2\\)\nSSE (Explained Sum of Squares): Variation in \\(Y\\) explained by the model. \\(SSE = \\sum (\\hat{Y}_i - \\bar{Y})^2\\)\nSSR (Residual Sum of Squares): Variation in \\(Y\\) not explained by the model. \\(SSR = \\sum \\hat{u}_i^2\\)\n\nThey are related by the identity: \\(SST = SSE + SSR\\).\n\n1.8.1 The R-Squared (\\(R^2\\))\nThe most common measure of fit is the R-squared statistic. It represents the fraction of the sample variation in \\(Y\\) that is explained by \\(X\\).\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\\(R^2\\) always lies between 0 and 1.\nAn \\(R^2\\) of 0 means \\(X\\) explains none of the variation in \\(Y\\).\nAn \\(R^2\\) of 1 means \\(X\\) explains all of the variation in \\(Y\\).\nIn a simple regression, \\(R^2\\) is also the square of the correlation coefficient between \\(X\\) and \\(Y\\), that is,\\(R^2 = r_{xy}^2\\).\n\nIn our mtcars example, the \\(R^2\\) is 0.7528. This means that about 75% of the variation in miles per gallon is explained by the weight of the car.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  }
]