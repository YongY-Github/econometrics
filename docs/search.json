[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics Notes",
    "section": "",
    "text": "Welcome\nThis is a Quarto document containing notes for the MAAE Applied Econometrics class, Fall 2025. Kindly note that this is work-in-progress and your comments are welcome to help improve this work!\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "1.1 Introduction\nEconomic theory suggests relationships between variables, but it rarely provides the quantitative magnitude of these causal effects. For example, we are interested in questions such as:\nIdeally, we would answer these questions with controlled experiments. However, this is often impractical, unethical, or impossible. Instead, econometricians must rely on observational data.\nThe core challenge with observational studies is that correlation does not imply causation. Some major threats to establishing a proper empirical understanding of economic relationships are:\nAs a way of introduction, we introduce the primary tools used to estimate relationships from observational data in econometrics: the Ordinary Least Squares (OLS) method.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-population-regression-function-prf",
    "href": "intro.html#the-population-regression-function-prf",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "3.1 The Population Regression Function (PRF)",
    "text": "3.1 The Population Regression Function (PRF)\nImagine we could collect data on everyone in the population of interest. The true relationship in the population is given by the Population Regression Function:\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\n\n\\(Y_i\\) is the dependent variable for observation \\(i\\).\n\\(X_i\\) is the independent variable for observation \\(i\\).\n\\(\\alpha\\) is the population intercept.\n\\(\\beta\\) is the population slope coefficient (the parameter of primary interest).\n\\(u_i\\) is the error term, which contains all factors other than \\(X\\) that influence \\(Y\\).\n\nWe can never observe the true PRF because we cannot collect data on the entire population. The error term \\(u_i\\) exists due to: (1) The inherent randomness of human behavior, (2) Unavailable or incomplete data, (3) Omitted variables from the model, (4) Imperfect functional form specification, (5) Aggregation errors, (6) Measurement errors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-sample-regression-function-srf",
    "href": "intro.html#the-sample-regression-function-srf",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "3.2 The Sample Regression Function (SRF)",
    "text": "3.2 The Sample Regression Function (SRF)\nSince we can’t work with the population, we take a sample and use it to estimate the PRF. The estimated model is called the Sample Regression Function:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\\]\n\\[Y_i = \\hat{\\alpha} + \\hat{\\beta} X_i + \\hat{u}_i\\]\n\n\\(\\hat{Y}_i\\) is the predicted or fitted value of \\(Y_i\\).\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the estimators of the population parameters \\(\\alpha\\) and \\(\\beta\\). These coefficients are calculated from our sample data.\n\\(\\hat{u}_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\), which is our estimate of the unobserved error term \\(u_i\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#fitting-an-ols-model-in-r",
    "href": "intro.html#fitting-an-ols-model-in-r",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "4.2 Fitting an OLS Model in R",
    "text": "4.2 Fitting an OLS Model in R\nLet’s use the mtcars dataset to estimate a simple regression model, predicting miles per gallon (mpg) using car weight (wt).\n\n# Load the built-in dataset\ndata(mtcars)\n\n# Estimate the OLS model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print a summary of the results\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe output shows our estimated coefficients: - \\(\\hat{\\alpha}\\) (Intercept) = 37.29 - \\(\\hat{\\beta}\\) (wt) = -5.34\nThis gives us the Sample Regression Line: \\[\\widehat{mpg}_i = 37.29 - 5.34 \\times wt_i\\] Interpretation: For a one-ton increase in car weight, we predict miles per gallon will decrease by about 5.34 units, holding all else constant.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#deriving-the-ols-estimators",
    "href": "intro.html#deriving-the-ols-estimators",
    "title": "1  Ordinary Least Squares (OLS) and the Simple Regression Model",
    "section": "4.2 Deriving the OLS estimators",
    "text": "4.2 Deriving the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are derived by solving the minimization problem of the Sum of Squared Residuals (SSR). This process involves calculus, specifically taking derivatives and setting them to zero to find the minimum. The resulting equations are called the normal equations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares (OLS) and the Simple Regression Model</span>"
    ]
  },
  {
    "objectID": "intro.html#the-minimization-problem",
    "href": "intro.html#the-minimization-problem",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "5.1 The Minimization Problem",
    "text": "5.1 The Minimization Problem\nWe aim to find the values of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimize: \\[S(\\hat{\\alpha}, \\hat{\\beta}) = \\sum_{i=1}^n \\hat{u}_i^2 = \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-normal-equations",
    "href": "intro.html#the-normal-equations",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "5.2 The Normal Equations",
    "text": "5.2 The Normal Equations\nTo find the minimum, we take the partial derivatives of \\(S(\\hat{\\alpha}, \\hat{\\beta})\\) with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and set them equal to zero.\n\nDerivative with respect to \\(\\hat{\\alpha}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\alpha}} = -2 \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the first normal equation: \\[\\sum_{i=1}^n Y_i = n\\hat{\\alpha} + \\hat{\\beta} \\sum_{i=1}^n X_i \\quad \\text{(1)}\\]\nDerivative with respect to \\(\\hat{\\beta}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\beta}} = -2 \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the second normal equation: \\[\\sum_{i=1}^n X_iY_i = \\hat{\\alpha} \\sum_{i=1}^n X_i + \\hat{\\beta} \\sum_{i=1}^n X_i^2 \\quad \\text{(2)}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#solving-the-normal-equations",
    "href": "intro.html#solving-the-normal-equations",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "5.3 Solving the Normal Equations",
    "text": "5.3 Solving the Normal Equations\nWe now have a system of two equations with two unknowns (\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)).\n\nSolving for \\(\\hat{\\alpha}\\): Start by rearranging the first normal equation (1): \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] where \\(\\bar{Y} = \\frac{1}{n}\\sum Y_i\\) and \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\). This is our formula for the intercept.\nSolving for \\(\\hat{\\beta}\\): Substitute the expression for \\(\\hat{\\alpha}\\) into the second normal equation (2): \\[\\sum X_iY_i = (\\bar{Y} - \\hat{\\beta}\\bar{X})\\sum X_i + \\hat{\\beta} \\sum X_i^2\\] Solving this for \\(\\hat{\\beta}\\) involves some algebra. Subtract \\(\\bar{Y}\\sum X_i\\) from both sides and factor out \\(\\hat{\\beta}\\): \\[\\sum X_iY_i - \\bar{Y}\\sum X_i = \\hat{\\beta} \\left( \\sum X_i^2 - \\bar{X}\\sum X_i \\right)\\] Note that \\(\\sum X_iY_i - \\bar{Y}\\sum X_i = \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\) and \\(\\sum X_i^2 - \\bar{X}\\sum X_i = \\sum (X_i - \\bar{X})^2\\). This gives us the final formula: \\[\\hat{\\beta} = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-formula-for-the-standard-error-of-hatbeta",
    "href": "intro.html#the-formula-for-the-standard-error-of-hatbeta",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)",
    "text": "6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)\nUnder the classical linear model assumptions, the formula for the variance of the slope estimator is: \\[Var(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\] where \\(\\sigma^2\\) is the variance of the error term \\(u_i\\).\nSince we never know the true \\(\\sigma^2\\), we estimate it using the sample. The estimator for the error variance is: \\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2 = \\frac{SSR}{n - k}\\]\nwhere \\(k\\) is the number of estimated parameters (for a simple regression, \\(k=2\\): the intercept and the slope). We use \\(n-k\\) instead of \\(n\\) to make this an unbiased estimator of \\(\\sigma^2\\) (\\(E[\\hat{\\sigma}^2] = \\sigma^2\\)). This adjustment is called using degrees of freedom.\nThe Standard Error of the Regression (SER) is an estimator of the standard deviation of the error term \\(u_i\\). It measures the average distance that the observed values fall from the regression line—the “typical” size of a residual.\n\\[SER = \\sqrt{\\frac{1}{n-k} \\sum_{i=1}^n \\hat{u}_i^2} = \\sqrt{\\frac{SSR}{n-k}}\\]\nwhere \\(n\\) is the sample size and \\(k\\) is the number of independent variables plus the constant (for a simple regression, \\(k=2\\)).\nHence, the estimated variance of \\(\\hat{\\beta}\\) is: \\[\\widehat{Var}(\\hat{\\beta}) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nThe standard error of \\(\\hat{\\beta}\\) is just the square root of this estimated variance: \\[SE(\\hat{\\beta}) = \\sqrt{\\widehat{Var}(\\hat{\\beta})} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}} = \\sigma \\sqrt{\\frac{1}{\\sum_{i=1}^n x_i}}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#what-drives-the-standard-error",
    "href": "intro.html#what-drives-the-standard-error",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "6.2 What Drives the Standard Error?",
    "text": "6.2 What Drives the Standard Error?\nThe formula for \\(SE(\\hat{\\beta})\\) provides deep intuition about what makes an estimate precise: 1. Spread of the error term (\\(\\hat{\\sigma}\\)): A larger error variance (a noisier relationship, where points are scattered farther from the line) leads to a larger standard error and less precise estimates. 2. Sample size (\\(n\\)): A larger sample size \\(n\\) will (all else equal) make \\(\\hat{\\sigma}\\) smaller and the denominator larger, leading to a smaller standard error and more precise estimates. 3. Spread of the regressor \\(X\\) (\\(SST_X\\)): More variation in the independent variable \\(X\\) provides more “information” and leads to a smaller standard error. If all values of \\(X\\) are clustered closely together, it is harder to pin down the slope of the relationship.\nThe standard error for the intercept \\(\\hat{\\alpha}\\) has a more complex formula but is driven by the same factors: \\(n\\), \\(\\hat{\\sigma}\\), and the spread of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#calculating-standard-errors-in-r",
    "href": "intro.html#calculating-standard-errors-in-r",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "6.3 Calculating Standard Errors in R",
    "text": "6.3 Calculating Standard Errors in R\nYou don’t need to calculate these by hand. The summary() function in R computes them automatically using the formulas above.\n\n# Re-running the model from earlier for clarity\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# The summary output shows the coefficients and their standard errors\nsummary_model &lt;- summary(model)\nprint(summary_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIn the output, the Std. Error column next to the (Intercept) and wt estimates contains the calculated \\(SE(\\hat{\\alpha})\\) and \\(SE(\\hat{\\beta})\\). These values are used to compute the t-statistics and p-values for hypothesis testing, allowing us to assess the statistical significance of our estimates.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "assumptions.html",
    "href": "assumptions.html",
    "title": "2  The Classical Linear Model (Gauss-Markov) Assumptions",
    "section": "",
    "text": "2.1 CLRM Assumptions\nFor the Ordinary Least Squares (OLS) estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) to have desirable properties (like being the Best Linear Unbiased Estimators, or BLUE), a set of assumptions about the population model must hold. These are the core assumptions for cross-sectional data analysis.\nGiven that the population model is\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\nAssumption 1: Conditional Mean Zero\nThe error term \\(u\\) has an expected value of zero, given any value of the explanatory variable \\(X\\).\n\\[E(u_i | X_i) = 0\\]\nAssumption 2: Homoskedasticity\nThe error term \\(u\\) has the same variance given any value of the explanatory variable.\n\\[Var(u_i | X_i) = \\sigma^2\\]\nAssumption 3: No Autocorrelation\nThe error terms for any two different observations are uncorrelated.\n\\[Cov(u_i, u_j | X_i) = 0 \\quad \\text{for all } i \\neq j\\]\nAssumption 4: Exogeneity\nThe explanatory variable \\(X\\) is uncorrelated with the error term \\(u\\).\n\\[Cov(X_i, u_i) = 0\\]\nAssumption 5: No Perfect Multicollinearity",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Classical Linear Model (Gauss-Markov) Assumptions</span>"
    ]
  },
  {
    "objectID": "intro.html#the-r-squared-r2",
    "href": "intro.html#the-r-squared-r2",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "7.1 The R-Squared (\\(R^2\\))",
    "text": "7.1 The R-Squared (\\(R^2\\))\nThe most common measure of fit is the R-squared statistic. It represents the fraction of the sample variation in \\(Y\\) that is explained by \\(X\\).\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\\(R^2\\) always lies between 0 and 1.\nAn \\(R^2\\) of 0 means \\(X\\) explains none of the variation in \\(Y\\).\nAn \\(R^2\\) of 1 means \\(X\\) explains all of the variation in \\(Y\\).\nIn a simple regression, \\(R^2\\) is also the square of the correlation coefficient between \\(X\\) and \\(Y\\), that is,\\(R^2 = r_{xy}^2\\).\n\nIn our mtcars example, the \\(R^2\\) is 0.7528. This means that about 75% of the variation in miles per gallon is explained by the weight of the car.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#intuition-behind-the-ols-estimators",
    "href": "intro.html#intuition-behind-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "4.1 Intuition behind the OLS estimators",
    "text": "4.1 Intuition behind the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) aren’t arbitrary; they are the direct mathematical solution to the problem of minimizing the sum of squared residuals. But we can also understand them intuitively.\n\n4.1.1 Intuition for the Slope (\\(\\hat{\\beta}\\))\nLet’s look at the formula for the slope estimator more closely:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\n\nDividing the numerator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\), which is the sample covariance between \\(X\\) and \\(Y\\). Recall that the covariance measures how two variables move together:\n\nIf \\(X\\) is above its mean when \\(Y\\) is above its mean (and vice versa), the products \\((X_i - \\bar{X})(Y_i - \\bar{Y})\\) will be positive, leading to a positive covariance and a positive \\(\\hat{\\beta}\\).\nIf \\(X\\) is above its mean when \\(Y\\) is below its mean, the products will be negative, leading to a negative covariance and a negative \\(\\hat{\\beta}\\).\n\nNote also that dividing the denominator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2\\), which is the sample variance of \\(X\\). The variance measures the spread or variation of \\(X\\) around its own mean.\n\nSo, we can think of \\(\\hat{\\beta}\\) as: \\[\\hat{\\beta} = \\frac{\\text{Sample Covariance between X and Y}}{\\text{Sample Variance of X}}\\]\nIn other words, the OLS slope estimator answers the question: “For a given amount of movement in \\(X\\), how much associated movement do we see in \\(Y\\)?” It scales the co-movement of \\(X\\) and \\(Y\\) by the movement in \\(X\\) itself. A steeper slope (larger \\(|\\hat{\\beta}|\\)) means a unit change in \\(X\\) is associated with a larger change in \\(Y\\).\n\n\n4.1.2 Intuition for the Intercept (\\(\\hat{\\alpha}\\))\nThe formula for the intercept is: \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\nThis ensures that the regression line always passes through the point of the means \\((\\bar{X}, \\bar{Y})\\). Think of it as an “anchor” point for the line.\n\n\\(\\hat{\\beta}\\bar{X}\\) tells us where the regression line would predict \\(\\bar{Y}\\) to be based only on the average value of \\(X\\).\n\\(\\bar{Y} - \\hat{\\beta}\\bar{X}\\) is the adjustment needed so that the prediction is correct precisely at the means. It represents the predicted value of \\(Y\\) when \\(X = 0\\), which may or may not be a meaningful value depending on the context (e.g., predicting a company’s profit when revenue is zero might not be sensible).\n\n\n\n4.1.3 The Core Idea of “Least Squares”\nThe goal is to minimize the sum of squared residuals (\\(\\sum \\hat{u}_i^2\\)). Why squares? 1. Squaring penalizes large errors more severely than small errors. A residual of 2 is four times “worse” than a residual of 1 \\((2^2 = 4\\) vs. \\(1^2 = 1)\\). This makes the estimator very sensitive to outliers. 2. Squaring ensures all errors are positive. We don’t want positive and negative errors to cancel each other out. 3. The math works out nicely. Minimizing a quadratic function (like the sum of squares) leads to the clean, linear equations (“normal equations”) that give us the formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nThe OLS method therefore finds the unique line that minimizes the total squared vertical distance between the observed data points \\((X_i, Y_i)\\) and the line itself. It’s a best-fit line by its own specific definition of “best” (minimum sum of squared errors).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "assumptions.html#the-gauss-markov-theorem",
    "href": "assumptions.html#the-gauss-markov-theorem",
    "title": "2  The Classical Linear Model (Gauss-Markov) Assumptions",
    "section": "2.2 The Gauss-Markov Theorem",
    "text": "2.2 The Gauss-Markov Theorem\nIf Assumptions 1 through 4 hold (and 5 for multiple regression), the OLS estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the Best Linear Unbiased Estimators (BLUE).\n\nLinear: They are linear functions of the data.\nUnbiased: On average, across repeated samples, they equal the true population parameters, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nBest: They have the smallest variance among all other linear unbiased estimators. This means they are the most precise (efficient).\n\nThis theorem is why OLS is the workhorse of econometrics—under these conditions, no other linear estimator is better.\n\n2.2.1 Checking Assumptions\nWhile a full diagnostic check will be done in later chapters, we’ll start by checking for non-constant variances, or heteroskedasticty.\n\n2.2.1.1 Testing for Heteroskedasticity\nHere is a quick example of how to generate residual plots whch can be useful to visually check for homoskedasticity and mean zero.\n\n# Fit the model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Create a dataframe of fitted values and residuals\ndiagnostic_data &lt;- data.frame(\n  fitted = fitted(model),\n  residuals = resid(model)\n)\n\n# Plot residuals vs. fitted values\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nggplot(diagnostic_data, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values (Y_hat)\",\n       y = \"Residuals (u_hat)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpreting the plot: We look for a random scatter of points around the red zero line. The absence of a clear pattern (e.g., a curve or a funnel) is a good sign that Assumptions 1 and 2 are plausible. In this mtcars example, there might be a slight pattern, suggesting a potential minor violation worth investigating further.\nA more formal test is the White test, a general and powerful test for heteroskedasticity. It does not assume a specific form for the heteroskedasticity (e.g., that variance increases with X). The test works by regressing the squared residuals from the original model on the original explanatory variables, their squares, and their cross-products.\nThe null and alternative hypotheses are:\n\\(H_0\\): Homoskedasticity exists (the error variance is constant).\n\\(H_1\\): Heteroskedasticity exists (the error variance is not constant).\n\n# install.packages(\"lmtest\") # Uncomment and run if needed\nlibrary(lmtest)\n\n# Perform the White test for the simple model mpg ~ wt\nbptest(model, ~ wt + I(wt^2), data = mtcars)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 1.3663, df = 2, p-value = 0.505\n\n\nThe bptest() output shows a BP test statistic and a p-value. A p-value &lt; 0.05 means you reject the null hypothesis of homoskedasticity. In the mtcars example above, we fail to reject the null hypothesis, suggesting that we have may not have heteroskedasticty.\nAlternatively:\n\n# 1. Estimate the original simple regression model\nsimple_model &lt;- lm(mpg ~ wt, data = mtcars)\n\n# 2. Obtain the squared residuals from the model\nsquared_residuals &lt;- resid(simple_model)^2\n\n# 3. Perform the \"auxiliary regression\" for the White test:\n# Regress the squared residuals on the original regressor (wt) and its square (wt²).\nwhite_aux_model &lt;- lm(squared_residuals ~ wt + I(wt^2), data = mtcars)\n\n# 4. Conduct an F-test on the auxiliary model.\n# The null hypothesis is that the coefficients on 'wt' and 'I(wt^2)' are zero.\n# install.packages(\"car\") # Uncomment and run if you don't have the 'car' package\nlibrary(car)\n\nlinearHypothesis(white_aux_model, c(\"wt=0\", \"I(wt^2)=0\"))\n\n\nLinear hypothesis test:\nwt = 0\nI(wt^2) = 0\n\nModel 1: restricted model\nModel 2: squared_residuals ~ wt + I(wt^2)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     31 4542.5                           \n2     29 4348.6  2    193.95 0.6467 0.5312\n\n\nWe look at the F-statistic and its p-value (Pr(&gt;F)). A p-value &lt; 0.05 provides evidence to reject the null hypothesis (\\(H_0\\)) of homoskedasticity, thereby suggesting the error variance is not constant and depends on weight (wt). Here, again Pr(&gt;F) is 0.5312, hence we fail to reject the null.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Classical Linear Model (Gauss-Markov) Assumptions</span>"
    ]
  },
  {
    "objectID": "multiple.html",
    "href": "multiple.html",
    "title": "3  The Multiple Regression Model",
    "section": "",
    "text": "3.1 The Trivariate Model & Interpretation\nThe population multiple regression model with two explanatory variables (the trivariate model) is written as:\n\\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\]\nTaking expectations on both sides gives the conditional mean function:\n\\[E(Y_i | X_{1i}, X_{2i}) = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i}\\]\nThe key advantage of multiple regression is the ceteris paribus (all else equal) interpretation of its coefficients.\nThis holding-constant effect is what helps isolate the direct effect of one variable, controlling for the influence of others.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-population-model-interpretation",
    "href": "multiple.html#the-population-model-interpretation",
    "title": "3  The Multiple Regression Model",
    "section": "",
    "text": "\\(\\beta_1\\) is the marginal effect of \\(X_1\\) on \\(Y\\). It is the effect of a small change in the \\(X_1\\) on the dependent variable, while holding \\(X_2\\) constant.\n\\(\\beta_2\\) is the marginal effect of \\(X_2\\) on \\(Y\\), while holding \\(X_1\\) constant.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-ols-estimators-and-normal-equations",
    "href": "multiple.html#the-ols-estimators-and-normal-equations",
    "title": "3  The Multiple Regression Model",
    "section": "3.2 The OLS Estimators and Normal Equations",
    "text": "3.2 The OLS Estimators and Normal Equations\nThe sample regression function (SRF) is:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i}\\] \\[Y_i = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i} + \\hat{u}_i\\]\nAs in the bivariate case, the OLS procedure consists of choosing the unknown parameters \\((\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2)\\) such that the residual sum of squares (SSR) is minimized:\n\\[\\min_{\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2} \\sum_{i=1}^n \\hat{u}_i^2 = \\min_{\\hat{\\alpha}, \\hat{\\beta}_1, \\hat{\\beta}_2} \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta}_1 X_{1i} - \\hat{\\beta}_2 X_{2i})^2\\]\nDifferentiating and setting the derivatives to zero yields the system of normal equations:\n\\[\\begin{aligned}\n\\sum Y_i &= n\\hat{\\alpha} + \\hat{\\beta}_1 \\sum X_{1i} + \\hat{\\beta}_2 \\sum X_{2i} \\\\\n\\sum X_{1i}Y_i &= \\hat{\\alpha} \\sum X_{1i} + \\hat{\\beta}_1 \\sum X_{1i}^2 + \\hat{\\beta}_2 \\sum X_{1i}X_{2i} \\\\\n\\sum X_{2i}Y_i &= \\hat{\\alpha} \\sum X_{2i} + \\hat{\\beta}_1 \\sum X_{1i}X_{2i} + \\hat{\\beta}_2 \\sum X_{2i}^2\n\\end{aligned}\\]\nSolving this system of three equations provides the formulas for the OLS estimators \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}_1\\), and \\(\\hat{\\beta}_2\\). While the formulas are more complex than in the simple regression case, the intuition is similar.\nHence, the OLS estimators for the slope coefficients in a multiple regression model \\(Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\) are given by:\n\\[\n\\begin{align*}\n\\hat{\\beta}_1&=\\frac{(\\sum_{i=1}^n x_{1i}y_i)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{2i}y_i)(\\sum_{i=1}^n x_{1i}x_{2i})}{(\\sum_{i=1}^n x_{1i}^2)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{1i}x_{2i})^2}\\\\\n\\hat{\\beta}_2&=\\frac{(\\sum_{i=1}^n x_{2i}y_i)(\\sum_{i=1}^n x_{1i}^2)-(\\sum_{i=1}^n x_{1i}y_i)(\\sum_{i=1}^n x_{1i}x_{2i})}{(\\sum_{i=1}^n x_{1i}^2)(\\sum_{i=1}^n x_{2i}^2)-(\\sum_{i=1}^n x_{1i}x_{2i})^2}\n\\end{align*}\n\\] And the estimator for the intercept is:\n\\[\n\\hat\\alpha = \\bar Y - \\hat\\beta_1 \\bar X_1 - \\hat\\beta_2 \\bar X_2\n\\]\n\n3.2.1 Extending the mtcars Model\nWe can estimate the coefficients easily using R. Let’s first expand our car mileage model. Perhaps a car’s horsepower (hp) also affects its fuel efficiency (mpg), in addition to its weight (wt). We can estimate this trivariate model.\n\n# Estimate a multiple regression model\n# mpg = alpha + beta1 * wt + beta2 * hp + u\n\nmulti_model &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Print the summary results\nsummary(multi_model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nInterpretation of the Coefficients:\n\nwt coefficient (-3.878): Holding horsepower constant, a one-ton increase in weight is associated with a decrease in fuel efficiency of approximately 3.88 miles per gallon, on average.\nhp coefficient (-0.032): Holding weight constant, a one-horsepower increase is associated with a decrease in fuel efficiency of approximately 0.032 miles per gallon, on average.\nIntercept (37.227): The predicted miles per gallon for a car that weighs 0 tons and has 0 horsepower. (Note: This is often not a meaningful value and serves just as a baseline for the regression line.)\n\nNotice how the coefficient on wt changed from -5.34 in the simple regression to -3.88 in this multiple regression. This suggests that horsepower was an omitted variable that was correlated with both weight and mileage, and failing to control for it biased our initial estimate of the effect of weight.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#standard-errors-of-the-ols-estimates",
    "href": "multiple.html#standard-errors-of-the-ols-estimates",
    "title": "3  The Multiple Regression Model",
    "section": "3.3 Standard Errors of the OLS Estimates",
    "text": "3.3 Standard Errors of the OLS Estimates\nThe formula for the variance of a slope estimator, say \\(\\hat{\\beta}_1\\), in the trivariate model is:\n\\[Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum (X_{1i} - \\bar{X}_1)^2 (1 - r^2_{12})}\\]\n\n\\(\\sigma^2\\) is the variance of the error term \\(u_i\\).\n\\(\\sum (X_{1i} - \\bar{X}_1)^2\\) is the total variation in \\(X_1\\).\n\\(r_{12}\\) is the sample correlation between \\(X_1\\) and \\(X_2\\).\n\nSince we never observe the true error variance (\\(\\sigma^2\\)), we bootstrap and use its estimate:\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2\\]\nwhere \\(k\\) is the number of estimated coefficients (including the constant, so \\(k=3\\) for our model). The standard error of the coefficient is then the square root of its estimated variance: \\(SE(\\hat{\\beta}_1) = \\sqrt{\\widehat{Var}(\\hat{\\beta}_1)}\\).\nHence, the standard errors for \\(\\beta 's\\) in the model \\(Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\) are given by:\n\\[\n\\begin{aligned}\nSE(\\hat{\\beta}_1) &= \\sqrt{\\widehat{Var}(\\hat{\\beta}_1)} \\quad  \\text{or} \\quad \\hat{\\sigma} \\frac{1}{\\sqrt{\\sum x_{1i}^2 (1 - r_{12}^2)}} \\\\\nSE(\\hat{\\beta}_2) &= \\sqrt{\\widehat{Var}(\\hat{\\beta}_2)} \\text{or} \\quad \\hat{\\sigma} \\frac{1}{\\sqrt{\\sum x_{2i}^2 (1 - r_{12}^2)}}\n\\end{aligned}\n\\]\nThe standard error for the intercept is:\n\\[\nSE(\\hat{\\alpha}) = \\sqrt{\\widehat{Var}(\\hat{\\alpha})} = \\hat{\\sigma} \\sqrt{ \\frac{1}{n} + \\frac{\\overline{X}_1^2 \\sum x_{2i}^2 + \\overline{X}_2^2 \\sum x_{1i}^2 - 2\\overline{X}_1 \\overline{X}_2 \\sum x_{1i} x_{2i}}{\\sum x_{1i}^2 \\sum x_{2i}^2 - (\\sum x_{1i} x_{2i})^2} }\n\\]\nwhere:\n\n\\(\\hat{\\sigma} = \\sqrt{\\frac{\\sum \\hat{u}_i^2}{n - k}}\\) is the standard error of the regression (\\(SER\\)),\n\\(r_{12}\\) is the sample correlation between \\(X_1\\) and \\(X_2\\),\n\\(x_{1i} = X_{1i} - \\bar{X}_1\\) and \\(x_{2i} = X_{2i} - \\bar{X}_2\\) are deviations\n\nThe summary() function in R displays these standard errors, as seen in the output above.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#r-squared-and-the-adjusted-r-squared",
    "href": "multiple.html#r-squared-and-the-adjusted-r-squared",
    "title": "3  The Multiple Regression Model",
    "section": "3.4 R-squared and The Adjusted R-squared",
    "text": "3.4 R-squared and The Adjusted R-squared\nAs in the bivariate case, the coefficient of determination, \\(R^2\\), is the fraction of the sample variation in \\(Y\\) explained by the model:\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\nA key feature of multiple regression is that adding any new variable (even an irrelevant one) will never decrease the \\(R^2\\). Because of this, we often prefer the adjusted R-squared, which penalizes for adding irrelevant variables.\n\\[\\bar{R}^2 = 1 - \\frac{SSR/(n-k)}{SST/(n-1)} = 1 - \\left( \\frac{n-1}{n-k} \\right) \\frac{SSR}{SST}\\]\n\n\\(n\\) is the sample size.\n\\(k\\) is the number of coefficients, including the constant.\n\\(\\bar{R}^2\\) can decrease if a new variable adds little explanatory power, providing a better gauge of whether a variable should be included.\nAlways compare \\(\\bar{R}^2\\), not \\(R^2\\), when models have a different number of predictors.\n\nIn our mtcars output, we see both R-squared:  0.8268 and Adjusted R-squared:  0.8148.\n\n3.4.1 Interpreting \\(R^2\\) and \\(\\bar{R}^2\\) in Practice\nIt is critical to remember that: 1. An increase in the \\(R^2\\) or \\(\\bar{R}^2\\) does not necessarily mean that an added variable is statistically significant. 2. A high \\(R^2\\) or \\(\\bar{R}^2\\) does not mean that the regressors are a true cause of the dependent variable (causation vs. correlation). 3. A high \\(R^2\\) or \\(\\bar{R}^2\\) does not mean that there is no omitted variable bias. 4. A high \\(R^2\\) or \\(\\bar{R}^2\\) does not necessarily mean that you have the most appropriate set of regressors, nor does a low \\(R^2\\) or \\(\\bar{R}^2\\) mean that you have an inappropriate model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#hypothesis-testing",
    "href": "multiple.html#hypothesis-testing",
    "title": "3  The Multiple Regression Model",
    "section": "3.5 Hypothesis Testing",
    "text": "3.5 Hypothesis Testing\n\n3.5.1 Testing Individual Coefficients\nThe procedure for testing hypotheses about a single coefficient is identical to the simple regression case. For example, to test if \\(X_1\\) has a significant effect on \\(Y\\) after controlling for \\(X_2\\):\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_1: \\beta_1 \\neq 0\\)\n\nThe test statistic is the t-ratio: \\(t = \\displaystyle\\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}\\).\nAs a rule of thumb, we reject the null hypothesis if the absolute value of the t-ratio is greater than “2”.\n\n\n3.5.2 Testing the Overall Significance: The F-Test\nThe test for the overall significance of the regression is a joint test that all slope coefficients are equal to zero:\n\\[\nH_0: \\beta_1 = \\beta_2 = 0 \\quad \\text{vs.} \\quad H_1: \\text{at least one } \\beta_j \\neq 0\n\\]\nThis test is conducted using the F-statistic. The F-statistic can be computed in several equivalent forms:\n1. Using sums of squares and cross-products: \\[\nF = \\frac{ \\left( \\hat{\\beta}_1 \\sum y_i x_{1i} + \\hat{\\beta}_2 \\sum y_i x_{2i} \\right) / m }{ \\sum \\hat{u}_i^2 / (n - k) }\n\\] where\n\n\\(m\\) is the number of restrictions\n\\(n-k\\) is the degree of freedom of the regression\n\nThe above is the ratio of explained and unexplained sums of squares divided by their respective degrees of freedom which corresponds to ANOVA:\n\\[\nF = \\frac{SSE / (k - 1)}{SSR / (n - k)}\n\\] where \\(k\\) is the total number of estimated parameters (for a model with two regressors, \\(k=3\\): \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\)).\nNote this is the same if we use the coefficient of determination (\\(R^2\\)):\n\\[\nF = \\frac{R^2 / (k - 1)}{(1 - R^2) / (n - k)}\n\\]\nHence, the new test in multiple regression is the test for overall significance of the regression, which is a joint test that all slope coefficients are simultaneously equal to zero.\n\n\\(H_0: \\beta_1 = \\beta_2 = 0\\)\n\\(H_1: \\text{At least one } \\beta_j \\neq 0\\)\n\nThis test is given by the F-statistic, which is reported in the standard regression output. The F-statistic is constructed using the sums of squares from the ANOVA (Analysis of Variance) framework:\n\\[SST = SSE + SSR\\]\nThe F-statistic is the ratio of the explained to unexplained variance, adjusted for degrees of freedom:\n\\[F = \\frac{SSE / (k-1)}{SSR / (n-k)} = \\frac{MSR}{MSE}\\]\nwhere \\(k-1\\) is the number of slope coefficients. A large F-statistic provides evidence against the null hypothesis that the model provides no better fit than a model with only an intercept.\n\n\n3.5.3 Joint Tests and Restricted Models\nThe F-test can be generalized to test any set of linear restrictions. For example, we can test if a subset of coefficients is equal to zero.\nAssume the unrestricted model is:\n\\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + u_i\\]\nWe could test the joint hypothesis:\n\n\\(H_0: \\beta_2 = 0, \\beta_3 = 0\\)\n\\(H_1: H_0 \\text{ is not true}\\)\n\nThis involves estimating a restricted model where the restrictions under the null are imposed: \\[Y_i = \\alpha + \\beta_1 X_{1i} + u_i\\]\nThe general F-statistic formula for testing \\(m\\) restrictions is:\n\\[F = \\frac{(SSR_r - SSR_{ur}) / m}{SSR_{ur} / (n - k)}\\]\nwhere:\n\n\\(SSR_r\\) is the sum of squared residuals from the restricted model.\n\\(SSR_{ur}\\) is the sum of squared residuals from the unrestricted model.\n\\(m\\) is the number of restrictions.\n\\(n - k\\) is the degrees of freedom in the unrestricted model.\n\nHere is another example where We could test the joint hypothesis:\n\n\\(H_0: \\beta_1 = 0, \\beta_2 + \\beta_3 = 1\\)\n\\(H_1: H_0 \\text{ is not true}\\)\n\nHere, the unrestricted model is the same as before:\n\\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + u_i\\]\nTo find the restricted model, we impose the null hypothesis:\nSubstitute \\(\\beta_1 = 0\\) and \\(\\beta_3 = 1 - \\beta_2\\) into the model:\n\\[Y_i = \\alpha + \\beta_2 X_{2i} + (1 - \\beta_2) X_{3i} + u_i\\]\nRearrange the equation:\n\\[Y_i = \\alpha + X_{3i} + \\beta_2 (X_{2i} - X_{3i}) + u_i\\]\nDefine a new dependent variable \\(Y_i^* = Y_i - X_{3i}\\):\n\\[Y_i^* = \\alpha + \\beta_2 (X_{2i} - X_{3i}) + u_i\\]\nWe then perform the F-test as usual and a low p-value would lead to a rejection of the null hypothesis \\(H_0\\). Note here that \\(m\\) the number of restrictions is 2 and not 3!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-problem-of-omitted-variable-bias",
    "href": "multiple.html#the-problem-of-omitted-variable-bias",
    "title": "3  The Multiple Regression Model",
    "section": "3.6 The Problem of Omitted Variable Bias",
    "text": "3.6 The Problem of Omitted Variable Bias\nOften, we are interested in understanding the relation between two variables (\\(Y\\) and \\(X\\)). But running a simple regression of \\(Y\\) on \\(X\\) might not be enough. The assumption \\(Cov(X, u)=0\\) might be violated if the error term \\(u\\) contains a variable that is correlated with \\(X\\), thereby introducing a bias. Multiple regression is a primary tool to help resolve this endogeneity problem.\n\n3.6.1 Direction of the Bias\nOmitting an important variable introduces a bias to the OLS estimator. The direction of this bias can be formalized.\nSuppose the true model is: \\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\] But we incorrectly estimate the misspecified model: \\[Y_i = \\alpha + \\beta_1 X_{1i} + v_i \\quad \\text{where} \\quad v_i = \\beta_2 X_{2i} + u_i\\]\nThe bias in the simple regression estimator \\(\\tilde{\\beta}_1\\) is: \\[Bias(\\tilde{\\beta}_1) = E[\\tilde{\\beta}_1] - \\beta_1 = \\beta_2 \\cdot \\tilde{\\delta}_1\\]\nwhere \\(\\tilde{\\delta}_1\\) is the slope coefficient from an auxiliary regression of the omitted variable (\\(X_2\\)) on the included variable (\\(X_1\\)): \\[X_{2i} = \\delta_0 + \\delta_1 X_{1i} + e_i\\]\n\nThe sign of the bias depends on the signs of \\(\\beta_2\\) (the effect of the omitted variable on \\(Y\\)) and \\(\\tilde{\\delta}_1\\) (the correlation between \\(X_2\\) and \\(X_1\\)).\nThe size of the bias depends on the magnitude of \\(\\beta_2\\) and \\(\\tilde{\\delta}_1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-cost-of-including-an-irrelevant-variable",
    "href": "multiple.html#the-cost-of-including-an-irrelevant-variable",
    "title": "3  The Multiple Regression Model",
    "section": "3.7 The Cost of Including an Irrelevant Variable",
    "text": "3.7 The Cost of Including an Irrelevant Variable\nConversely, including a variable that does not belong in the true model (i.e., whose true coefficient is zero) has different consequences.\nSuppose the true model is: \\[Y_i = \\alpha + \\beta_1 X_{1i} + u_i\\] But we incorrectly estimate: \\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + v_i\\]\n\nDoes it introduce bias? No. The OLS estimators for all coefficients, including \\(\\hat{\\beta}_1\\), remain unbiased.\nWhat is the cost? Increased variance. The estimates become less precise. The standard error of \\(\\hat{\\beta}_1\\) will generally be larger than it would be in the correctly specified simple regression model, leading to less powerful hypothesis tests and wider confidence intervals.\n\nThe trade-off is clear: omitting a relevant variable causes bias, while including an irrelevant variable reduces efficiency. When in doubt, it is often less harmful to include a potentially irrelevant variable than to omit a potentially relevant one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "intro.html#introduction",
    "href": "intro.html#introduction",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "What is the effect of reducing class size on student academic performance?\nWhat is the price elasticity of cigarettes?\nWhat is the return to an additional year of education?\nHow does a 1 percentage point increase in interest rates affect output growth?\n\n\n\n\nOmitted Variable Bias (Confounding Factors): A variable we have not accounted for is influencing both the dependent and independent variable.\nSimultaneous Causality: Two variables influence each other simultaneously (e.g., police numbers and crime rates).\nSample Selection Bias: The process by which data is collected influences the availability of data, leading to a non-random sample that may not represent the population of interest.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-data",
    "href": "intro.html#types-of-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.2 Types of Data",
    "text": "1.2 Types of Data\nBefore we begin, it’s useful to recognize the common structures of econometric data:\n\nCross-sectional: Data on multiple entities (individuals, firms, countries) at a single point in time.\nTime-series: Data on a single entity collected at multiple time periods (e.g., daily, quarterly, yearly).\nPanel/Longitudinal: Data on multiple entities where each entity is observed at multiple time periods. This combines cross-sectional and time-series dimensions.\nPooled Cross-sectional: Multiple cross-sectional samples taken at different points in time, where the entities in each sample are different.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-simple-regression-model",
    "href": "intro.html#the-simple-regression-model",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.3 The Simple Regression Model",
    "text": "1.3 The Simple Regression Model\nLet’s begin by investigating the linear relationship between two variables, \\(Y\\) (the dependent variable) and \\(X\\) (the independent or explanatory variable).\n\n1.3.1 The Population Regression Function (PRF)\nImagine we could collect data on everyone in the population of interest. The true relationship in the population is given by the Population Regression Function:\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\n\n\\(Y_i\\) is the dependent variable for observation \\(i\\).\n\\(X_i\\) is the independent variable for observation \\(i\\).\n\\(\\alpha\\) is the population intercept.\n\\(\\beta\\) is the population slope coefficient (the parameter of primary interest).\n\\(u_i\\) is the error term, which contains all factors other than \\(X\\) that influence \\(Y\\).\n\nWe can never observe the true PRF because we cannot collect data on the entire population. The error term \\(u_i\\) exists due to: (1) The inherent randomness of human behavior, (2) Unavailable or incomplete data, (3) Omitted variables from the model, (4) Imperfect functional form specification, (5) Aggregation errors, (6) Measurement errors.\n\n\n1.3.2 The Sample Regression Function (SRF)\nSince we can’t work with the population, we take a sample and use it to estimate the PRF. The estimated model is called the Sample Regression Function:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\\]\n\\[Y_i = \\hat{\\alpha} + \\hat{\\beta} X_i + \\hat{u}_i\\]\n\n\\(\\hat{Y}_i\\) is the predicted or fitted value of \\(Y_i\\).\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the estimators of the population parameters \\(\\alpha\\) and \\(\\beta\\). These coefficients are calculated from our sample data.\n\\(\\hat{u}_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\), which is our estimate of the unobserved error term \\(u_i\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#the-ordinary-least-squares-ols-method",
    "href": "intro.html#the-ordinary-least-squares-ols-method",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.4 The Ordinary Least Squares (OLS) Method",
    "text": "1.4 The Ordinary Least Squares (OLS) Method\nHow do we find the “best” line through our scatter of data points? The OLS method chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimizes the Sum of Squared Residuals (SSR).\n\\[\\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n \\hat{u}_i^2 = \\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\nThe formulas for the OLS estimators, derived by solving this minimization problem (the “normal equations”), are:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]\n\\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\n\n1.4.1 Intuition behind the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) aren’t arbitrary; they are the direct mathematical solution to the problem of minimizing the sum of squared residuals. But we can also understand them intuitively.\n\n1.4.1.1 Intuition for the Slope (\\(\\hat{\\beta}\\))\nLet’s look at the formula for the slope estimator more closely:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\n\nDividing the numerator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\), which is the sample covariance between \\(X\\) and \\(Y\\). Recall that the covariance measures how two variables move together:\n\nIf \\(X\\) is above its mean when \\(Y\\) is above its mean (and vice versa), the products \\((X_i - \\bar{X})(Y_i - \\bar{Y})\\) will be positive, leading to a positive covariance and a positive \\(\\hat{\\beta}\\).\nIf \\(X\\) is above its mean when \\(Y\\) is below its mean, the products will be negative, leading to a negative covariance and a negative \\(\\hat{\\beta}\\).\n\nNote also that dividing the denominator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2\\), which is the sample variance of \\(X\\). The variance measures the spread or variation of \\(X\\) around its own mean.\n\nSo, we can think of \\(\\hat{\\beta}\\) as: \\[\\hat{\\beta} = \\frac{\\text{Sample Covariance between X and Y}}{\\text{Sample Variance of X}}\\]\nIn other words, the OLS slope estimator answers the question: “For a given amount of movement in \\(X\\), how much associated movement do we see in \\(Y\\)?” It scales the co-movement of \\(X\\) and \\(Y\\) by the movement in \\(X\\) itself. A steeper slope (larger \\(|\\hat{\\beta}|\\)) means a unit change in \\(X\\) is associated with a larger change in \\(Y\\).\n\n\n1.4.1.2 Intuition for the Intercept (\\(\\hat{\\alpha}\\))\nThe formula for the intercept is: \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\nThis ensures that the regression line always passes through the point of the means \\((\\bar{X}, \\bar{Y})\\). Think of it as an “anchor” point for the line.\n\n\\(\\hat{\\beta}\\bar{X}\\) tells us where the regression line would predict \\(\\bar{Y}\\) to be based only on the average value of \\(X\\).\n\\(\\bar{Y} - \\hat{\\beta}\\bar{X}\\) is the adjustment needed so that the prediction is correct precisely at the means. It represents the predicted value of \\(Y\\) when \\(X = 0\\), which may or may not be a meaningful value depending on the context (e.g., predicting a company’s profit when revenue is zero might not be sensible).\n\n\n\n1.4.1.3 The Core Idea of “Least Squares”\nThe goal is to minimize the sum of squared residuals (\\(\\sum \\hat{u}_i^2\\)). Why squares? 1. Squaring penalizes large errors more severely than small errors. A residual of 2 is four times “worse” than a residual of 1 \\((2^2 = 4\\) vs. \\(1^2 = 1)\\). This makes the estimator very sensitive to outliers. 2. Squaring ensures all errors are positive. We don’t want positive and negative errors to cancel each other out. 3. The math works out nicely. Minimizing a quadratic function (like the sum of squares) leads to the clean, linear equations (“normal equations”) that give us the formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nThe OLS method therefore finds the unique line that minimizes the total squared vertical distance between the observed data points \\((X_i, Y_i)\\) and the line itself. It’s a best-fit line by its own specific definition of “best” (minimum sum of squared errors).\n\n\n\n1.4.2 Fitting an OLS Model in R\nLet’s use the mtcars dataset to estimate a simple regression model, predicting miles per gallon (mpg) using car weight (wt).\n\n# Load the built-in dataset\ndata(mtcars)\n\n# Estimate the OLS model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print a summary of the results\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe output shows our estimated coefficients is \\(\\hat{\\alpha}\\) (Intercept) = 37.29 and \\(\\hat{\\beta}\\) (wt) = -5.34\nThis gives us the Sample Regression Line\n\\[\\widehat{mpg}_i = 37.29 - 5.34 \\times wt_i\\]\nHence, for a one-ton increase in car weight, we predict miles per gallon will decrease by about 5.34 units.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#derivation-of-the-ols-estimators",
    "href": "intro.html#derivation-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.5 Derivation of the OLS Estimators",
    "text": "1.5 Derivation of the OLS Estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are derived by solving the minimization problem of the Sum of Squared Residuals (SSR). This process involves calculus, specifically taking derivatives and setting them to zero to find the minimum. The resulting equations are called the normal equations.\n\n1.5.1 The Minimization Problem\nWe aim to find the values of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimize: \\[S(\\hat{\\alpha}, \\hat{\\beta}) = \\sum_{i=1}^n \\hat{u}_i^2 = \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\n\n\n1.5.2 The Normal Equations\nTo find the minimum, we take the partial derivatives of \\(S(\\hat{\\alpha}, \\hat{\\beta})\\) with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and set them equal to zero.\n\nDerivative with respect to \\(\\hat{\\alpha}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\alpha}} = -2 \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the first normal equation: \\[\\sum_{i=1}^n Y_i = n\\hat{\\alpha} + \\hat{\\beta} \\sum_{i=1}^n X_i \\quad \\text{(1)}\\]\nDerivative with respect to \\(\\hat{\\beta}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\beta}} = -2 \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the second normal equation: \\[\\sum_{i=1}^n X_iY_i = \\hat{\\alpha} \\sum_{i=1}^n X_i + \\hat{\\beta} \\sum_{i=1}^n X_i^2 \\quad \\text{(2)}\\]\n\n\n\n1.5.3 Solving the Normal Equations\nWe now have a system of two equations with two unknowns (\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)).\n\nSolving for \\(\\hat{\\alpha}\\): Start by rearranging the first normal equation (1): \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] where \\(\\bar{Y} = \\frac{1}{n}\\sum Y_i\\) and \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\). This is our formula for the intercept.\nSolving for \\(\\hat{\\beta}\\): Substitute the expression for \\(\\hat{\\alpha}\\) into the second normal equation (2): \\[\\sum X_iY_i = (\\bar{Y} - \\hat{\\beta}\\bar{X})\\sum X_i + \\hat{\\beta} \\sum X_i^2\\] Solving this for \\(\\hat{\\beta}\\) involves some algebra. Subtract \\(\\bar{Y}\\sum X_i\\) from both sides and factor out \\(\\hat{\\beta}\\): \\[\\sum X_iY_i - \\bar{Y}\\sum X_i = \\hat{\\beta} \\left( \\sum X_i^2 - \\bar{X}\\sum X_i \\right)\\] Note that \\(\\sum X_iY_i - \\bar{Y}\\sum X_i = \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\) and \\(\\sum X_i^2 - \\bar{X}\\sum X_i = \\sum (X_i - \\bar{X})^2\\). This gives us the final formula: \\[\\hat{\\beta} = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#standard-errors-of-the-ols-estimators",
    "href": "intro.html#standard-errors-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.6 Standard Errors of the OLS Estimators",
    "text": "1.6 Standard Errors of the OLS Estimators\nThe OLS estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random variables—their values vary from sample to sample. The standard error measures the precision of these estimates by estimating the standard deviation of their sampling distributions. Smaller standard errors indicate more precise estimates.\n\n1.6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)\nTo conduct statistical inference on our OLS estimate \\(\\hat{\\beta}\\) (e.g., to build confidence intervals or test hypotheses), we need to estimate its sampling variability. This variability is measured by its variance or, more commonly, its standard error.\n\n1.6.1.1 The True Variance of \\(\\hat{\\beta}\\)\nUnder the classical linear model assumptions, the true variance of the OLS slope estimator in a simple regression is given by:\n\\[Var(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nwhere, \\(\\sigma^2 = Var(u_i)\\) is the variance of the unobserved error term, and \\(\\sum_{i=1}^n (X_i - \\bar{X})^2\\) is the total variation of the independent variable \\(X\\) around its mean.\nThis formula shows that the precision of \\(\\hat{\\beta}\\) improves (its variance decreases) when either (1) the error variance (\\(\\sigma^2\\)) is smaller (the data points are tighter around the line, and/or (2) the spread of the explanatory variable \\(X\\) is larger (there is more information in the data).\n\n\n1.6.1.2 Estimating the Unknown Error Variance (\\(\\sigma^2\\))\nSince the error variance \\(\\sigma^2\\) is unknown, we must estimate it using the sample data. An unbiased estimator for \\(\\sigma^2\\) is\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2   \\quad \\text{or}   \\quad \\frac{SSR}{n - k}\\]\nwhere, \\(SSR = \\sum_{i=1}^n \\hat{u}_i^2\\) is the Sum of Squared Residuals (SSR), \\(n\\) is the sample size, and \\(k\\) is the total number of parameters estimated. In a simple regression, we estimate two parameters, i.e. the slope (\\(\\beta\\)) and the intercept (\\(\\alpha\\)), so \\(k=2\\). The term \\(n - k\\) is the degrees of freedom. Using \\(n-k\\) instead of \\(n\\) ensures that \\(E[\\hat{\\sigma}^2] = \\sigma^2\\), making it an unbiased estimator.\n\n\n1.6.1.3 The Standard Error of the Regression (SER)\nThe square root of \\(\\hat{\\sigma}^2\\) is called the Standard Error of the Regression (SER) or the residual standard error. It is an estimate of the standard deviation of the error term \\(u_i\\) and represents the average distance that the observed values fall from the regression line—the typical size of a residual.\n\\[SER = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{SSR}{n - k}}\\]\n\n\n1.6.1.4 The Estimated Variance and Standard Error of \\(\\hat{\\beta}\\)\nBy plugging the estimate \\(\\hat{\\sigma}^2\\) into the true variance formula, we obtain the estimated variance of the OLS estimator\n\\[\\widehat{Var}(\\hat{\\beta}) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nThe standard error of \\(\\hat{\\beta}\\) is simply the square root of this estimated variance. It is the estimated standard deviation of the sampling distribution of \\(\\hat{\\beta}\\).\n\\[SE(\\hat{\\beta}) = \\sqrt{\\widehat{Var}(\\hat{\\beta})} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}} \\quad \\text{or} \\quad \\hat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n x^2}}\\]\nThis final formula is the most intuitive: the standard error of the coefficient depends directly on the “noise” in the model (\\(SER\\)) and inversely on the amount of information in the explanatory variable, i.e. \\(\\sqrt{\\sum_{i=1}^n x^2}\\).\n\n\n\n1.6.2 What Drives the Standard Error?\nThe formula for \\(SE(\\hat{\\beta})\\) provides deep intuition about what makes an estimate precise: 1. Spread of the error term (\\(\\hat{\\sigma}\\)): A larger error variance (a noisier relationship, where points are scattered farther from the line) leads to a larger standard error and less precise estimates. 2. Sample size (\\(n\\)): A larger sample size \\(n\\) will (all else equal) make \\(\\hat{\\sigma}\\) smaller and the denominator larger, leading to a smaller standard error and more precise estimates. 3. Spread of the regressor \\(X\\) (\\(SST_X\\)): More variation in the independent variable \\(X\\) provides more “information” and leads to a smaller standard error. If all values of \\(X\\) are clustered closely together, it is harder to pin down the slope of the relationship.\nThe standard error for the intercept \\(\\hat{\\alpha}\\) has a more complex formula but is driven by the same factors: \\(n\\), \\(\\hat{\\sigma}\\), and the spread of \\(X\\).\n\n\n1.6.3 Calculating Standard Errors\nUsually, you don’t need to calculate these by hand. In R, the summary() function computes them automatically using the formulas above.\n\n# Re-running the model from earlier for clarity\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# The summary output shows the coefficients and their standard errors\nsummary_model &lt;- summary(model)\nprint(summary_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIn the output, the Std. Error column next to the (Intercept) and wt estimates contains the calculated \\(SE(\\hat{\\alpha})\\) and \\(SE(\\hat{\\beta})\\). These values are used to compute the t-statistics and p-values for hypothesis testing, allowing us to assess the statistical significance of our estimates.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "href": "intro.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.8 Measures of Fit: How Well Does the Line Explain the Data?",
    "text": "1.8 Measures of Fit: How Well Does the Line Explain the Data?\nOnce we have our regression line, we want to know how well it fits the data. We decompose the total variation in \\(Y\\):\n\nSST (Total Sum of Squares): Total variation in \\(Y\\) around its mean. \\(SST = \\sum (Y_i - \\bar{Y})^2\\)\nSSE (Explained Sum of Squares): Variation in \\(Y\\) explained by the model. \\(SSE = \\sum (\\hat{Y}_i - \\bar{Y})^2\\)\nSSR (Residual Sum of Squares): Variation in \\(Y\\) not explained by the model. \\(SSR = \\sum \\hat{u}_i^2\\)\n\nThey are related by the identity: \\(SST = SSE + SSR\\).\n\n1.8.1 The R-Squared (\\(R^2\\))\nThe most common measure of fit is the R-squared statistic. It represents the fraction of the sample variation in \\(Y\\) that is explained by \\(X\\).\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\\(R^2\\) always lies between 0 and 1.\nAn \\(R^2\\) of 0 means \\(X\\) explains none of the variation in \\(Y\\).\nAn \\(R^2\\) of 1 means \\(X\\) explains all of the variation in \\(Y\\).\nIn a simple regression, \\(R^2\\) is also the square of the correlation coefficient between \\(X\\) and \\(Y\\), that is,\\(R^2 = r_{xy}^2\\).\n\nIn our mtcars example, the \\(R^2\\) is 0.7528. This means that about 75% of the variation in miles per gallon is explained by the weight of the car.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "intro.html#hypothesis-testing-and-standard-errors",
    "href": "intro.html#hypothesis-testing-and-standard-errors",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.7 Hypothesis Testing and Standard Errors",
    "text": "1.7 Hypothesis Testing and Standard Errors\nOur estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random—they would change if we collected a new sample. To conduct statistical inference, we need to estimate their variability, which is captured by the Standard Error (S.E.).\nThe most common hypothesis test in regression is whether the slope coefficient \\(\\beta\\) is statistically different from zero.\n\\(H_0: \\beta = 0\\) (There is no relationship between \\(X\\) and \\(Y\\)) vs. \\(H_1: \\beta \\neq 0\\) (There is a relationship between \\(X\\) and \\(Y\\))\nWe use a t-test to evaluate this hypothesis, where the test statistic is\n\\[TS = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\]\nYou can reject the null hypothesis (\\(H_0\\)) if\n\n|t-statistic| &gt; critical value (approx. “2” for a 5% significance level)\np-value &lt; significance level (e.g., \\(\\alpha = 0.05\\)). The p-value is the probability of observing a result as extreme as the one in your sample if the null hypothesis were true.\nIf the 95% confidence interval \\([\\hat{\\beta} - 1.96 \\cdot SE(\\hat{\\beta}), \\hat{\\beta} + 1.96 \\cdot SE(\\hat{\\beta})]\\) does not contain zero.\n\nIn our mtcars output\n\nThe t-statistic for wt is -9.559.\nThe p-value is 1.29e-10 (effectively 0), which is much less than 0.05.\nThe 95% confidence interval can be calculated with confint(model) and will not contain zero.\n\nConclusion: We strongly reject the null hypothesis. Hence there is a statistically significant relationship between car weight and fuel efficiency at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "assumptions.html#clrm-assumptions",
    "href": "assumptions.html#clrm-assumptions",
    "title": "2  The Classical Linear Model (Gauss-Markov) Assumptions",
    "section": "",
    "text": "Interpretation: This means that the explanatory variable \\(X\\) provides no information about the mean of the unobserved factors. On average, the positive and negative omitted effects cancel out. This is the single most important assumption.\nImplication: It implies that the model is correctly specified in its functional form and that there are no omitted variables that are correlated with \\(X\\). If this assumption fails, our OLS estimates are biased.\n\n\n\n\n\nInterpretation: The variance of the unobserved factors is constant across all values of \\(X\\). The spread of the data points around the regression line is the same whether \\(X\\) is small or large.\nImplication: If this holds, OLS standard errors are valid. If it fails, we have heteroskedasticity, which means OLS estimates are still unbiased but their standard errors are incorrect. This leads to faulty hypothesis tests and confidence intervals.\n\n\n\n\n\nInterpretation: The unobserved factors affecting \\(Y\\) for one observation are not correlated with the unobserved factors affecting \\(Y\\) for any other observation. In cross-sectional data, this is usually guaranteed by random sampling. However autocorrelation is usually a concern for time-series data.\nImplication: Like heteroskedasticity, if this assumption fails, OLS estimates remain unbiased but the standard errors are incorrect/inefficient, leading to unreliable inference.\n\n\n\n\n\nInterpretation: This is essentially a weaker version of Assumption 1. It means \\(X\\) is not influenced by the unobserved factors in \\(u\\).\nImplication: This assumption is crucial especially for causal interpretation. If \\(X\\) is correlated with \\(u\\), it could mean an omitted variable that affects \\(Y\\) is also correlated with \\(X\\) (a.k.a confounding, or the “third variable” problem). This is the famous omitted variable bias, which causes \\(\\hat{\\beta}\\) to be biased and inconsistent.\n\n\n\nInterpretation (for simple regression): In the simple regression model with one explanatory variable, this assumption is automatically satisfied as long as \\(X\\) is not constant.\nInterpretation (for multiple regression): This assumption becomes critical when we have more than one explanatory variable. It states that no independent variable is a perfect linear combination of another independent variable(s).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Classical Linear Model (Gauss-Markov) Assumptions</span>"
    ]
  },
  {
    "objectID": "multiple.html#the-trivariate-model-interpretation",
    "href": "multiple.html#the-trivariate-model-interpretation",
    "title": "3  The Multiple Regression Model",
    "section": "",
    "text": "\\(\\beta_1\\) is the marginal effect of \\(X_1\\) on \\(Y\\). It is the effect of a small change in the \\(X_1\\) on the dependent variable, while holding \\(X_2\\) constant.\n\\(\\beta_2\\) is the marginal effect of \\(X_2\\) on \\(Y\\), while holding \\(X_1\\) constant.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Multiple Regression Model</span>"
    ]
  },
  {
    "objectID": "simple.html",
    "href": "simple.html",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "1.1 Introduction\nEconomic theory suggests relationships between variables, but it rarely provides the quantitative magnitude of these causal effects. For example, we are interested in questions such as:\nIdeally, we would answer these questions with controlled experiments. However, this is often impractical, unethical, or impossible. Instead, econometricians must rely on observational data.\nThe core challenge with observational studies is that correlation does not imply causation. Some major threats to establishing a proper empirical understanding of economic relationships are:\nAs a way of introduction, we introduce the primary tools used to estimate relationships from observational data in econometrics: the Ordinary Least Squares (OLS) method.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#introduction",
    "href": "simple.html#introduction",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "",
    "text": "What is the effect of reducing class size on student academic performance?\nWhat is the price elasticity of cigarettes?\nWhat is the return to an additional year of education?\nHow does a 1 percentage point increase in interest rates affect output growth?\n\n\n\n\nOmitted Variable Bias (Confounding Factors): A variable we have not accounted for is influencing both the dependent and independent variable.\nSimultaneous Causality: Two variables influence each other simultaneously (e.g., police numbers and crime rates).\nSample Selection Bias: The process by which data is collected influences the availability of data, leading to a non-random sample that may not represent the population of interest.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#types-of-data",
    "href": "simple.html#types-of-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.2 Types of Data",
    "text": "1.2 Types of Data\nBefore we begin, it’s useful to recognize the common structures of econometric data:\n\nCross-sectional: Data on multiple entities (individuals, firms, countries) at a single point in time.\nTime-series: Data on a single entity collected at multiple time periods (e.g., daily, quarterly, yearly).\nPanel/Longitudinal: Data on multiple entities where each entity is observed at multiple time periods. This combines cross-sectional and time-series dimensions.\nPooled Cross-sectional: Multiple cross-sectional samples taken at different points in time, where the entities in each sample are different.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#the-simple-regression-model",
    "href": "simple.html#the-simple-regression-model",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.3 The Simple Regression Model",
    "text": "1.3 The Simple Regression Model\nLet’s begin by investigating the linear relationship between two variables, \\(Y\\) (the dependent variable) and \\(X\\) (the independent or explanatory variable).\n\n1.3.1 The Population Regression Function (PRF)\nImagine we could collect data on everyone in the population of interest. The true relationship in the population is given by the Population Regression Function:\n\\[Y_i = \\alpha + \\beta X_i + u_i\\]\n\n\\(Y_i\\) is the dependent variable for observation \\(i\\).\n\\(X_i\\) is the independent variable for observation \\(i\\).\n\\(\\alpha\\) is the population intercept.\n\\(\\beta\\) is the population slope coefficient (the parameter of primary interest).\n\\(u_i\\) is the error term, which contains all factors other than \\(X\\) that influence \\(Y\\).\n\nWe can never observe the true PRF because we cannot collect data on the entire population. The error term \\(u_i\\) exists due to: (1) The inherent randomness of human behavior, (2) Unavailable or incomplete data, (3) Omitted variables from the model, (4) Imperfect functional form specification, (5) Aggregation errors, (6) Measurement errors.\n\n\n1.3.2 The Sample Regression Function (SRF)\nSince we can’t work with the population, we take a sample and use it to estimate the PRF. The estimated model is called the Sample Regression Function:\n\\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\\]\n\\[Y_i = \\hat{\\alpha} + \\hat{\\beta} X_i + \\hat{u}_i\\]\n\n\\(\\hat{Y}_i\\) is the predicted or fitted value of \\(Y_i\\).\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are the estimators of the population parameters \\(\\alpha\\) and \\(\\beta\\). These coefficients are calculated from our sample data.\n\\(\\hat{u}_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\), which is our estimate of the unobserved error term \\(u_i\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#the-ordinary-least-squares-ols-method",
    "href": "simple.html#the-ordinary-least-squares-ols-method",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.4 The Ordinary Least Squares (OLS) Method",
    "text": "1.4 The Ordinary Least Squares (OLS) Method\nHow do we find the “best” line through our scatter of data points? The OLS method chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimizes the Sum of Squared Residuals (SSR).\n\\[\\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n \\hat{u}_i^2 = \\min_{\\hat{\\alpha}, \\hat{\\beta}} \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\nThe formulas for the OLS estimators, derived by solving this minimization problem (the “normal equations”), are:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]\n\\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\n\n1.4.1 Intuition behind the OLS estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) aren’t arbitrary; they are the direct mathematical solution to the problem of minimizing the sum of squared residuals. But we can also understand them intuitively.\n\n1.4.1.1 Intuition for the Slope (\\(\\hat{\\beta}\\))\nLet’s look at the formula for the slope estimator more closely:\n\\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\n\nDividing the numerator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\), which is the sample covariance between \\(X\\) and \\(Y\\). Recall that the covariance measures how two variables move together:\n\nIf \\(X\\) is above its mean when \\(Y\\) is above its mean (and vice versa), the products \\((X_i - \\bar{X})(Y_i - \\bar{Y})\\) will be positive, leading to a positive covariance and a positive \\(\\hat{\\beta}\\).\nIf \\(X\\) is above its mean when \\(Y\\) is below its mean, the products will be negative, leading to a negative covariance and a negative \\(\\hat{\\beta}\\).\n\nNote also that dividing the denominator by \\(n\\) gives \\(\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2\\), which is the sample variance of \\(X\\). The variance measures the spread or variation of \\(X\\) around its own mean.\n\nSo, we can think of \\(\\hat{\\beta}\\) as: \\[\\hat{\\beta} = \\frac{\\text{Sample Covariance between X and Y}}{\\text{Sample Variance of X}}\\]\nIn other words, the OLS slope estimator answers the question: “For a given amount of movement in \\(X\\), how much associated movement do we see in \\(Y\\)?” It scales the co-movement of \\(X\\) and \\(Y\\) by the movement in \\(X\\) itself. A steeper slope (larger \\(|\\hat{\\beta}|\\)) means a unit change in \\(X\\) is associated with a larger change in \\(Y\\).\n\n\n1.4.1.2 Intuition for the Intercept (\\(\\hat{\\alpha}\\))\nThe formula for the intercept is: \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\]\nThis ensures that the regression line always passes through the point of the means \\((\\bar{X}, \\bar{Y})\\). Think of it as an “anchor” point for the line.\n\n\\(\\hat{\\beta}\\bar{X}\\) tells us where the regression line would predict \\(\\bar{Y}\\) to be based only on the average value of \\(X\\).\n\\(\\bar{Y} - \\hat{\\beta}\\bar{X}\\) is the adjustment needed so that the prediction is correct precisely at the means. It represents the predicted value of \\(Y\\) when \\(X = 0\\), which may or may not be a meaningful value depending on the context (e.g., predicting a company’s profit when revenue is zero might not be sensible).\n\n\n\n1.4.1.3 The Core Idea of “Least Squares”\nThe goal is to minimize the sum of squared residuals (\\(\\sum \\hat{u}_i^2\\)). Why squares? 1. Squaring penalizes large errors more severely than small errors. A residual of 2 is four times “worse” than a residual of 1 \\((2^2 = 4\\) vs. \\(1^2 = 1)\\). This makes the estimator very sensitive to outliers. 2. Squaring ensures all errors are positive. We don’t want positive and negative errors to cancel each other out. 3. The math works out nicely. Minimizing a quadratic function (like the sum of squares) leads to the clean, linear equations (“normal equations”) that give us the formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nThe OLS method therefore finds the unique line that minimizes the total squared vertical distance between the observed data points \\((X_i, Y_i)\\) and the line itself. It’s a best-fit line by its own specific definition of “best” (minimum sum of squared errors).\n\n\n\n1.4.2 Fitting an OLS Model in R\nLet’s use the mtcars dataset to estimate a simple regression model, predicting miles per gallon (mpg) using car weight (wt).\n\n# Load the built-in dataset\ndata(mtcars)\n\n# Estimate the OLS model\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print a summary of the results\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe output shows our estimated coefficients is \\(\\hat{\\alpha}\\) (Intercept) = 37.29 and \\(\\hat{\\beta}\\) (wt) = -5.34\nThis gives us the Sample Regression Line\n\\[\\widehat{mpg}_i = 37.29 - 5.34 \\times wt_i\\]\nHence, for a one-ton increase in car weight, we predict miles per gallon will decrease by about 5.34 units.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#derivation-of-the-ols-estimators",
    "href": "simple.html#derivation-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.5 Derivation of the OLS Estimators",
    "text": "1.5 Derivation of the OLS Estimators\nThe formulas for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are derived by solving the minimization problem of the Sum of Squared Residuals (SSR). This process involves calculus, specifically taking derivatives and setting them to zero to find the minimum. The resulting equations are called the normal equations.\n\n1.5.1 The Minimization Problem\nWe aim to find the values of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) that minimize: \\[S(\\hat{\\alpha}, \\hat{\\beta}) = \\sum_{i=1}^n \\hat{u}_i^2 = \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i)^2\\]\n\n\n1.5.2 The Normal Equations\nTo find the minimum, we take the partial derivatives of \\(S(\\hat{\\alpha}, \\hat{\\beta})\\) with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and set them equal to zero.\n\nDerivative with respect to \\(\\hat{\\alpha}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\alpha}} = -2 \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the first normal equation: \\[\\sum_{i=1}^n Y_i = n\\hat{\\alpha} + \\hat{\\beta} \\sum_{i=1}^n X_i \\quad \\text{(1)}\\]\nDerivative with respect to \\(\\hat{\\beta}\\): \\[\\frac{\\partial S}{\\partial \\hat{\\beta}} = -2 \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\] This simplifies to the second normal equation: \\[\\sum_{i=1}^n X_iY_i = \\hat{\\alpha} \\sum_{i=1}^n X_i + \\hat{\\beta} \\sum_{i=1}^n X_i^2 \\quad \\text{(2)}\\]\n\n\n\n1.5.3 Solving the Normal Equations\nWe now have a system of two equations with two unknowns (\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)).\n\nSolving for \\(\\hat{\\alpha}\\): Start by rearranging the first normal equation (1): \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] where \\(\\bar{Y} = \\frac{1}{n}\\sum Y_i\\) and \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\). This is our formula for the intercept.\nSolving for \\(\\hat{\\beta}\\): Substitute the expression for \\(\\hat{\\alpha}\\) into the second normal equation (2): \\[\\sum X_iY_i = (\\bar{Y} - \\hat{\\beta}\\bar{X})\\sum X_i + \\hat{\\beta} \\sum X_i^2\\] Solving this for \\(\\hat{\\beta}\\) involves some algebra. Subtract \\(\\bar{Y}\\sum X_i\\) from both sides and factor out \\(\\hat{\\beta}\\): \\[\\sum X_iY_i - \\bar{Y}\\sum X_i = \\hat{\\beta} \\left( \\sum X_i^2 - \\bar{X}\\sum X_i \\right)\\] Note that \\(\\sum X_iY_i - \\bar{Y}\\sum X_i = \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\) and \\(\\sum X_i^2 - \\bar{X}\\sum X_i = \\sum (X_i - \\bar{X})^2\\). This gives us the final formula: \\[\\hat{\\beta} = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#standard-errors-of-the-ols-estimators",
    "href": "simple.html#standard-errors-of-the-ols-estimators",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.6 Standard Errors of the OLS Estimators",
    "text": "1.6 Standard Errors of the OLS Estimators\nThe OLS estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random variables—their values vary from sample to sample. The standard error measures the precision of these estimates by estimating the standard deviation of their sampling distributions. Smaller standard errors indicate more precise estimates.\n\n1.6.1 The Formula for the Standard Error of \\(\\hat{\\beta}\\)\nTo conduct statistical inference on our OLS estimate \\(\\hat{\\beta}\\) (e.g., to build confidence intervals or test hypotheses), we need to estimate its sampling variability. This variability is measured by its variance or, more commonly, its standard error.\nThe True Variance of \\(\\hat{\\beta}\\)\nUnder the classical linear model assumptions, the true variance of the OLS slope estimator in a simple regression is given by:\n\\[Var(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nwhere, \\(\\sigma^2 = Var(u_i)\\) is the variance of the unobserved error term, and \\(\\sum_{i=1}^n (X_i - \\bar{X})^2\\) is the total variation of the independent variable \\(X\\) around its mean.\nThis formula shows that the precision of \\(\\hat{\\beta}\\) improves (its variance decreases) when either (1) the error variance (\\(\\sigma^2\\)) is smaller (the data points are tighter around the line, and/or (2) the spread of the explanatory variable \\(X\\) is larger (there is more information in the data).\nEstimating the Unknown Error Variance (\\(\\sigma^2\\))\nSince the error variance \\(\\sigma^2\\) is unknown, we must estimate it using the sample data. An unbiased estimator for \\(\\sigma^2\\) is\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - k} \\sum_{i=1}^n \\hat{u}_i^2   \\quad \\text{or}   \\quad \\frac{SSR}{n - k}\\]\nwhere, \\(SSR = \\sum_{i=1}^n \\hat{u}_i^2\\) is the Sum of Squared Residuals (SSR), \\(n\\) is the sample size, and \\(k\\) is the total number of parameters estimated. In a simple regression, we estimate two parameters, i.e. the slope (\\(\\beta\\)) and the intercept (\\(\\alpha\\)), so \\(k=2\\). The term \\(n - k\\) is the degrees of freedom. Using \\(n-k\\) instead of \\(n\\) ensures that \\(E[\\hat{\\sigma}^2] = \\sigma^2\\), making it an unbiased estimator.\nThe Standard Error of the Regression (SER)\nThe square root of \\(\\hat{\\sigma}^2\\) is called the Standard Error of the Regression (SER) or the residual standard error. It is an estimate of the standard deviation of the error term \\(u_i\\) and represents the average distance that the observed values fall from the regression line—the typical size of a residual.\n\\[SER = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{SSR}{n - k}}\\]\nThe Estimated Variance and Standard Error of \\(\\hat{\\beta}\\)\nBy plugging the estimate \\(\\hat{\\sigma}^2\\) into the true variance formula, we obtain the estimated variance of the OLS estimator\n\\[\\widehat{Var}(\\hat{\\beta}) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\]\nThe standard error of \\(\\hat{\\beta}\\) is simply the square root of this estimated variance. It is the estimated standard deviation of the sampling distribution of \\(\\hat{\\beta}\\).\n\\[SE(\\hat{\\beta}) = \\sqrt{\\widehat{Var}(\\hat{\\beta})} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}} \\quad \\text{or} \\quad \\hat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n x^2}}\\]\nThis final formula is the most intuitive: the standard error of the coefficient depends directly on the “noise” in the model (\\(SER\\)) and inversely on the amount of information in the explanatory variable, i.e. \\(\\sqrt{\\sum_{i=1}^n x^2}\\).\n\n\n1.6.2 What Drives the Standard Error?\nThe formula for \\(SE(\\hat{\\beta})\\) provides deep intuition about what makes an estimate precise: 1. Spread of the error term (\\(\\hat{\\sigma}\\)): A larger error variance (a noisier relationship, where points are scattered farther from the line) leads to a larger standard error and less precise estimates. 2. Sample size (\\(n\\)): A larger sample size \\(n\\) will (all else equal) make \\(\\hat{\\sigma}\\) smaller and the denominator larger, leading to a smaller standard error and more precise estimates. 3. Spread of the regressor \\(X\\) (\\(SST_X\\)): More variation in the independent variable \\(X\\) provides more “information” and leads to a smaller standard error. If all values of \\(X\\) are clustered closely together, it is harder to pin down the slope of the relationship.\nThe standard error for the intercept \\(\\hat{\\alpha}\\) has a more complex formula but is driven by the same factors: \\(n\\), \\(\\hat{\\sigma}\\), and the spread of \\(X\\).\n\n\n1.6.3 Calculating Standard Errors\nUsually, you don’t need to calculate these by hand. In R, the summary() function computes them automatically using the formulas above.\n\n# Re-running the model from earlier for clarity\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\n\n# The summary output shows the coefficients and their standard errors\nsummary_model &lt;- summary(model)\nprint(summary_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIn the output, the Std. Error column next to the (Intercept) and wt estimates contains the calculated \\(SE(\\hat{\\alpha})\\) and \\(SE(\\hat{\\beta})\\). These values are used to compute the t-statistics and p-values for hypothesis testing, allowing us to assess the statistical significance of our estimates.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#hypothesis-testing-and-standard-errors",
    "href": "simple.html#hypothesis-testing-and-standard-errors",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.7 Hypothesis Testing and Standard Errors",
    "text": "1.7 Hypothesis Testing and Standard Errors\nOur estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are random—they would change if we collected a new sample. To conduct statistical inference, we need to estimate their variability, which is captured by the Standard Error (S.E.).\nThe most common hypothesis test in regression is whether the slope coefficient \\(\\beta\\) is statistically different from zero.\n\\(H_0: \\beta = 0\\) (There is no relationship between \\(X\\) and \\(Y\\)) vs. \\(H_1: \\beta \\neq 0\\) (There is a relationship between \\(X\\) and \\(Y\\))\nWe use a t-test to evaluate this hypothesis, where the test statistic is\n\\[TS = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\]\nYou can reject the null hypothesis (\\(H_0\\)) if\n\n|t-statistic| &gt; critical value (approx. “2” for a 5% significance level)\np-value &lt; significance level (e.g., \\(\\alpha = 0.05\\)). The p-value is the probability of observing a result as extreme as the one in your sample if the null hypothesis were true.\nIf the 95% confidence interval \\([\\hat{\\beta} - 1.96 \\cdot SE(\\hat{\\beta}), \\hat{\\beta} + 1.96 \\cdot SE(\\hat{\\beta})]\\) does not contain zero.\n\nIn our mtcars output\n\nThe t-statistic for wt is -9.559.\nThe p-value is 1.29e-10 (effectively 0), which is much less than 0.05.\nThe 95% confidence interval can be calculated with confint(model) and will not contain zero.\n\nConclusion: We strongly reject the null hypothesis. Hence there is a statistically significant relationship between car weight and fuel efficiency at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "simple.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "href": "simple.html#measures-of-fit-how-well-does-the-line-explain-the-data",
    "title": "1  Simple Regression Model: Basics of OLS",
    "section": "1.8 Measures of Fit: How Well Does the Line Explain the Data?",
    "text": "1.8 Measures of Fit: How Well Does the Line Explain the Data?\nOnce we have our regression line, we want to know how well it fits the data. We decompose the total variation in \\(Y\\):\n\nSST (Total Sum of Squares): Total variation in \\(Y\\) around its mean. \\(SST = \\sum (Y_i - \\bar{Y})^2\\)\nSSE (Explained Sum of Squares): Variation in \\(Y\\) explained by the model. \\(SSE = \\sum (\\hat{Y}_i - \\bar{Y})^2\\)\nSSR (Residual Sum of Squares): Variation in \\(Y\\) not explained by the model. \\(SSR = \\sum \\hat{u}_i^2\\)\n\nThey are related by the identity: \\(SST = SSE + SSR\\).\n\n1.8.1 The R-Squared (\\(R^2\\))\nThe most common measure of fit is the R-squared statistic. It represents the fraction of the sample variation in \\(Y\\) that is explained by \\(X\\).\n\\[R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\]\n\n\\(R^2\\) always lies between 0 and 1.\nAn \\(R^2\\) of 0 means \\(X\\) explains none of the variation in \\(Y\\).\nAn \\(R^2\\) of 1 means \\(X\\) explains all of the variation in \\(Y\\).\nIn a simple regression, \\(R^2\\) is also the square of the correlation coefficient between \\(X\\) and \\(Y\\), that is,\\(R^2 = r_{xy}^2\\).\n\nIn our mtcars example, the \\(R^2\\) is 0.7528. This means that about 75% of the variation in miles per gallon is explained by the weight of the car.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Regression Model: Basics of OLS</span>"
    ]
  },
  {
    "objectID": "violationofassumptions.html",
    "href": "violationofassumptions.html",
    "title": "4  Violation of CLRM Assumptions",
    "section": "",
    "text": "4.1 The CLRM Assumptions Again\nWe have seen the assumptions for the Ordinary Least Squares (OLS) estimators \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}'s\\) to have desirable properties. What does it mean that assumptions do not hold?.\nConsider the following bivariate population model:\n\\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\]\nViolation of Assumption 4: Endogeneity\nWe start with this one, as it is the most commons and serious of many econometric specifications. Basically we have endogeneity when any of the explanatory variable \\(X's\\) is correlated with the error term \\(u\\).\n\\[Cov(X_i's, u_i) \\neq 0\\]\n\\[E(u_i | X_i's) \\neq 0\\]\nViolation of Assumption 2: Heteroskedasticity\nThe error term \\(u\\) has non-constant variances given the values of the explanatory variables.\n\\[Var(u_i | X_i's) \\neq \\sigma^2\\]\nViolation of Assumption 3: Autocorrelation\nThe error terms for any two or more different observations are correlated.\n\\[Cov(u_i, u_j | X_i) \\neq 0 \\quad \\text{for all } i \\neq j\\]\nViolation of Assumption 5: Perfect (or near perfect) Multicollinearity",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Violation of CLRM Assumptions</span>"
    ]
  },
  {
    "objectID": "violationofassumptions.html#the-clrm-assumptions-again",
    "href": "violationofassumptions.html#the-clrm-assumptions-again",
    "title": "4  Violation of CLRM Assumptions",
    "section": "",
    "text": "This basically means that there are other important factors that affect the dependent variable \\(Y_i\\) which if not included in the regression model will result in incorrect estimates!\nImplication: Essentially \\(\\hat{\\beta}\\) will be biased and inconsistent.\nViolation of Assumption 1: Conditional mean is not zero\nEndogeniety or violation of assumption 4 implies that Assumption 1, the zero conditional mean, also does not hold! The error term \\(u\\) will not have expected value of zero, given the values of the explanatory variables \\(X's\\).\n\n\n\n\n\n\nImplication: In this case, OLS estimates are still unbiased but their standard errors are incorrect/inefficient, which leads to faulty hypothesis tests and confidence intervals.\n\n\n\n\n\nImplication: Like heteroskedasticity, if we have autocorrelation then OLS estimates remain unbiased but the standard errors are incorrect/inefficient, leading to unreliable inference.\nNote: Autocorrelation (a.k.a. serial autocorrelation) is usually a concern for time-series data, i.e. usually \\(Cov(u_t , u\\_{t-1}) \\neq 0\\).\n\n\n\nInterpretation (for simple regression): In the simple regression model with one explanatory variable, this assumption is automatically satisfied as long as \\(X\\) is not constant.\nImplication: When we have perfect multicollinearity, any independent variable is a perfect linear combination of other independent variable(s), then we canot get OLS estimates, and econometric packages will often delete one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Violation of CLRM Assumptions</span>"
    ]
  },
  {
    "objectID": "violationofassumptions.html#checking-assumptions",
    "href": "violationofassumptions.html#checking-assumptions",
    "title": "4  Violation of CLRM Assumptions",
    "section": "4.2 Checking Assumptions",
    "text": "4.2 Checking Assumptions\n\n4.2.1 Testing for Heteroskedasticity\nAs previously mentioned, a quick look at residual plots is useful a good start to vidually look for hoteroskedasticity and/or non-zero mean.\n\n# Fit the model\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Create a dataframe of fitted values and residuals\ndiagnostic_data &lt;- data.frame(\n  fitted = fitted(model),\n  residuals = resid(model)\n)\n\n# Plot residuals vs. fitted values\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nggplot(diagnostic_data, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values (Y_hat)\",\n       y = \"Residuals (u_hat)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nA more formal test is the White test, a general and powerful test for heteroskedasticity. The test works by regressing the squared residuals from the original model on the original explanatory variables, their squares, and their cross-products.\nThe null and alternative hypotheses are:\n\\(H_0\\): Homoskedasticity exists (the error variance is constant).\n\\(H_1\\): Heteroskedasticity exists (the error variance is not constant).\n\n# install.packages(\"lmtest\") # Uncomment and run if needed\nlibrary(lmtest)\n\n# Perform the White test for the simple model mpg ~ wt\nbptest(model, ~ wt * hp + I(wt^2) + I(hp^2), data = mtcars)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 6.5431, df = 5, p-value = 0.2569\n\n\nThe bptest() output shows a BP test statistic and a p-value. A p-value &lt; 0.05 means you reject the null hypothesis of homoskedasticity. In the mtcars example above, we fail to reject the null hypothesis, suggesting that we have may not have heteroskedasticty.\nAlternatively:\n\n# 1. Estimate the original simple regression model\nsimple_model &lt;- lm(mpg ~ wt, data = mtcars)\n\n# 2. Obtain the squared residuals from the model\nsquared_residuals &lt;- resid(simple_model)^2\n\n# 3. Perform the \"auxiliary regression\" for the White test:\n# Regress the squared residuals on the original regressor (wt) and its square (wt²).\nwhite_aux_model &lt;- lm(squared_residuals ~ wt + I(wt^2), data = mtcars)\n\n# 4. Conduct an F-test on the auxiliary model.\n# The null hypothesis is that the coefficients on 'wt' and 'I(wt^2)' are zero.\n# install.packages(\"car\") # Uncomment and run if you don't have the 'car' package\nlibrary(car)\n\nlinearHypothesis(white_aux_model, c(\"wt=0\", \"I(wt^2)=0\"))\n\n\nLinear hypothesis test:\nwt = 0\nI(wt^2) = 0\n\nModel 1: restricted model\nModel 2: squared_residuals ~ wt + I(wt^2)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     31 4542.5                           \n2     29 4348.6  2    193.95 0.6467 0.5312\n\n\nWe look at the F-statistic and its p-value (Pr(&gt;F)). A p-value &lt; 0.05 provides evidence to reject the null hypothesis (\\(H_0\\)) of homoskedasticity, thereby suggesting the error variance is not constant and depends on weight (wt). Here, again Pr(&gt;F) is 0.5312, hence we fail to reject the null.\n\n\n4.2.2 Testing for Autocorrelation\n\n\n4.2.3 Testing for Multicollinearity\n\n\n4.2.4 Testing for Endogeneity\nEndogeneity occurs when an explanatory variable is correlated with the error term (\\(Cov(X, u) \\neq 0\\)), violating a key Gauss-Markov assumption (4). This often arises from:\n\nConfounding leading to omitted variable bias\nReverse causality\nMeasurement Error\nSimultaneity\n\nThe consequence is that the OLS estimator becomes biased and inconsistent.\nThe Logic of the Hausman Test\nThe test follows a straightforward logic:\n\nNull Hypothesis (\\(H_0\\)): The variable in question is exogenous (\\(Cov(X, u) = 0\\)). OLS is consistent and efficient.\nAlternative Hypothesis (\\(H_1\\)): The variable is endogenous (\\(Cov(X, u) \\neq 0\\)). OLS is inconsistent.\n\nThe test compares two estimators:\n\nOLS Estimator: Efficient (has the smallest possible variance) under \\(H_0\\), but inconsistent under \\(H_1\\).\nIV (Instrumental Variables) Estimator: Consistent under both \\(H_0\\) and \\(H_1\\), but inefficient (has larger variance) under \\(H_0\\).\n\nIf the variable is exogenous (\\(H_0\\) is true), the OLS and IV estimates should be similar. If they are significantly different, we have evidence that endogeneity is present (\\(H_1\\) is true).\nImplementing the Hausman Test in R: A Step-by-Step Guide\nThe test is implemented as a Durbin-Wu-Hausman test via a convenient auxiliary regression.\nPrerequisite: You must have at least one valid instrument for the potentially endogenous variable. A valid instrument must be:\n\nRelevant: Correlated with the endogenous variable.\nExogenous: Not correlated with the error term (\\(Cov(Z, u) = 0\\)).\n\nScenario: Suppose we fear that wt (weight) is endogenous in our model mpg ~ wt. Suppose we use hp (horsepower) as instruments.\nStep 1: Estimate the First Stage Regression\nRegress the potentially endogenous variable (hp) on all exogenous variables and the instruments.\n\n# First Stage: Regress the endogenous variable on instruments and other exogenous vars\nfirst_stage &lt;- lm(hp ~ wt + hp, data = mtcars)\n\n# Retrieve the residuals from the first stage\nfirst_stage_residuals &lt;- resid(first_stage)\n\n# Add these residuals to the original dataset for the next step\nmtcars$fs_resid &lt;- first_stage_residuals\n\nStep 2: Estimate the Auxiliary Regression Run the original model, but include the first-stage residuals as an additional regressor.\n\n# Auxiliary Regression: Original model + first stage residuals\nauxiliary_model &lt;- lm(mpg ~ wt + fs_resid, data = mtcars)\nsummary(auxiliary_model)\n\n\nCall:\nlm(formula = mpg ~ wt + fs_resid, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.28513    1.59870  23.322  &lt; 2e-16 ***\nwt          -5.34447    0.47605 -11.227 4.49e-12 ***\nfs_resid    -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nStep 3: Interpret the Result - The key is the t-test on the coefficient of the residual variable (fs_resid). - Null Hypothesis (\\(H_0\\)): The coefficient on the residuals is zero. This means the variable (wt) is exogenous. - Alternative Hypothesis (\\(H_1\\)): The coefficient on the residuals is not zero. This is evidence of endogeneity.\nA low p-value (typically &lt; 0.05) on the fs_resid coefficient leads to a rejection of the null hypothesis, suggesting that wt is indeed endogenous.\nUsing the ivreg and lmtest packages\nA more efficient method is to use the ivreg() function from the AER package and then formally test for endogeneity.\nSpecifying a model with instruments: The syntax y ~ x1 + x2 | z1 + z2 means that we are regressing y on x1 and x2, using for x2 instruments z1 and z2 (and x1 is included as its own instrument).\n\n# install.packages(\"AER\") # Install the Applied Econometrics with R package\nlibrary(AER)\nlibrary(lmtest)\n\n# 1. Estimate the model via IV and OLS\n# IV model: Specify the formula and instruments\niv_model &lt;- ivreg(mpg ~ wt | hp, data = mtcars)\n\n# 2. Perform the Hausman test\n# The null is that OLS is consistent (no endogeneity)\n\n# Run summary with diagnostics\nsummary(iv_model, diagnostics = TRUE)\n\n\nCall:\nivreg(formula = mpg ~ wt | hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.634 -2.428 -1.063  2.291 10.052 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   43.440      3.282  13.237 4.62e-14 ***\nwt            -7.258      1.001  -7.252 4.50e-08 ***\n\nDiagnostic tests:\n                 df1 df2 statistic  p-value    \nWeak instruments   1  30     23.00 4.15e-05 ***\nWu-Hausman         1  29     12.38  0.00145 ** \nSargan             0  NA        NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.591 on 30 degrees of freedom\nMultiple R-Squared: 0.6564, Adjusted R-squared: 0.6449 \nWald test: 52.59 on 1 and 30 DF,  p-value: 4.497e-08 \n\n\nInterpreting the Test:\nWeak Instruments Test\nWhat it tests: Whether the instrument(s) (hp) is sufficiently correlated with the endogenous regressor (wt). A strong first-stage relationship is crucial for IV to work.\nInterpretation: The null hypothesis is that the instruments are weak.\nThe result (p-value = 4.15e-05), an extremely low p-value, suggests that we should strongly reject the null. The instrument(s) are not weak; they are strong and relevant. This is a good sign.\nWu-Hausman Test (for Endogeneity)\nWhat it tests: This is the test for endogeneity. The null hypothesis (\\(H_0\\)) is that the variable (wt) is exogenous. The alternative (\\(H_1\\)) is that it is endogenous.\nInterpretation: A low p-value suggests you should reject \\(H_0\\) and use the IV estimator. A high p-value means you cannot reject \\(H_0\\) and should prefer the efficient OLS estimator.\nThe result (p-value = 0.0014), a low p-value means we should reject the null hypothesis. There is statistical evidence that wt is endogenous. Therefore, the standard OLS estimates for mpg ~ wt gives biased etimates.\nSargan Test (for Overidentifying Restrictions)\nWhat it tests: This test checks the validity of your overidentifying instruments. It is only relevant if you have more instruments than endogenous variables. The null hypothesis (\\(H_0\\)) is that the extra instruments are valid (uncorrelated with the error term).\nInterpretation: A low p-value is undesirable, as it means you should reject \\(H_0\\) and suspect that at least one of your extra instruments is invalid.\nWhat to Do If You Find Endogeneity?\nIf the test suggests endogeneity, you should not trust the OLS results. You must use a method that addresses the endogeneity, such as:\n\nFinding Better Controls: If the endogeneity is from omitted variable bias.\nInstrumental Variables (IV) Regression: The primary solution, implemented with ivreg().\nUsing Panel Data Methods: Such as fixed effects models, if you have panel data.\n\nImportant: The validity of the Hausman test hinges on the quality of your instruments. If your instruments are weak or invalid, the test itself is unreliable. Always check the first-stage F-statistic to ensure instrument strength (a rule of thumb is F-stat &gt; 10).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Violation of CLRM Assumptions</span>"
    ]
  },
  {
    "objectID": "dummynonlinear.html",
    "href": "dummynonlinear.html",
    "title": "5  Non-linear Models and Dummy Variable Regression",
    "section": "",
    "text": "5.1 Dummy Variables\nLet’s begin with an short introduction of dummy variables (binary variables), which are often used to include qualitative information (e.g., gender, region, treatment/control) in a regression model. In econometrics, it is common for dummy variables to take the value 0 or 1, i.e. \\(D_i = \\{ 0, 1 \\}\\).\nModel Specification: \\[Y_i = \\alpha + \\beta D_i + u_i\\]\nInterpretation: - The baseline group (\\(D=0\\)) will have a mean or expected value of \\(\\alpha\\). - The other group (\\(D=1\\)), meanwhile, has as its mean or expected value of \\(\\alpha + \\beta\\) - Hence \\(\\beta\\) is the difference-in-means of the outcome variable \\(Y_i\\) between the two groups.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Models and Dummy Variable Regression</span>"
    ]
  },
  {
    "objectID": "dummynonlinear.html#modeling-non-linear-relationships",
    "href": "dummynonlinear.html#modeling-non-linear-relationships",
    "title": "5  Non-linear Models and Dummy Variable Regression",
    "section": "5.2 Modeling Non-Linear Relationships",
    "text": "5.2 Modeling Non-Linear Relationships\nWhile the relationship between variables may be non-linear, we can often transform the variables so that the model is linear in the parameters (\\(\\beta\\)s), allowing us to use OLS.\n\n5.2.1 Polynomial (Quadratic) Models\nA quadratic model is used to capture curvilinear relationships, such as diminishing or increasing returns.\nModel Specification: \\[Y_i = \\alpha + \\beta_1 X_i + \\beta_2 X_i^2 + u_i\\]\n\nThe model is linear in the parameters (\\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)), so we can run OLS.\nWe create a new variable \\(X_i^2\\) and include it as a separate regressor.\n\nInterpretation: - The slope is no longer constant. The marginal effect of \\(X\\) on \\(Y\\) is given by the derivative: \\(\\frac{\\partial Y}{\\partial X} = \\beta_1 + 2\\beta_2 X\\). - If \\(\\beta_2 &lt; 0\\), the relationship is concave (inverted U-shape). If \\(\\beta_2 &gt; 0\\), it is convex (U-shape).\nExample in R:\n\n# Estimate a quadratic model for mpg vs. weight\nmtcars$wt_sq &lt;- mtcars$wt^2 # Create the squared term\nquad_model &lt;- lm(mpg ~ wt + wt_sq, data = mtcars)\nsummary(quad_model)\n\n\nCall:\nlm(formula = mpg ~ wt + wt_sq, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.483 -1.998 -0.773  1.462  6.238 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  49.9308     4.2113  11.856 1.21e-12 ***\nwt          -13.3803     2.5140  -5.322 1.04e-05 ***\nwt_sq         1.1711     0.3594   3.258  0.00286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 29 degrees of freedom\nMultiple R-squared:  0.8191,    Adjusted R-squared:  0.8066 \nF-statistic: 65.64 on 2 and 29 DF,  p-value: 1.715e-11\n\n\n\n\n5.2.2 Logarithmic Transformations\nLogarithms are powerful tools for modeling percentage changes and non-constant elasticities. There are three common forms.\n\n5.2.2.1 1. Linear-Log Model\nModel Specification: \\[Y_i = \\alpha + \\beta_1 \\ln(X_i) + u_i\\]\nInterpretation: - A 1% increase in X is associated with a \\(\\beta_1 / 100\\) unit change in Y. - \\(\\beta_1\\) represents the change in \\(Y\\) for a 1% change in \\(X\\).\n\n\n5.2.2.2 2. Log-Linear Model\nModel Specification: \\[\\ln(Y_i) = \\alpha + \\beta_1 X_i + u_i\\]\nInterpretation: - A one-unit increase in X is associated with a \\((\\beta_1 \\times 100)\\)% change in Y (approximately, for small \\(\\beta_1\\)). - Exact percentage change is \\(100 \\times [\\exp(\\beta_1) - 1]\\%\\).\n\n\n5.2.2.3 3. Log-Log Model\nModel Specification: \\[\\ln(Y_i) = \\alpha + \\beta_1 \\ln(X_i) + u_i\\]\nInterpretation: - \\(\\beta_1\\) is the elasticity of \\(Y\\) with respect to \\(X\\). - A 1% increase in X is associated with a \\(\\beta_1\\)% change in Y.\nExample in R:\n\n# Log-Log model example: elasticity of mpg with respect to weight\nlog_log_model &lt;- lm(log(mpg) ~ log(wt), data = mtcars)\nsummary(log_log_model)\n\n\nCall:\nlm(formula = log(mpg) ~ log(wt), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18141 -0.10681 -0.02125  0.08109  0.26930 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.90181    0.08790   44.39  &lt; 2e-16 ***\nlog(wt)     -0.84182    0.07549  -11.15 3.41e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1334 on 30 degrees of freedom\nMultiple R-squared:  0.8056,    Adjusted R-squared:  0.7992 \nF-statistic: 124.4 on 1 and 30 DF,  p-value: 3.406e-12\n\n# The coefficient on log(wt) is the elasticity.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Models and Dummy Variable Regression</span>"
    ]
  },
  {
    "objectID": "dummynonlinear.html#interaction-variables",
    "href": "dummynonlinear.html#interaction-variables",
    "title": "5  Non-linear Models and Dummy Variable Regression",
    "section": "5.3 Interaction Variables",
    "text": "5.3 Interaction Variables\nInteraction terms allow the effect of one independent variable (\\(X_1\\)) on the dependent variable (\\(Y\\)) to depend on the level of another independent variable (\\(X_2\\)).\nModel Specification: \\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 (X_{1i} \\times X_{2i}) + u_i\\]\nInterpretation: - The marginal effect of \\(X_1\\) on \\(Y\\) is: \\(\\frac{\\partial Y}{\\partial X_1} = \\beta_1 + \\beta_3 X_2\\). - This effect depends on the value of \\(X_2\\). - Similarly, the marginal effect of \\(X_2\\) is \\(\\beta_2 + \\beta_3 X_1\\).\nExample in R:\n\n# Does the effect of weight (wt) on mpg depend on horsepower (hp)?\ninteraction_model &lt;- lm(mpg ~ wt + hp + wt:hp, data = mtcars)\n# Equivalently: mpg ~ wt * hp\nsummary(interaction_model)\n\n\nCall:\nlm(formula = mpg ~ wt + hp + wt:hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0632 -1.6491 -0.7362  1.4211  4.5513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***\nwt          -8.21662    1.26971  -6.471 5.20e-07 ***\nhp          -0.12010    0.02470  -4.863 4.04e-05 ***\nwt:hp        0.02785    0.00742   3.753 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.153 on 28 degrees of freedom\nMultiple R-squared:  0.8848,    Adjusted R-squared:  0.8724 \nF-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Models and Dummy Variable Regression</span>"
    ]
  },
  {
    "objectID": "dummynonlinear.html#dummy-variables",
    "href": "dummynonlinear.html#dummy-variables",
    "title": "5  Non-linear Models and Dummy Variable Regression",
    "section": "5.3 Dummy Variables",
    "text": "5.3 Dummy Variables\nDummy variables (binary variables) are used to include qualitative information (e.g., gender, region, treatment/control) in a regression model. They take the value 0 or 1.\n\n5.3.1 1. Different Intercept, Same Slope\nThis is the most common use. The dummy variable shifts the regression line up or down.\nModel Specification: \\[Y_i = \\alpha + \\beta D_i + \\gamma X_i + u_i\\] where \\(D_i = 1\\) if an observation belongs to a certain group, 0 otherwise.\nInterpretation: - \\(\\alpha\\) is the intercept for the baseline group (\\(D=0\\)). - \\(\\alpha + \\beta\\) is the intercept for the group where \\(D=1\\). - \\(\\beta\\) captures the difference in the mean of \\(Y\\) between the two groups, holding \\(X\\) constant.\n\n\n5.3.2 2. Different Intercept, Different Slope (Interaction with a Dummy)\nThis model allows both the intercept and the slope to differ between groups.\nModel Specification: \\[Y_i = \\alpha + \\beta_1 D_i + \\beta_2 X_i + \\beta_3 (D_i \\times X_i) + u_i\\]\nInterpretation: - For the baseline group (\\(D=0\\)): \\(Y_i = \\alpha + \\beta_2 X_i + u_i\\) - For the other group (\\(D=1\\)): \\(Y_i = (\\alpha + \\beta_1) + (\\beta_2 + \\beta_3) X_i + u_i\\) - \\(\\beta_1\\): Difference in intercepts. - \\(\\beta_3\\): Difference in slopes.\n\n\n5.3.3 3. Same Intercept, Different Slope\nThis is a restricted version of the model above, less commonly used, where the intercept is forced to be the same but the slopes are allowed to differ.\nModel Specification: \\[Y_i = \\alpha + \\beta_1 X_i + \\beta_2 (D_i \\times X_i) + u_i\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Models and Dummy Variable Regression</span>"
    ]
  },
  {
    "objectID": "dummynonlinear.html#model-comparison-and-testing",
    "href": "dummynonlinear.html#model-comparison-and-testing",
    "title": "5  Non-linear Models and Dummy Variable Regression",
    "section": "5.5 Model Comparison and Testing",
    "text": "5.5 Model Comparison and Testing\n\n5.5.1 F-test (Wald Test) for Model Selection\nWhen comparing nested models (e.g., a model with an interaction term vs. one without), the F-test for linear restrictions (also known as the Wald test) is the most appropriate method.\n\nExample: To test if the interaction term is significant, compare the unrestricted model (Y ~ X1 + X2 + X1*X2) to the restricted model (Y ~ X1 + X2).\nA low p-value suggests the more complex (unrestricted) model is better.\n\nR Code:\n\n# Compare a model with and without an interaction term\nmodel_unrestricted &lt;- lm(mpg ~ wt * hp, data = mtcars) # Includes interaction\nmodel_restricted &lt;- lm(mpg ~ wt + hp, data = mtcars)   # No interaction\n\n# Use an F-test (Wald test) to compare them\nanova(model_restricted, model_unrestricted)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ wt + hp\nModel 2: mpg ~ wt * hp\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     29 195.05                                  \n2     28 129.76  1    65.286 14.088 0.0008108 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n5.5.2 A Note on Adjusted R-squared and Model Comparison\n\nUse Adjusted R-squared (\\(\\bar{R}^2\\)) to compare models with the same dependent variable (Y). It penalizes for adding extra variables.\nDo NOT use \\(\\bar{R}^2\\) to compare models with different dependent variables (e.g., a model for Y vs. a model for log(Y)). The Total Sum of Squares (SST) is different, making the \\(R^2\\) values incomparable.\nThe F-test (Wald test), Likelihood Ratio (LR) test, and Lagrange Multiplier (LM) test are more robust methods for model comparison, especially for non-nested models or those with different dependent variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Models and Dummy Variable Regression</span>"
    ]
  },
  {
    "objectID": "dummynonlinear.html#more-models-using-dummy-variables",
    "href": "dummynonlinear.html#more-models-using-dummy-variables",
    "title": "5  Non-linear Models and Dummy Variable Regression",
    "section": "5.4 More Models using Dummy Variables",
    "text": "5.4 More Models using Dummy Variables\nAs mentioned dummy variables (binary variables) are used to include qualitative information (e.g., gender, region, treatment/control) in a regression model. They take the value 0 or 1. Let’s explore how we can use dummy variables to further model nonlinear econometric relationships.\n\n5.4.1 Different Intercept, Same Slope\nThis is the most common use. The dummy variable shifts the regression line up or down.\nModel Specification: \\[Y_i = \\alpha + \\beta D_i + \\gamma X_i + u_i\\] where \\(D_i = 1\\) if an observation belongs to a certain group, 0 otherwise.\nInterpretation: - \\(\\alpha\\) is the intercept for the baseline group (\\(D=0\\)). - \\(\\alpha + \\beta\\) is the intercept for the group where \\(D=1\\). - \\(\\beta\\) captures the difference in the mean of \\(Y\\) between the two groups, holding \\(X\\) constant.\n\n\n5.4.2 Different Intercept, Different Slope (Interaction with a Dummy)\nThis model allows both the intercept and the slope to differ between groups.\nModel Specification: \\[Y_i = \\alpha + \\beta_1 D_i + \\beta_2 X_i + \\beta_3 (D_i \\times X_i) + u_i\\]\nInterpretation: - For the baseline group (\\(D=0\\)): \\(Y_i = \\alpha + \\beta_2 X_i + u_i\\) - For the other group (\\(D=1\\)): \\(Y_i = (\\alpha + \\beta_1) + (\\beta_2 + \\beta_3) X_i + u_i\\) - \\(\\beta_1\\): Difference in intercepts. - \\(\\beta_3\\): Difference in slopes.\n\n\n5.4.3 Same Intercept, Different Slope\nThis is a restricted version of the model above, less commonly used, where the intercept is forced to be the same but the slopes are allowed to differ.\nModel Specification: \\[Y_i = \\alpha + \\beta_1 X_i + \\beta_2 (D_i \\times X_i) + u_i\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Models and Dummy Variable Regression</span>"
    ]
  },
  {
    "objectID": "logitprobit.html",
    "href": "logitprobit.html",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "",
    "text": "6.1 Maximum Likelihood Estimation (MLE)\nOLS is not the only method for estimating parameters. MLE is another powerful and widely used estimator.\nLet’s explore some important LDV models, i.e. models in which dependent variable can assume a limited form, making linear regression unsuitable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "logitprobit.html#maximum-likelihood-estimation-mle",
    "href": "logitprobit.html#maximum-likelihood-estimation-mle",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "",
    "text": "Concept: Maximum Likelihood Estimation finds the parameter values that make the observed sample data most probable (i.e., maximize the likelihood function).\nIntuition: Given a statistical model (e.g., a normal distribution) and a sample of data, MLE answers: “What values of the model’s parameters (mean, variance) would most likely have generated this data?”\nComparison: While OLS minimizes the sum of squared residuals, MLE maximizes the likelihood function. For the classical linear model with normal errors, OLS and MLE produce identical estimates.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "logitprobit.html#limited-dependent-variable-models",
    "href": "logitprobit.html#limited-dependent-variable-models",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.2 Limited Dependent Variable Models",
    "text": "6.2 Limited Dependent Variable Models\nThe dependent variable can assume a limited form, making linear regression unsuitable.\n\n6.2.1 1. Binary Dependent Variables (Logit and Probit)\nWhen the outcome is binary, e.g., ( Y_i = {1, 0} ) (e.g., 1=owns a car, 0=does not own a car).\n\n6.2.1.1 The Linear Probability Model (LPM) and its Problems\nA naive approach is to use OLS on the binary outcome: ( Y_i = _1 + 2 X{2i} + u_i ). - The fitted values, ( _i ), can be interpreted as the probability that ( Y_i=1 ). - Problems: 1. Probabilities outside [0,1]: OLS can predict probabilities less than 0 or greater than 1. 2. Non-normal errors: The error term ( u_i ) can only take two values, violating the normality assumption. 3. Heteroskedasticity: The variance of the error term is not constant. 4. Low R²: R-squared is often very low for cross-sectional binary outcomes, which is not a good measure of fit.\n\n\n6.2.1.2 The Latent Variable Framework\nA better approach is to model a continuous, unobserved (latent) variable ( Y_i^* ) that determines the observed outcome. - Latent Model: ( Y_i^* = _1 + 2 X{2i} + u_i ) - Observation Rule: ( Y_i =\n\\[\\begin{cases} 1 & \\text{if } Y_i^* \\geq 0 \\\\ 0 & \\text{if } Y_i^* &lt; 0 \\end{cases}\\]\n)\nThe probability of observing ( Y_i=1 ) is: [\n\\[\\begin{aligned}\nP(Y_i = 1) &= P(Y_i^* \\geq 0) \\\\\n&= P(\\beta_1 + \\beta_2 X_{2i} + u_i \\geq 0) \\\\\n&= P(u_i \\geq -\\mathbf{X}_i'\\boldsymbol{\\beta}) \\\\\n&= 1 - F(-\\mathbf{X}_i'\\boldsymbol{\\beta}) = F(\\mathbf{X}_i'\\boldsymbol{\\beta})\n\\end{aligned}\\]\n] where ( F() ) is a cumulative distribution function (CDF). The last equality holds if the distribution of ( u_i ) is symmetric around zero (like the normal or logistic).\n\n\n6.2.1.3 Logit and Probit Models\nThe choice of ( F() ) gives rise to different models:\n\nProbit Model: Uses the standard normal CDF, denoted ( () ). [ P(Y_i = 1) = (_i’) ]\nLogit Model: Uses the logistic CDF. [ P(Y_i = 1) = (_i’) = ]\n\nThe parameters ( ) are estimated by Maximum Likelihood Estimation (MLE).\n\n\n6.2.1.4 Interpretation of Coefficients\nUnlike OLS, the coefficients ( _k ) do not represent a constant marginal effect. The marginal effect of a change in ( X_k ) on the probability ( P(Y=1) ) depends on the values of all explanatory variables.\n\nProbit Marginal Effect: [ = (_i’) _k ] where ( () ) is the standard normal probability density function (PDF).\nLogit Marginal Effect: [ = (_i’)[1-(_i’)] _k ]\nOdds Ratio (Logit): The logit model can also be interpreted in terms of odds.\n\nThe odds in favor of ( Y=1 ) are ( = (_i’) ).\nA one-unit change in ( X_k ) multiplies the odds by ( (_k) ), holding all else constant.\n\n\nExample in R:\n\n# Create a binary variable: 1 if mpg &gt; 20, 0 otherwise\nmtcars$high_mpg &lt;- ifelse(mtcars$mpg &gt; 20, 1, 0)\n\n# Estimate a Logit Model\nlogit_model &lt;- glm(high_mpg ~ wt + hp, family = binomial(link = \"logit\"), data = mtcars)\nsummary(logit_model)\n\n\nCall:\nglm(formula = high_mpg ~ wt + hp, family = binomial(link = \"logit\"), \n    data = mtcars)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    894.228 365884.162   0.002    0.998\nwt            -202.865  84688.218  -0.002    0.998\nhp              -2.021    858.062  -0.002    0.998\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4.3860e+01  on 31  degrees of freedom\nResidual deviance: 1.1156e-08  on 29  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n\n# Estimate a Probit Model\nprobit_model &lt;- glm(high_mpg ~ wt + hp, family = binomial(link = \"probit\"), data = mtcars)\nsummary(probit_model)\n\n\nCall:\nglm(formula = high_mpg ~ wt + hp, family = binomial(link = \"probit\"), \n    data = mtcars)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   262.5104 58549.5044   0.004    0.996\nwt            -59.6114 13537.6593  -0.004    0.996\nhp             -0.5914   137.9606  -0.004    0.997\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4.3860e+01  on 31  degrees of freedom\nResidual deviance: 1.2574e-08  on 29  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n\n# Calculate average marginal effects for the logit model\n# install.packages(\"margins\")\nlibrary(margins)\nmargins_logit &lt;- margins(logit_model)\nsummary(margins_logit)\n\n factor     AME     SE       z      p   lower  upper\n     hp -0.0000 0.0000 -0.0001 0.9999 -0.0000 0.0000\n     wt -0.0000 0.0009 -0.0000 1.0000 -0.0017 0.0017\n\n\n\n\n\n6.2.2 2. Multinomial and Ordered Models\n\nMultinomial Logit/Probit: Used when the dependent variable has more than two categories without a natural ordering (e.g., choice of transport: walk, car, BTS).\nOrdered Logit/Probit: Used when the categories have a natural order (e.g., exam grades: A, B, C, D). Both are estimated via MLE.\n\n\n\n6.2.3 3. Censored and Truncated Regression (Tobit Model)\nThe Tobit model is used when the dependent variable is censored. For example, a variable can be zero for a substantial fraction of the observations but positive for the rest (e.g., hours worked, where some people work 0 hours).\n\nLatent Model: ( Y_i^* = _i’ + u_i )\nObserved Rule: ( Y_i =\n\\[\\begin{cases} Y_i^* & \\text{if } Y_i^* &gt; 0 \\\\ 0 & \\text{if } Y_i^* \\leq 0 \\end{cases}\\]\n)\n\nUsing OLS on the censored data leads to biased estimates. The Tobit model uses MLE to estimate the parameters, which accounts for both the probability of being censored and the value of the uncensored observations.\nExample in R:\n\n# install.packages(\"AER\")\nlibrary(AER)\n# Example using a simulated dataset. 'hours' is censored at 0.\n# tobit_model &lt;- tobit(hours ~ age + education, data = dataset)\n\n\n\n6.2.4 4. Count Data Models (Poisson Regression)\nWhen the dependent variable is a count (e.g., number of patents, number of doctor visits), a Poisson regression model is often appropriate.\n\nThe Poisson probability density function is ( P(Y=y) = ), where ( E(Y) = Var(Y) = ).\nWe model the mean ( _i ) as ( _i = E(Y_i | _i) = (_i’) ). This ensures the mean is always positive.\nParameters are estimated by MLE.\n\nExample in R:\n\n# Example: Modeling number of awards (a count) in a dataset\n# poisson_model &lt;- glm(awards ~ math + prog, family = poisson, data = data)\n\n\n\n6.2.5 5. Sample Selection Models (Heckman’s Heckit)\nSample selection bias occurs when the sample is not randomly selected from the population. For example, estimating wages only for people who are employed (a non-random subset).\n\nThe Heckit model is a two-step procedure:\n\nSelection Equation (Probit): Model the probability of being included in the sample.\nOutcome Equation: Model the outcome of interest, but include a correction term called the Inverse Mills Ratio (λ) estimated from the first step to control for selection bias.\n\nThis corrects the bias that would occur if the second step were run on the selected sample alone.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "logitprobit.html#binary-dependent-variables-logit-and-probit",
    "href": "logitprobit.html#binary-dependent-variables-logit-and-probit",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.2 Binary Dependent Variables (Logit and Probit)",
    "text": "6.2 Binary Dependent Variables (Logit and Probit)\nWhen the outcome is binary, e.g., ( Y_i = {1, 0} ) (e.g., 1=owns a car, 0=does not own a car).\n\n6.2.1 The Linear Probability Model (LPM) and its Problems\nA naive approach is to use OLS on the binary outcome: ( Y_i = _1 + 2 X{2i} + u_i ). - The fitted values, ( _i ), can be interpreted as the probability that ( Y_i=1 ). - Problems: 1. Probabilities outside [0,1]: OLS can predict probabilities less than 0 or greater than 1. 2. Non-normal errors: The error term ( u_i ) can only take two values, violating the normality assumption. 3. Heteroskedasticity: The variance of the error term is not constant. 4. Low R²: R-squared is often very low for cross-sectional binary outcomes, which is not a good measure of fit.\n\n\n6.2.2 The Latent Variable Framework\nA better approach is to model a continuous, unobserved (latent) variable ( Y_i^* ) that determines the observed outcome. - Latent Model: ( Y_i^* = _1 + 2 X{2i} + u_i ) - Observation Rule: ( Y_i =\n\\[\\begin{cases} 1 & \\text{if } Y_i^* \\geq 0 \\\\ 0 & \\text{if } Y_i^* &lt; 0 \\end{cases}\\]\n)\nThe probability of observing ( Y_i=1 ) is: [\n\\[\\begin{aligned}\nP(Y_i = 1) &= P(Y_i^* \\geq 0) \\\\\n&= P(\\beta_1 + \\beta_2 X_{2i} + u_i \\geq 0) \\\\\n&= P(u_i \\geq -\\mathbf{X}_i'\\boldsymbol{\\beta}) \\\\\n&= 1 - F(-\\mathbf{X}_i'\\boldsymbol{\\beta}) = F(\\mathbf{X}_i'\\boldsymbol{\\beta})\n\\end{aligned}\\]\n] where ( F() ) is a cumulative distribution function (CDF). The last equality holds if the distribution of ( u_i ) is symmetric around zero (like the normal or logistic).\n\n\n6.2.3 Logit and Probit Models\nThe choice of ( F() ) gives rise to different models:\n\nProbit Model: Uses the standard normal CDF, denoted ( () ). [ P(Y_i = 1) = (_i’) ]\nLogit Model: Uses the logistic CDF. [ P(Y_i = 1) = (_i’) = ]\n\nThe parameters ( ) are estimated by Maximum Likelihood Estimation (MLE).\n\n6.2.3.1 Interpretation of Coefficients\nUnlike OLS, the coefficients ( _k ) do not represent a constant marginal effect. The marginal effect of a change in ( X_k ) on the probability ( P(Y=1) ) depends on the values of all explanatory variables.\n\nProbit Marginal Effect: [ = (_i’) _k ] where ( () ) is the standard normal probability density function (PDF).\nLogit Marginal Effect: [ = (_i’)[1-(_i’)] _k ]\nOdds Ratio (Logit): The logit model can also be interpreted in terms of odds.\n\nThe odds in favor of ( Y=1 ) are ( = (_i’) ).\nA one-unit change in ( X_k ) multiplies the odds by ( (_k) ), holding all else constant.\n\n\nExample in R:\n\n# Create a binary variable: 1 if mpg &gt; 20, 0 otherwise\nmtcars$high_mpg &lt;- ifelse(mtcars$mpg &gt; 20, 1, 0)\n\n# Estimate a Logit Model\nlogit_model &lt;- glm(high_mpg ~ wt + hp, family = binomial(link = \"logit\"), data = mtcars)\nsummary(logit_model)\n\n\nCall:\nglm(formula = high_mpg ~ wt + hp, family = binomial(link = \"logit\"), \n    data = mtcars)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    894.228 365884.162   0.002    0.998\nwt            -202.865  84688.218  -0.002    0.998\nhp              -2.021    858.062  -0.002    0.998\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4.3860e+01  on 31  degrees of freedom\nResidual deviance: 1.1156e-08  on 29  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n\n# Estimate a Probit Model\nprobit_model &lt;- glm(high_mpg ~ wt + hp, family = binomial(link = \"probit\"), data = mtcars)\nsummary(probit_model)\n\n\nCall:\nglm(formula = high_mpg ~ wt + hp, family = binomial(link = \"probit\"), \n    data = mtcars)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   262.5104 58549.5044   0.004    0.996\nwt            -59.6114 13537.6593  -0.004    0.996\nhp             -0.5914   137.9606  -0.004    0.997\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4.3860e+01  on 31  degrees of freedom\nResidual deviance: 1.2574e-08  on 29  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n\n# Calculate average marginal effects for the logit model\n# install.packages(\"margins\")\nlibrary(margins)\nmargins_logit &lt;- margins(logit_model)\nsummary(margins_logit)\n\n factor     AME     SE       z      p   lower  upper\n     hp -0.0000 0.0000 -0.0001 0.9999 -0.0000 0.0000\n     wt -0.0000 0.0009 -0.0000 1.0000 -0.0017 0.0017",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "logitprobit.html#multinomial-and-ordered-models",
    "href": "logitprobit.html#multinomial-and-ordered-models",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.3 Multinomial and Ordered Models",
    "text": "6.3 Multinomial and Ordered Models\n\nMultinomial Logit/Probit: Used when the dependent variable has more than two categories without a natural ordering (e.g., choice of transport: walk, car, BTS).\nOrdered Logit/Probit: Used when the categories have a natural order (e.g., exam grades: A, B, C, D). Both are estimated via MLE.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "logitprobit.html#censored-and-truncated-regression-tobit-model",
    "href": "logitprobit.html#censored-and-truncated-regression-tobit-model",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.4 Censored and Truncated Regression (Tobit Model)",
    "text": "6.4 Censored and Truncated Regression (Tobit Model)\nThe Tobit model is used when the dependent variable is censored. For example, a variable can be zero for a substantial fraction of the observations but positive for the rest (e.g., hours worked, where some people work 0 hours).\n\nLatent Model: ( Y_i^* = _i’ + u_i )\nObserved Rule: ( Y_i =\n\\[\\begin{cases} Y_i^* & \\text{if } Y_i^* &gt; 0 \\\\ 0 & \\text{if } Y_i^* \\leq 0 \\end{cases}\\]\n)\n\nUsing OLS on the censored data leads to biased estimates. The Tobit model uses MLE to estimate the parameters, which accounts for both the probability of being censored and the value of the uncensored observations.\nExample in R:\n\n# install.packages(\"AER\")\nlibrary(AER)\n# Example using a simulated dataset. 'hours' is censored at 0.\n# tobit_model &lt;- tobit(hours ~ age + education, data = dataset)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "logitprobit.html#count-data-models-poisson-regression",
    "href": "logitprobit.html#count-data-models-poisson-regression",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.5 Count Data Models (Poisson Regression)",
    "text": "6.5 Count Data Models (Poisson Regression)\nWhen the dependent variable is a count (e.g., number of patents, number of doctor visits), a Poisson regression model is often appropriate.\n\nThe Poisson probability density function is ( P(Y=y) = ), where ( E(Y) = Var(Y) = ).\nWe model the mean ( _i ) as ( _i = E(Y_i | _i) = (_i’) ). This ensures the mean is always positive.\nParameters are estimated by MLE.\n\nExample in R:\n\n# Example: Modeling number of awards (a count) in a dataset\n# poisson_model &lt;- glm(awards ~ math + prog, family = poisson, data = data)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "logitprobit.html#sample-selection-models-heckmans-heckit",
    "href": "logitprobit.html#sample-selection-models-heckmans-heckit",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.6 Sample Selection Models (Heckman’s Heckit)",
    "text": "6.6 Sample Selection Models (Heckman’s Heckit)\nSample selection bias occurs when the sample is not randomly selected from the population. For example, estimating wages only for people who are employed (a non-random subset).\n\nThe Heckit model is a two-step procedure:\n\nSelection Equation (Probit): Model the probability of being included in the sample.\nOutcome Equation: Model the outcome of interest, but include a correction term called the Inverse Mills Ratio (λ) estimated from the first step to control for selection bias.\n\nThis corrects the bias that would occur if the second step were run on the selected sample alone.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "panel.html",
    "href": "panel.html",
    "title": "7  Panel Data Regression",
    "section": "",
    "text": "7.1 What is Panel Data?\nPanel data consists of observations on the same \\(n\\) entities (individuals, firms, countries) at two or more time periods \\(T\\).\nThe major advantage of panel data is its ability to help resolve omitted variable bias, a primary source of endogeneity. If the omitted variable is constant over time for each entity, panel data methods can effectively control for it.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel Data Regression</span>"
    ]
  },
  {
    "objectID": "panel.html#what-is-panel-data",
    "href": "panel.html#what-is-panel-data",
    "title": "7  Panel Data Regression",
    "section": "",
    "text": "Notation: \\((X_{it}, Y_{it})\\), where \\(i = 1, \\ldots, n\\) and \\(t = 1, \\ldots, T\\).\nExample: Data on the same 100 companies (\\(i\\)) over 10 years (\\(t\\)).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel Data Regression</span>"
    ]
  },
  {
    "objectID": "panel.html#the-before-and-after-comparison-first-differences",
    "href": "panel.html#the-before-and-after-comparison-first-differences",
    "title": "7  Panel Data Regression",
    "section": "7.2 The “Before and After” Comparison (First Differences)",
    "text": "7.2 The “Before and After” Comparison (First Differences)\nA simple intuitive approach for two time periods (\\(T=2\\)) is to compare changes over time.\nSuppose the “true” model includes an unobserved, time-invariant variable \\(Z_i\\) (e.g., managerial talent for a firm, innate ability for a person): \\[\nY_{it} = c + \\beta X_{it} + \\gamma Z_i + u_{it}\n\\]\nFor time period \\(t-1\\), the model is: \\[\nY_{it-1} = c + \\beta X_{it-1} + \\gamma Z_i + u_{it-1}\n\\]\nTaking the difference between the two periods eliminates the time-invariant variable \\(Z_i\\): \\[\nY_{it} - Y_{it-1} = \\beta (X_{it} - X_{it-1}) + (u_{it} - u_{it-1})\n\\]\nThis First-Differenced (FD) model can be estimated by OLS. The key insight is that we do not need to observe \\(Z_i\\) to consistently estimate \\(\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel Data Regression</span>"
    ]
  },
  {
    "objectID": "panel.html#fixed-effects-regression",
    "href": "panel.html#fixed-effects-regression",
    "title": "7  Panel Data Regression",
    "section": "7.3 Fixed Effects Regression",
    "text": "7.3 Fixed Effects Regression\nThe Fixed Effects (FE) model generalizes the “before and after” idea to more than two time periods. It controls for all unobserved, time-invariant characteristics of each entity.\n\n7.3.1 The Fixed Effects Model\nThe model allows each entity to have its own intercept: \\[\nY_{it} = c + \\beta_1 X_{1,it} + \\ldots + \\beta_k X_{k,it} + \\alpha_i + \\epsilon_{it}\n\\]\nwhere \\(\\alpha_i\\) is the entity-specific fixed effect.\nHere the error term is decomposed into two parts: \\(\\alpha_i\\) which captures all unobserved variables that are constant over time for entity \\(i\\), and; \\(\\epsilon_{it}\\) is the usual stochastic term.\n\n\n7.3.2 Estimation Methods\nThere are two common ways to estimate the Fixed Effects model:\n\n7.3.2.1 1. Least Squares Dummy Variable (LSDV) Regression\nThe model can be written with a common intercept and dummy variables for each entity (except one, to avoid perfect multicollinearity, a.k.a “dummy regression trap”): \\[\nY_{it} = c + \\beta_1 X_{1,it} + \\ldots + \\beta_k X_{k,it} + \\delta_1 D_{1,i} + \\delta_2 D_{2, i} + \\ldots + \\delta_{n-1} D_{n-1,i} + u_{it}\n\\] where \\(D_{1,i}\\) is a dummy variable equal to 1 for the first entity, and so on, up to \\(n-1\\).\n\nDisadvantage: With a large number of entities (\\(n\\)), you lose many degrees of freedom by estimating \\(n-1\\) dummy coefficients.\n\n\n\n7.3.2.2 2. The “Entity-Demeaned” OLS Algorithm (Preferred)\nThis is the more efficient computational method whichinvolves subtracting the entity-specific mean from each variable.\n\nCalculate the entity-specific averages: \\(\\bar{Y}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{it}\\) and \\(\\bar{X}_i = \\frac{1}{T}\\sum_{t=1}^T X_{it}\\).\nDemean the data: \\(Y_{it} - \\bar{Y}_i\\) and \\(X_{it} - \\bar{X}_i\\).\nRun an OLS regression on the transformed (demeaned) variables: \\[\n(Y_{it} - \\bar{Y}_i) = \\beta_1 (X_{1,it} - \\bar{X}_{1,i}) + \\ldots + \\beta_k (X_{k,it} - \\bar{X}_{k,i}) + (u_{it} - \\bar{u}_i)\n\\]\n\nThis process, known as the within transformation, and effectively sweeps out the fixed effect \\(\\alpha_i\\).\n\n\n\n7.3.3 Time Fixed Effects\nIn addition to entity-specific effects, there might be time-specific effects that affect all entities in a given time period (e.g., a common economic shock in a particular year).\nA model with both entity and time fixed effects is: \\[\nY_{it} = c + \\beta_1 X_{1,it} + \\ldots + \\beta_k X_{k,it} + \\alpha_i + \\lambda_t + u_{it}\n\\] where \\(\\lambda_t\\) is the time fixed effect. This can be estimated by including entity dummies and time period dummies (or by demeaning both with respect to entities and time, or demeaning entity but including time period dummies less one).\n\n\n7.3.4 Testing for Fixed Effects\nYou can test the joint significance of the entity fixed effects using an F-test that compares the Fixed Effects model to a simple OLS model (pooled regression) with a single constant term.\n\\(H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_n\\) (No entity-specific effects; pooled OLS is fine).\n\\(H_1\\): The \\(\\alpha_i\\) are not all equal (Fixed Effects model is appropriate).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel Data Regression</span>"
    ]
  },
  {
    "objectID": "panel.html#random-effects-model",
    "href": "panel.html#random-effects-model",
    "title": "7  Panel Data Regression",
    "section": "7.4 Random Effects Model",
    "text": "7.4 Random Effects Model\nThe Random Effects (RE) model is an alternative estimator when the unobserved entity-specific effect is uncorrelated with all the explanatory variables.\n\n7.4.1 The Random Effects Model\nThe model treats the entity-specific intercept as a random variable: \\[\nY_{it} = c + \\beta_1 X_{1,it} + \\ldots + \\beta_k X_{k,it} + u_{it}\n\\]\nwhere, as before, the composite error term can be expressed as \\(u_{it} = \\alpha_i + \\epsilon_{it}\\).\nHere the key assumption is that \\(\\alpha_i\\) (the random effect) is uncorrelated with the \\(X's\\) , i.e., \\(Cov(\\alpha_i, X_{it}) = 0\\).\n\n\n7.4.2 Fixed Effects vs. Random Effects\n\nFixed Effects (FE): Use when the unobserved entity-specific effect \\(\\alpha_i\\) is likely to be correlated with the explanatory variables \\(X_{it}\\). FE should mitigate endogeneity issues as it eliminates this source of bias.\nRandom Effects (RE): Use when \\(\\alpha_i\\) is uncorrelated with the \\(X_{it}\\). RE is more efficient (provides smaller standard errors) than FE if this assumption holds.\n\nThe Hausman test is often used to decide between FE and RE models.\n\\(H_0\\): The RE assumption is valid (\\(Cov(\\alpha_i, X_{it}) = 0\\)).\n\\(H_1\\): \\(H_0\\) is false. The FE model is consistent.\nAns as usual, a low p-value indicates that the Fixed Effects model is preferred.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel Data Regression</span>"
    ]
  },
  {
    "objectID": "panel.html#implementation-in-r",
    "href": "panel.html#implementation-in-r",
    "title": "7  Panel Data Regression",
    "section": "7.5 Implementation in R",
    "text": "7.5 Implementation in R\n\n# Load necessary packages\n# install.packages(\"plm\")\nlibrary(plm) # Panel data econometrics in R\n\n# Use the Grunfeld dataset (a classic panel dataset included in the plm package)\n# This dataset contains investment data for 10 US firms from 1935-1954\ndata(\"Grunfeld\", package = \"plm\")\n\n# Look at the structure of the data\nhead(Grunfeld)\n\n  firm year   inv  value capital\n1    1 1935 317.6 3078.5     2.8\n2    1 1936 391.8 4661.7    52.6\n3    1 1937 410.6 5387.1   156.9\n4    1 1938 257.7 2792.2   209.2\n5    1 1939 330.8 4313.2   203.4\n6    1 1940 461.2 4643.9   207.2\n\n# Note: firm = company identifier, year = time identifier\n#       inv = investment, value = value of the firm, capital = stock of plant and equipment\n\n# Create a panel data frame. 'index' specifies the entity and time identifiers.\np.data &lt;- pdata.frame(Grunfeld, index = c(\"firm\", \"year\"))\n\n# 1. Pooled OLS (ignoring panel structure)\npooled_model &lt;- plm(inv ~ value + capital, data = p.data, model = \"pooling\")\nsummary(pooled_model)\n\nPooling Model\n\nCall:\nplm(formula = inv ~ value + capital, data = p.data, model = \"pooling\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-291.6757  -30.0137    5.3033   34.8293  369.4464 \n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(&gt;|t|)    \n(Intercept) -42.7143694   9.5116760 -4.4907 1.207e-05 ***\nvalue         0.1155622   0.0058357 19.8026 &lt; 2.2e-16 ***\ncapital       0.2306785   0.0254758  9.0548 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    9359900\nResidual Sum of Squares: 1755900\nR-Squared:      0.81241\nAdj. R-Squared: 0.8105\nF-statistic: 426.576 on 2 and 197 DF, p-value: &lt; 2.22e-16\n\n# 2. Fixed Effects (Entity Demeaned)\nfe_model &lt;- plm(inv ~ value + capital, data = p.data, model = \"within\")\nsummary(fe_model)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = inv ~ value + capital, data = p.data, model = \"within\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-184.00857  -17.64316    0.56337   19.19222  250.70974 \n\nCoefficients:\n        Estimate Std. Error t-value  Pr(&gt;|t|)    \nvalue   0.110124   0.011857  9.2879 &lt; 2.2e-16 ***\ncapital 0.310065   0.017355 17.8666 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    2244400\nResidual Sum of Squares: 523480\nR-Squared:      0.76676\nAdj. R-Squared: 0.75311\nF-statistic: 309.014 on 2 and 188 DF, p-value: &lt; 2.22e-16\n\n# 3. Random Effects\nre_model &lt;- plm(inv ~ value + capital, data = p.data, model = \"random\")\nsummary(re_model)\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = inv ~ value + capital, data = p.data, model = \"random\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nEffects:\n                  var std.dev share\nidiosyncratic 2784.46   52.77 0.282\nindividual    7089.80   84.20 0.718\ntheta: 0.8612\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-177.6063  -19.7350    4.6851   19.5105  252.8743 \n\nCoefficients:\n              Estimate Std. Error z-value Pr(&gt;|z|)    \n(Intercept) -57.834415  28.898935 -2.0013  0.04536 *  \nvalue         0.109781   0.010493 10.4627  &lt; 2e-16 ***\ncapital       0.308113   0.017180 17.9339  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    2381400\nResidual Sum of Squares: 548900\nR-Squared:      0.7695\nAdj. R-Squared: 0.76716\nChisq: 657.674 on 2 DF, p-value: &lt; 2.22e-16\n\n# 4. Hausman Test to choose between FE and RE\nhausman_test &lt;- phtest(fe_model, re_model)\nprint(hausman_test)\n\n\n    Hausman Test\n\ndata:  inv ~ value + capital\nchisq = 2.3304, df = 2, p-value = 0.3119\nalternative hypothesis: one model is inconsistent\n\n# 5. Fixed Effects with both Entity and Time Effects\nfe_twoway_model &lt;- plm(inv ~ value + capital, data = p.data, model = \"within\", effect = \"twoways\")\nsummary(fe_twoway_model)\n\nTwoways effects Within Model\n\nCall:\nplm(formula = inv ~ value + capital, data = p.data, effect = \"twoways\", \n    model = \"within\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-162.6094  -19.4710   -1.2669   19.1277  211.8420 \n\nCoefficients:\n        Estimate Std. Error t-value  Pr(&gt;|t|)    \nvalue   0.117716   0.013751  8.5604 6.653e-15 ***\ncapital 0.357916   0.022719 15.7540 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1615600\nResidual Sum of Squares: 452150\nR-Squared:      0.72015\nAdj. R-Squared: 0.67047\nF-statistic: 217.442 on 2 and 169 DF, p-value: &lt; 2.22e-16\n\n# 6. First Differences model (for T=2 periods)\n# Let's create a subset with just 2 years to demonstrate\nGrunfeld_2years &lt;- Grunfeld[Grunfeld$year %in% c(1935, 1936), ]\np.data_2years &lt;- pdata.frame(Grunfeld_2years, index = c(\"firm\", \"year\"))\nfd_model &lt;- plm(inv ~ value + capital, data = p.data_2years, model = \"fd\")\nsummary(fd_model)\n\nOneway (individual) effect First-Difference Model\n\nCall:\nplm(formula = inv ~ value + capital, data = p.data_2years, model = \"fd\")\n\nBalanced Panel: n = 10, T = 2, N = 20\nObservations used in estimation: 10\n\nResiduals:\n    Min.  1st Qu.   Median  3rd Qu.     Max. \n-51.5451 -17.3865  -6.3621   7.1823  96.6838 \n\nCoefficients:\n             Estimate Std. Error t-value Pr(&gt;|t|)\n(Intercept) 17.901125  18.516740  0.9668   0.3659\nvalue        0.061786   0.034264  1.8032   0.1143\ncapital     -1.011766   1.065441 -0.9496   0.3739\n\nTotal Sum of Squares:    19847\nResidual Sum of Squares: 13551\nR-Squared:      0.31722\nAdj. R-Squared: 0.12214\nF-statistic: 1.62613 on 2 and 7 DF, p-value: 0.26301",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Panel Data Regression</span>"
    ]
  },
  {
    "objectID": "LDV.html",
    "href": "LDV.html",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "",
    "text": "6.1 Maximum Likelihood Estimation (MLE)\nOLS is not the only method for estimating parameters. MLE is another powerful and widely used estimator.\nLet’s explore some important LDV models, i.e. models in which dependent variable can assume a limited form, making linear regression unsuitable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "LDV.html#maximum-likelihood-estimation-mle",
    "href": "LDV.html#maximum-likelihood-estimation-mle",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "",
    "text": "Concept: Maximum Likelihood Estimation finds the parameter values that make the observed sample data most probable (i.e., maximize the likelihood function).\nIntuition: Given a statistical model (e.g., a normal distribution) and a sample of data, MLE answers: “What values of the model’s parameters (mean, variance) would most likely have generated this data?”\nComparison: While OLS minimizes the sum of squared residuals, MLE maximizes the likelihood function. For the classical linear model with normal errors, OLS and MLE produce identical estimates.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "LDV.html#binary-dependent-variables-logit-and-probit",
    "href": "LDV.html#binary-dependent-variables-logit-and-probit",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.2 Binary Dependent Variables (Logit and Probit)",
    "text": "6.2 Binary Dependent Variables (Logit and Probit)\nWhen the outcome is binary, e.g., \\(Y_i = \\{1, 0\\}\\) (e.g., 1=owns a car, 0=does not own a car).\n\n6.2.1 The Linear Probability Model (LPM) and its Problems\nA naive approach is to use OLS on the binary outcome: \\(Y_i = \\beta_1 + \\beta_2 X_{2i} + u_i\\). - The fitted values, \\(\\hat{Y}_i\\), can be interpreted as the probability that \\(Y_i=1\\). - Problems: 1. Probabilities outside [0,1]: OLS can predict probabilities less than 0 or greater than 1. 2. Non-normal errors: The error term \\(u_i\\) can only take two values, violating the normality assumption. 3. Heteroskedasticity: The variance of the error term is not constant. 4. Low R²: R-squared is often very low for cross-sectional binary outcomes, which is not a good measure of fit.\n\n\n6.2.2 The Latent Variable Framework\nA better approach is to model a continuous, unobserved (latent) variable \\(Y_i^*\\) that determines the observed outcome.\n\nLatent Model: \\(Y_i^* = \\beta_1 + \\beta_2 X_{2i} + u_i\\)\nObservation Rule: \\(Y_i = \\begin{cases} 1 & \\text{if } Y_i^* \\geq 0 \\\\ 0 & \\text{if } Y_i^* &lt; 0 \\end{cases}\\)\n\nThe probability of observing \\(Y_i=1\\) is:\n\\[\n\\begin{aligned}\nP(Y_i = 1) &= P(Y_i^* \\geq 0) \\\\\n&= P(\\beta_1 + \\beta_2 X_{2i} + u_i \\geq 0) \\\\\n&= P(u_i \\geq -\\mathbf{X}_i'\\boldsymbol{\\beta}) \\\\\n&= 1 - F(-\\mathbf{X}_i'\\boldsymbol{\\beta}) = F(\\mathbf{X}_i'\\boldsymbol{\\beta})\n\\end{aligned}\n\\]\nwhere \\(F(\\cdot)\\) is a cumulative distribution function (CDF). The last equality holds if the distribution of \\(u_i\\) is symmetric around zero (like the normal or logistic).\n\n\n6.2.3 Logit and Probit Models\nThe choice of \\(F(\\cdot)\\) gives rise to different models:\n\nProbit Model: Uses the standard normal CDF, denoted \\(\\Phi(\\cdot)\\). \\[\nP(Y_i = 1) = \\Phi(\\mathbf{X}_i'\\boldsymbol{\\beta})\n\\]\nLogit Model: Uses the logistic CDF. \\[\nP(Y_i = 1) = \\Lambda(\\mathbf{X}_i'\\boldsymbol{\\beta}) = \\frac{\\exp(\\mathbf{X}_i'\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}_i'\\boldsymbol{\\beta})}\n\\]\n\nThe parameters \\(\\boldsymbol{\\beta}\\) are estimated by Maximum Likelihood Estimation (MLE).\n\n6.2.3.1 Interpretation of Coefficients\nUnlike OLS, the coefficients \\(\\beta_k\\) do not represent a constant marginal effect. The marginal effect of a change in \\(X_k\\) on the probability \\(P(Y=1)\\) depends on the values of all explanatory variables.\n\nProbit Marginal Effect: \\[\n\\frac{\\partial P(Y_i=1)}{\\partial X_k} = \\phi(\\mathbf{X}_i'\\boldsymbol{\\beta}) \\beta_k\n\\] where \\(\\phi(\\cdot)\\) is the standard normal probability density function (PDF).\nLogit Marginal Effect: \\[\n\\frac{\\partial P(Y_i=1)}{\\partial X_k} = \\Lambda(\\mathbf{X}_i'\\boldsymbol{\\beta})[1-\\Lambda(\\mathbf{X}_i'\\boldsymbol{\\beta})] \\beta_k\n\\]\nOdds Ratio (Logit): The logit model can also be interpreted in terms of odds.\n\nThe odds in favor of \\(Y=1\\) are \\(\\frac{P(Y=1)}{P(Y=0)} = \\exp(\\mathbf{X}_i'\\boldsymbol{\\beta})\\).\nA one-unit change in \\(X_k\\) multiplies the odds by \\(\\exp(\\beta_k)\\), holding all else constant.\n\n\nExample in R:\n\n# Create a binary variable: 1 if mpg &gt; 20, 0 otherwise\nmtcars$high_mpg &lt;- ifelse(mtcars$mpg &gt; 20, 1, 0)\n\n# Estimate a Logit Model\nlogit_model &lt;- glm(high_mpg ~ wt + hp, family = binomial(link = \"logit\"), data = mtcars)\nsummary(logit_model)\n\n\nCall:\nglm(formula = high_mpg ~ wt + hp, family = binomial(link = \"logit\"), \n    data = mtcars)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    894.228 365884.162   0.002    0.998\nwt            -202.865  84688.218  -0.002    0.998\nhp              -2.021    858.062  -0.002    0.998\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4.3860e+01  on 31  degrees of freedom\nResidual deviance: 1.1156e-08  on 29  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n\n# Estimate a Probit Model\nprobit_model &lt;- glm(high_mpg ~ wt + hp, family = binomial(link = \"probit\"), data = mtcars)\nsummary(probit_model)\n\n\nCall:\nglm(formula = high_mpg ~ wt + hp, family = binomial(link = \"probit\"), \n    data = mtcars)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   262.5104 58549.5044   0.004    0.996\nwt            -59.6114 13537.6593  -0.004    0.996\nhp             -0.5914   137.9606  -0.004    0.997\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4.3860e+01  on 31  degrees of freedom\nResidual deviance: 1.2574e-08  on 29  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n\n# Calculate average marginal effects for the logit model\n# install.packages(\"margins\")\nlibrary(margins)\nmargins_logit &lt;- margins(logit_model)\nsummary(margins_logit)\n\n factor     AME     SE       z      p   lower  upper\n     hp -0.0000 0.0000 -0.0001 0.9999 -0.0000 0.0000\n     wt -0.0000 0.0009 -0.0000 1.0000 -0.0017 0.0017",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "LDV.html#multinomial-and-ordered-models",
    "href": "LDV.html#multinomial-and-ordered-models",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.3 Multinomial and Ordered Models",
    "text": "6.3 Multinomial and Ordered Models\n\nMultinomial Logit/Probit: Used when the dependent variable has more than two categories without a natural ordering (e.g., choice of transport: walk, car, BTS).\nOrdered Logit/Probit: Used when the categories have a natural order (e.g., exam grades: A, B, C, D). Both are estimated via MLE.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "LDV.html#censored-and-truncated-regression-tobit-model",
    "href": "LDV.html#censored-and-truncated-regression-tobit-model",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.4 Censored and Truncated Regression (Tobit Model)",
    "text": "6.4 Censored and Truncated Regression (Tobit Model)\nThe Tobit model is used when the dependent variable is censored. For example, a variable can be zero for a substantial fraction of the observations but positive for the rest (e.g., hours worked, where some people work 0 hours).\n\nLatent Model: \\(Y_i^* = \\mathbf{X}_i'\\boldsymbol{\\beta} + u_i\\)\nObserved Rule: \\(Y_i = \\begin{cases} Y_i^* & \\text{if } Y_i^* &gt; 0 \\\\ 0 & \\text{if } Y_i^* \\leq 0 \\end{cases}\\)\n\nUsing OLS on the censored data leads to biased estimates. The Tobit model uses MLE to estimate the parameters, which accounts for both the probability of being censored and the value of the uncensored observations.\nExample in R:\n\n# install.packages(\"AER\")\nlibrary(AER)\n# Example using a simulated dataset. 'hours' is censored at 0.\n# tobit_model &lt;- tobit(hours ~ age + education, data = dataset)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "LDV.html#count-data-models-poisson-regression",
    "href": "LDV.html#count-data-models-poisson-regression",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.5 Count Data Models (Poisson Regression)",
    "text": "6.5 Count Data Models (Poisson Regression)\nWhen the dependent variable is a count (e.g., number of patents, number of doctor visits), a Poisson regression model is often appropriate.\n\nThe Poisson probability density function is \\(P(Y=y) = \\frac{e^{-\\mu} \\mu^y}{y!}\\), where \\(E(Y) = Var(Y) = \\mu\\).\nWe model the mean \\(\\mu_i\\) as \\(\\mu_i = E(Y_i | \\mathbf{X}_i) = \\exp(\\mathbf{X}_i'\\boldsymbol{\\beta})\\). This ensures the mean is always positive.\nParameters are estimated by MLE.\n\nExample in R:\n\n# Example: Modeling number of awards (a count) in a dataset\n# poisson_model &lt;- glm(awards ~ math + prog, family = poisson, data = data)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "LDV.html#sample-selection-models-heckmans-heckit",
    "href": "LDV.html#sample-selection-models-heckmans-heckit",
    "title": "6  Limited Dependent Variable Models and Maximum Likelihood Estimation",
    "section": "6.6 Sample Selection Models (Heckman’s Heckit)",
    "text": "6.6 Sample Selection Models (Heckman’s Heckit)\nSample selection bias occurs when the sample is not randomly selected from the population. For example, estimating wages only for people who are employed (a non-random subset).\n\nThe Heckit model is a two-step procedure:\n\nSelection Equation (Probit): Model the probability of being included in the sample.\nOutcome Equation: Model the outcome of interest, but include a correction term called the Inverse Mills Ratio (\\(\\lambda\\)) estimated from the first step to control for selection bias.\n\nThis corrects the bias that would occur if the second step were run on the selected sample alone.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limited Dependent Variable Models and Maximum Likelihood Estimation</span>"
    ]
  }
]